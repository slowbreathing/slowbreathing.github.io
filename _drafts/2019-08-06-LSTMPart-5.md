---
layout: post
title:  "RNN Series:LSTM internals:Part-5: Checking"
excerpt: "In this multi-part series(<a href='/articles/2019-07/LSTMPart-1'>part-1</a>,<a href='/articles/2019-07/LSTMPart-2'>part-2</a>,<a href='/articles/2019-07/LSTMPart-3'>part-3</a>) we look into composing LSTM into multiple higher layers and its directionality. Though <strong>Multiple layers</strong> are compute-intensive, they have better accuracy and so does <strong>bidirectional connections.</strong> More importantly, a solid understanding of the above mentioned paves the way for concepts like <strong>Highway connections</strong>, <strong>Residual Connections</strong>, <strong>Pointer networks</strong>, <strong>Encoder-Decoder</strong> Architectures and so forth in a future article. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>"
mathjax: true
date:   2019-07-22 15:07:19
categories: [article]
tags: Artificial-Intelligence Deep-Learning LSTM RNN Sequence-Modelling
comments: true
---

### Introduction



### Summary
Finally, that would be the complete documentation of LSTMs inner workings. Maybe a comparison with RNNs for vanishing gradient improvement will complete it. But that apart, once we have a drop on LSTMs like we did, using them in more complex architectures will become a lot easier. In the immediate next article, we'll look at the improvements in propagation because of vanishing and exploding gradients, if any, brought about by LSTMs over RNNs.   

[Recurent depth]: LSTMPart-4#Recurent depth
[Feed Forward depth]: LSTMPart-4#Feed Forward depth
[Multi layer RNNs Forward pass]: LSTMPart-4#Multi layer RNNs Forward pass
[Multi layer RNNs backward propagation]: LSTMPart-4#Multi layer RNNs backward propagation
[Backward propagation through LSTMCell(Step-2)]: LSTMPart-3#Backward propagation through LSTMCell(Step-2)

[Multi layer RNNs]: LSTMPart-4#Multi layer RNNs
[Bidirectional RNNs section]: LSTMPart-4#Bidirectional RNNs section
[Combining Bidirectional and MultiLayerRNNs]: LSTMPart-4#Combining Bidirectional and MultiLayerRNNs

[DHt]: LSTMPart-3#DHt
[DOt]: LSTMPart-3#DOt
[DCt]: LSTMPart-3#DCt
[DCprojt]: LSTMPart-3#DCprojt
[DFt]: LSTMPart-3#DFt
[DIt]: LSTMPart-3#DIt
[DHX]: LSTMPart-3#DHX
[DCt_recur]: LSTMPart-3#DCt_recur
[figure-6]: LSTMPart-3#figure-6


[part-1]: /articles/2019-07/LSTMPart-1
[part-2]: /articles/2019-07/LSTMPart-2
[part-3]: /articles/2019-07/LSTMPart-3
[part-4]: /articles/2019-07/LSTMPart-4
[multi-part series]: /tags/#LSTM
[first principles]: https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0
[Deep-Breathe]: https://github.com/slowbreathing/Deep-Breathe
[LayerNormBasicLSTMCell]: https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell
[lstm-1]: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
[lstm-2]: https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
[lstm-3]: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[lstm-4]: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html
[lstm-5]: https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577

[eq-1]: /articles/2019-05/softmax-and-its-gradient#eq-1
[eq-2]: /articles/2019-05/softmax-and-cross-entropy#eq-2
[eq-3]: /articles/2019-05/softmax-and-cross-entropy#eq-3


[code-1]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344
[code-2]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L146-L153

[MultiRNNCell]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193
[Bidirectional RNNs]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416
[softmax]: /articles/2019-05/softmax-and-its-gradient
[cross_entropy_loss]: /articles/2019-05/softmax-and-cross-entropy
[Tensorflow]: https://www.tensorflow.org/
[Listing-1]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/tfwordslstm.py
[Listing-2]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMainGraph.py
[Listing-3]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMain.py
[scr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L13
[scr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L15
[scr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L14
[pygr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn.py#L75-L76
[pygr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/common.py#L241-L255
[pygr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn_cell.py#L79-L81
