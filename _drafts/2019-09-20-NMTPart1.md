---
layout: post
title:  "Natural Language Processing Series: Neural Machine Translation(NMT):Part-1: Highly Simplified, completely Pictorial understanding of Neural Machine Translation"
excerpt: "In the current article, we look at <strong>Artificial Intelligence(AI)</strong> based translation systems from one <strong>Natural Language</strong> to another. Natural language, as in the way humans speak and how far have we come in <strong>designing machines that understand and act on it</strong>. The first of the <strong>multi-part series</strong> is <strong>highly simplified</strong>, <strong>completely pictorial</strong>, dressing down of <strong>Neural Machine Translation</strong> systems.  Once you have a solid understanding of systems like these, we take a look at where are we currently applying it to(case studies), to broaden the horizons. The rest of the parts will be a rigorous under-the-skin(math,code and logic) look of Neural Machine Translators, Attention and, the cherry on the cake Googles Neural Machine Translators"
mathjax: true
date:   2019-09-19 15:07:19
categories: [article]
tags: Artificial-Intelligence Deep-Learning LSTM RNN Sequence-Modelling Natural-Language-Processing NLP Neural-Machine-Translation NMT Attention
comments: true
---
# <a name="Multi layer RNNs backward propagation"></a>
### Introduction
In the current article, we look at <strong>Artificial Intelligence(AI)</strong> based translation systems from one <strong>Natural Language</strong> to another. Natural language, as in the way humans speak and how far have we come in <strong>designing machines that understand and act on it</strong>. The first of the <strong>multi-part series</strong> is <strong>highly simplified</strong>, <strong>completely pictorial</strong>, dressing down of <strong>[Neural-Machine-Translation]</strong> systems.  Once you have a solid understanding of systems like these, we take a look at where are we currently applying it to(case studies), to broaden the horizons.

Artificial Intelligence(AI) is a vast subject. To explain in short and without getting technical is even more difficult. However, if we confine ourselves to examples in a particular domain, it might be a lot easier. Once we understand AI in the context of our chosen example, we could then broaden the horizon.  In our case, we consider software that translates text from one language to another. Let's look at a quick history.

There are multiple parts to the article:
1. [Machine Translation: A quick history]
2. [Neural Machine Translators(NMT) with Attention]


# <a name="Machine Translation: A quick history"></a>
### Machine Translation: A quick history
#### Machine Translation(MT)
<p>On a basic level, MT performs simple substitution of words in one language for words in another. But that alone usually cannot produce a good translation of a text because, recognition of whole phrases and their closest counterparts in the target language is needed. Have you ever watched a movie with subtitles being so bad it actually distracts. A few years back I have had the crass luck of accidentally watching a movie with subtitles turn on for the Hindi language. I am wondering if I can call it hilarious or scary, maybe both. A serious plot-based movie’s literal translation of “holy shit” in Hindi forever ruined my movie-watching experience.</p>
<p>The context of what is being said is extremely vital to do a decent translation. Let's look at an example. Out of many possibilities, one correct literal translations of “Higher ape” to Hindi would be “ooncha bandar”, translated back to English it becomes “Tall monkey”. The last example completely loses the context of what is being said. The genetic ancestral context is completely lost.</p>

#### Rule-based Machine Translation(RBMT)
An RBMT system generates output sentences (in some target language) on the basis of morphological, syntactic, and semantic analysis of both the source and the target languages involved in a concrete translation task. This is difficult, to say the least. And the rules for a language is not reusable for another.

#### Statistical Machine Translation(SMT)
Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM's Thomas J. Watson Research Center. Roughly translated and simplified, SMT measures the conditional probability that a sequence of words Y in the target language is a true translation of a sequence of words X in the source language. The problem with SMT is that the use of probabilities to understand the complex structure of language was a bit too simplistic. They have had moderate success and were used nealy a decade back.  

#### Neural Machine Translators(NMT)
The object of our discussion for the current article and the accuracy has to be seen to be believed. To be precise, what I am presenting today is <strong>Neural Machine Translators(NMT) with Attention<strong>. Like most(not all) AI-based models, <strong>NMTs<strong> also learn by looking at examples, in our case example translations. In this respect, they are similar to SMTs. However, their internal representation where they store their understanding is a <strong>neural network<strong> instead of a probabilistic one.

# <a name="Neural Machine Translators(NMT) with Attention"></a>
### Neural Machine Translators(NMT) with Attention
Thare are multiple parts:
1. [Process of training]
2. [Jump to accuracy first]
3. [Generality]
4. [Practice makes the NMT perfect]
5. [Hindi to Engish Examples]
6. [Could not resist having some fun]
7. [French to english]
<a name="Process of training"></a>
#### Process of training

> * Ignore the blue rectangle for now.
> * Words are converted to their numerical representation called embeddings before being fed into the NMT.
> * Once you have trained with enough data, the NMTs start to understand the 2 languages in question. In figure-1 its Hindi and English.   
{%
    include image.html
    src="/img/nmt/hinditoengnmt.jpg"
    caption="figure-1: <strong>Hindi to english</strong>. Ignore the blue rectangle for now"
    hight="110%"
    width="110%"
%}

> * Ignore the blue rectangle for now.
> * Once you have trained with enough data, the NMTs start to understand the 2 languages in question. In figure-2 its French and English.   
{%
    include image.html
    src="/img/nmt/frenchtoengnmt.jpg"
    caption="figure-2: <strong>French to english</strong>. Ignore the blue rectangle for now"
    hight="110%"
    width="110%"
%}

> * Ignore the blue rectangle for now.
> * Once you have trained with enough data, the NMTs start to understand the 2 languages in question. In figure-2 its Vietnamese and English.   
{%
    include image.html
    src="/img/nmt/vietnamesetoengnmt.jpg"
    caption="figure-3: <strong>Vietnamese to english</strong>. Ignore the blue rectangle for now"
    hight="110%"
    width="110%"
%}

> * The training has to be meticulous, in that, the data has to be as accurate as possible. Various measures of accuracy checks have to employed to see if the desired accuracy has been achieved. Some data has to be kept aside for testing that is not shown during training. That and more in the deep technical analysis the articles that follow.

<a name="Jump to accuracy first"></a>
#### Jump to accuracy first

To hell with the theory, show me the accuracy first. <strong>If you have not seen a system like this before, the results are nothing but magical<strong>.

> * Take a pause, and imagine yourself doing a translation between any 2 languages you know. Happens subconsciously, but pause and analyze.  You read the source sentence first, and then as you translate you keep focusing on the phrases/words as you translate. Attention-based NMTs are built with the same idea.
> * But what is that matrix?

{%
    include image.html
    src="/img/nmt/hitoenex11.jpg"
    caption="figure-4: <strong>Hindi to english</strong>"
    hight="110%"
    width="110%"
%}

> * The NMT jots down its understanding of the 2 languages in it's internal [LSTM][LSTM series] based neural network.
> * I'm merely visualizing it specific to this example translation.
> * It illustrates the <strong>attention(pun intended)</strong> paid on the source words/phrases to translate to target words phrases.
> * Similar to human Translators.

{%
    include image.html
    src="/img/nmt/hitoenex13.jpg"
    caption="figure-5: <strong>Hindi to english</strong>"
    hight="110%"
    width="110%"
%}

> * The highlighted portions of a matrix illustrate the specific word/phrases within the sentences.

{%
    include image.html
    src="/img/nmt/hitoenex15.jpg"
    caption="figure-6: <strong>Hindi to english</strong>"
    hight="110%"
    width="110%"
%}

> * The highlighted portions show prepositions in both languages.
> * Well, it should come as no surprise that the NMTs can work out "Parts of speech" of a language and not just the prepositions.

{%
    include image.html
    src="/img/nmt/hitoenex14.jpg"
    caption="figure-7: <strong>Hindi to english</strong>"
    hight="110%"
    width="110%"
%}

<a name="Generality"></a>
#### Generality:
If the accuracy is convincing then lets talk about generality.
As you can witness yourself, the model architecture remains the same but when trained with different datasets “it acquires knowledge” about that data set and starts making predictions, in our case translations. <strong>This probably the most important reason why AI is going to redefine how programming is done in the future.</strong>   

<a name="Practice makes the NMT perfect"></a>
#### Practice makes the NMT perfect:

> * 500 rounds/epochs of training.

{%
    include image.html
    src="/img/nmt/hitoenexprog500.jpg"
    caption="figure-8: <strong>Hindi to english</strong>:500 rounds/epochs of training"
    hight="110%"
    width="110%"
%}

> * 1000 rounds/epochs of training.
> * The most important aspect being the <strong>difference between what is predicted by the NMT and what is the ground truth<strong>.
> * This difference which we historically call error or loss is "communicated" back to the NMT.
> * The NMT promptly adjusts its internal state and explore other possibilities.
> * This is what "we"(scientific community) refer to as learning, and we think(well, the same scientific community) this learning "somewhat" mimics how human brain work.   

{%
    include image.html
    src="/img/nmt/hitoenexprog1000.jpg"
    caption="figure-9: <strong>Hindi to english</strong>:1000 rounds/epochs of training"
    hight="110%"
    width="110%"
%}

> * 2000 rounds/epochs of training.

{%
    include image.html
    src="/img/nmt/hitoenexprog2000.jpg"
    caption="figure-10: <strong>Hindi to english</strong>:2000 rounds/epochs of training"
    hight="110%"
    width="110%"
%}

> * 5000 rounds/epochs of training.

{%
    include image.html
    src="/img/nmt/hitoenexprog5000.jpg"
    caption="figure-11: <strong>Hindi to english</strong>:5000 rounds/epochs of training"
    hight="110%"
    width="110%"
%}

> * 7000 rounds/epochs of training.

{%
    include image.html
    src="/img/nmt/hitoenexprog7000.jpg"
    caption="figure-12: <strong>Hindi to english</strong>:7000 rounds/epochs of training"
    hight="110%"
    width="110%"
%}

> * 10000 rounds/epochs of training.

{%
    include image.html
    src="/img/nmt/hitoenexprog10000.jpg"
    caption="figure-13: <strong>Hindi to english</strong>:10000 rounds/epochs of training"
    hight="110%"
    width="110%"
%}

> * Incremental comparison

{%
    include image.html
    src="/img/nmt/hitoenexprog109900300000.jpg"
    caption="figure-14: <strong>Hindi to english</strong>: Attention comparison"
    hight="110%"
    width="110%"
%}

<a name="Hindi to Engish Examples"></a>
#### Hindi to Engish Examples

Why Hindi? I could have chosen any language where data was easy to come by. I did not because, with Hindi I can act as a human evaluator. This is a language we speak at home, to be more prcise, it is a mix of English, Hindi and Kannada. My Hindi skills are, at best average but good enough, for evaluating the translations.

The data for Indian languages are surprisingly difficult to come by. <strong>The data that is used for this blog, does not belong to [Stillwaters] or [Stillwaters]'  client by any means</strong>. This has been personally sourced for this blog from [Charles University, Czech Republic] and [Statistical Machine Translation]. <strong>When [Stillwaters] or me personally work with a fintech giant or a social media giant, the data is sourced by their team and, one can make guarantees about it's accuracy. Infact, they have dedicated team(s) which exists solely for this purpose.</strong> But this data cannot be shared for various reasons. However, the same cannot be said about personally sourced dataset like the ones I'm using for this blog. Example below shows an example of really bad translation. It isn't even a good literal translation. Infact, it isnt even a correct statement in english.

"मलेरिया के मुकाबले।"  
"than are put into malaria."


<strong>Long story short, the accuracy of the translations can be much much much better with better data.</strong>


> * [BPE] Stands for [Byte Pair Encoding]. It was originally developed for compressing data. While it is still used for compression in Natural Language processing, it has a few more important advantages.  

> * Let's say that we train a model on words like "walk", "walking", "walked" and it begins to understand the relationship between the words. However, similar relationship between "talk", "talked", "talking" isnt clear <strong>by analogy</strong> if it has not trained on all the words before. This can be remedied by breaking words into smaller syllabic tokens. Words like "walking" can be broken down into "walk" and "ing". Similary "walked" can be broken doen into "walk" and "ed". <strong>This also clarifies the meaning of "ed" and "ing" to the model and by analogy the model is able to infer the meaning of talking even though it has never encountered the word before.</strong>

> * Another positive side effect is, rare words not seen at training time can still be recognized because they are made up of smaller syllabic tokens.
> * Lastly, it improves accuracy.
> * Now, even if you have not understood the above points, just remember the words are broken down into smaller syllabic tokens before being fed into the NMT.

> * <strong>Hindi to english</strong>:ex-1

{%
    include image.html
    src="/img/nmt/hitoenex16.jpg"
    caption="figure-15: <strong>Hindi to english</strong>:ex-1"
    hight="110%"
    width="110%"
%}

> * <strong>Hindi to english</strong>:ex-2

{%
    include image.html
    src="/img/nmt/hitoenex17.jpg"
    caption="figure-16: <strong>Hindi to english</strong>:ex-2"
    hight="110%"
    width="110%"
%}

> * <strong>Hindi to english</strong>:ex-3

{%
    include image.html
    src="/img/nmt/hitoenex18.jpg"
    caption="figure-17: <strong>Hindi to english</strong>:ex-3"
    hight="110%"
    width="110%"
%}

> * <strong>Hindi to english</strong>:ex-4
> * These are really long sentences, that we could crack only after the arrival of "Attention".  


Src:"ल@@ ले@@ याल सत@@ वारी के ना@@ यब सर@@ पंच चैन सिंह ने बताया कि यहां आने वाले श ् रद ् धा@@ लुओं के लिए ल@@ ंगर की भी व ् यवस ् था है ।"  
Ref:"Nay@@ ab Sar@@ pan@@ ch of Lal@@ ey@@ al Sat@@ wari , Cha@@ in Singh said that a L@@ ang@@ ar has also been organised for the pilgrims visiting this place ."  
NMT:"Nay@@ ab Sar@@ pan@@ ch of Lal@@ ey@@ al Sat@@ wari , Cha@@ in Singh said that a L@@ ang@@ ar has also been organised for the pilgrims visiting this place ."

{%
    include image.html
    src="/img/nmt/output_bibsattention_infer_ind112.jpg"
    caption="figure-18: <strong>Hindi to english</strong>:ex-4"
    hight="110%"
    width="110%"
%}

> * <strong>Hindi to english</strong>:ex-5
> * These are really long sentences, that we could crack only after the arrival of "Attention".


Src:"शुक ् रवार को जय@@ राम ने कहा कि अदालतों का अपनी सीमाएं पार करते हुए अफसरों और सरकार के काम को अपने हाथ में ले लेना ठीक नहीं है ।"  
Ref:"On Friday J@@ airam said that it is not right for the Courts to exceed their powers and take the job of the officers and the Government into their own hands ."  
NMT:"On Friday J@@ airam said that it is not right for the Courts to exceed their powers and take the job of the officers and the Government into their own hands ."  


{%
    include image.html
    src="/img/nmt/output_bibsattention_infer_ind374.jpg"
    caption="figure-19: <strong>Hindi to english</strong>:ex-5"
    hight="110%"
    width="110%"
%}

<a name="Could not resist having some fun"></a>
#### Could not resist having some fun
<I>If you're not having fun, you're not learning. There's a pleasure in finding things out.<I>

> * <strong>Hindi to english</strong>:ex-6
> * These are really long sentences, that we could crack only after the arrival of "Attention".

Src:"बड ़ े-@@ बड ़ े देशों में छोटी-@@ छोटी बातें होती रहती हैं &apos; ।"  
Ref:"In big countries , little things like these keep happening ."  
NMT:"In big countries , little things like these keep happening ."  

{%
    include image.html
    src="/img/nmt/output_bibsattention_infer_ind520.jpg"
    caption="figure-19: <strong>Hindi to english</strong>:ex-6"
    hight="110%"
    width="110%"
%}

> * <strong>Hindi to english</strong>:ex-7
> * These are really long sentences, that we could crack only after the arrival of "Attention".

Src:"इतनी शि@@ द@@ अत से मैं तुम ् हे पाने की कोशिश की है , की हर ज़@@ ारे ने मुझे तुमसे मिलाने की सा@@ ज़@@ ि@@ श की है ।"  
Ref:"I have tried to get you with all my heart , that the whole universe has con@@ sp@@ ired to introduce me to you ."  
NMT:"I have tried to get you with all my heart , the whole universe has con@@ sp@@ ired to introduce me to you ."  

{%
    include image.html
    src="/img/nmt/output_bibsattention_infer_ind521.jpg"
    caption="figure-19: <strong>Hindi to english</strong>:ex-7"
    hight="110%"
    width="110%"
%}

> * <strong>Hindi to english</strong>:ex-8
> * These are really long sentences, that we could crack only after the arrival of "Attention".

Src:"ब@@ सं@@ ती इन कुत ् तों के सामने मत ना@@ चना"  
Ref:"Bas@@ anti don &apos;t dance in front of these dogs ."  
NMT:"Bas@@ anti don &apos;t dance in front of these dogs ."  

{%
    include image.html
    src="/img/nmt/output_bibsattention_infer_ind522.jpg"
    caption="figure-19: <strong>Hindi to english</strong>:ex-8"
    hight="110%"
    width="110%"
%}

<a name="French to english"></a>
### French to english

My knowledge of the French language is rudimentary, I can barely save my life. I doubt the last statement though. I learnt French to for the purpose of the NMT. That is the limit of French knowledge. Here is one of the things that stand out in French. In English, adjectives are pretty easy to use. You put them before the noun they describe and you’re done. In French however, the placement of adjectives varies. And if that wasn’t enough to confuse you and me, adjectives also change depending on whether the noun they describe is masculine, feminine, singular or plurial.

> * <strong>French to english</strong>:ex-1
> * These are really long sentences, that we could crack only after the arrival of "Attention".

{%
    include image.html
    src="/img/nmt/frtoenex1.jpg"
    caption="figure-19: <strong>French to english</strong>:ex-1"
    hight="110%"
    width="110%"
%}

> * <strong>French to english</strong>:ex-2
> * These are really long sentences, that we could crack only after the arrival of "Attention".

{%
    include image.html
    src="/img/nmt/frtoenex2.jpg"
    caption="figure-19: <strong>French to english</strong>:ex-2"
    hight="110%"
    width="110%"
%}

<a name="French to english"></a>
### Natural Language Processing Case Studies




### Summary
Finally, that would be the complete documentation of LSTMs inner workings. Maybe a comparison with RNNs for vanishing gradient improvement will complete it. But that apart, once we have a drop on LSTMs like we did, using them in more complex architectures will become a lot easier. In the immediate next article, we'll look at the improvements in propagation because of vanishing and exploding gradients, if any, brought about by LSTMs over RNNs.   

[Neural-Machine-Translation]: /tags/#NMT
[NMT]: /tags/#NMT
[Attention]: /tags/#Attention
[LSTM series]: /tags/#LSTM

# page navigation
[Machine Translation: A quick history]: NMTPart1#Machine Translation: A quick history
[Neural Machine Translators(NMT) with Attention]: NMTPart1#Neural Machine Translators(NMT) with Attention
[Process of training]: NMTPart1#Process of training
[Jump to accuracy first]: NMTPart1#Jump to accuracy first
[Generality]: NMTPart1#Generality
[Practice makes the NMT perfect]: NMTPart1#Practice makes the NMT perfect
[Hindi to Engish Examples]: NMTPart1#Hindi to Engish Examples
[Stillwaters]: https://www.stillwaters.ai
[Charles University, Czech Republic]: http://ufal.mff.cuni.cz/
[Statistical Machine Translation]: http://www.statmt.org/
[BPE]: https://en.wikipedia.org/wiki/Byte_pair_encoding
[Byte Pair Encoding]: https://en.wikipedia.org/wiki/Byte_pair_encoding
[French to english]: NMTPart1#French to english


[Bidirectional RNNs section]: LSTMPart-4#Bidirectional RNNs section
[Combining Bidirectional and MultiLayerRNNs]: LSTMPart-4#Combining Bidirectional and MultiLayerRNNs

[DHt]: LSTMPart-3#DHt
[DOt]: LSTMPart-3#DOt
[DCt]: LSTMPart-3#DCt
[DCprojt]: LSTMPart-3#DCprojt
[DFt]: LSTMPart-3#DFt
[DIt]: LSTMPart-3#DIt
[DHX]: LSTMPart-3#DHX
[DCt_recur]: LSTMPart-3#DCt_recur
[figure-6]: LSTMPart-3#figure-6


[part-1]: /articles/2019-07/LSTMPart-1
[part-2]: /articles/2019-07/LSTMPart-2
[part-3]: /articles/2019-07/LSTMPart-3
[part-4]: /articles/2019-07/LSTMPart-4
[multi-part series]: /tags/#LSTM
[first principles]: https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0
[Deep-Breathe]: https://github.com/slowbreathing/Deep-Breathe
[LayerNormBasicLSTMCell]: https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell
[lstm-1]: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
[lstm-2]: https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
[lstm-3]: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[lstm-4]: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html
[lstm-5]: https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577

[eq-1]: /articles/2019-05/softmax-and-its-gradient#eq-1
[eq-2]: /articles/2019-05/softmax-and-cross-entropy#eq-2
[eq-3]: /articles/2019-05/softmax-and-cross-entropy#eq-3


[code-1]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344
[code-2]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L146-L153

[MultiRNNCell]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193
[Bidirectional RNNs]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416
[softmax]: /articles/2019-05/softmax-and-its-gradient
[cross_entropy_loss]: /articles/2019-05/softmax-and-cross-entropy
[Tensorflow]: https://www.tensorflow.org/
[Listing-1]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/tfwordslstm.py
[Listing-2]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMainGraph.py
[Listing-3]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMain.py
[scr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L13
[scr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L15
[scr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L14
[pygr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn.py#L75-L76
[pygr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/common.py#L241-L255
[pygr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn_cell.py#L79-L81
