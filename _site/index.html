<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="Programming is more than just typing.">
  <meta name="keywords" content="blog and jekyll">
  <meta name="author" content="Slowbreathing">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Slowbreathing">
  <meta name="twitter:description" content="Programming is more than just typing.">
  
    <meta property="twitter:image" content="http://localhost:4000/img/leonids-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="http://localhost:4000/">
  <meta property="og:title" content="Slowbreathing">
  <meta property="og:description" content="Programming is more than just typing.">
  
    <meta property="og:image" content="http://localhost:4000/img/leonids-logo.png">
  
  <title>Slowbreathing</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="http://localhost:4000/css/font-awesome.min.css">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">

  <link rel="canonical" href="http://localhost:4000/">
  <link rel="alternate" type="application/rss+xml" title="Slowbreathing" href="http://localhost:4000/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.png">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142206738-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-142206738-1');
  </script>-->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Slowbreathing" />
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="http://localhost:4000/">
    <img src="http://localhost:4000/img/leonids-logo.png" alt="" class="avatar">
  </a>
  
  <a href="http://localhost:4000/" class="author_name">Mohit Kumar</a>
  <span class="author_job">Researcher/Consultant/Trainer</span>
  <span class="author_bio mbm">Programming is more than just typing.</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="http://localhost:4000/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="http://localhost:4000/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="http://localhost:4000/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="http://localhost:4000/resume/">about me</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/tags/">Tags</a>
      </li>
                 
    </ul>
  </nav>
  <script type="text/javascript">
  // based on https://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('mohit.riverstone@gmail.com', 'Hello from website');</script>
      </li>
    
    <li><a href="https://twitter.com/stillwaters_ia" class="social-link-item" target="_blank"><i class="fa fa-fw fa-twitter"></i></a></li>
    <li><a href="https://facebook.com/mohit.kumar.965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-facebook"></i></a></li>
    
    <li><a href="https://linkedin.com/in/mohit-kumar-05621b62" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    <li><a href="https://instagram.com/slowbreathing.github.io" class="social-link-item" target="_blank"><i class="fa fa-fw fa-instagram"></i></a></li>
    
    <li><a href="https://github.com/Slowbreathing" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    <li><a href="https://www.pinterest.com/mohitkumar965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-pinterest"></i></a></li>
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        
<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        4 JUL 2021
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2021-07/ChipDesign" class="post-title" title="Artificial Intelligence based chip design: Chip Design">Artificial Intelligence based chip design: Chip Design</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/chipdesign/chipdesignai.png"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        This is a departure from my usual tech blog. In the <strong><a href='/tags/#Artificial-Intelligence-based-chip-design'>current series</a></strong> titled <strong><a href='/tags/#Artificial-Intelligence-based-chip-design'>Artificial-Intelligence-based-chip-design</a></strong>, I/we present the use of <strong>Artificial Intelligence</strong> to <strong>design FPGA chips</strong> for <strong>Ultra-Low-Latency</strong> based speech transformers. This showcases our products as well as our skills in the respective domains. This started as an experiment to lower latency for a seq-to-seq LSTM based speech synthesis system. Post a thorough analysis, we broke it down into <strong>1. mapping the matrix multiplication of deep learning transformers into an FPGA chip</strong>(<strong><a href='articles/2021-07/Transformers-On-Chip'>Transformers On Chip(part-1)</a></strong>) and <strong>2. co-designing the FPGA chip for the appropriate workload</strong>(<strong><a href='articles/2021-07/ChipDesign'>ChipDesign(part-2)</a></strong>). In part-2 we treat the resources on FPGA chip, Workload, other design parameters as <strong>design variables</strong>. Finally, we feed the design variables into the Deep Reinforcement Learning agent to learn. Post learning the expectation from Deep Reinforcement Learning agent being the <strong>optimal placement</strong> of blocks to maximize certain goals like latency, etc. The Deep Reinforcement Learning Algorithm is supposed to figure out a balance that <strong>speeds up computation for tolerable accuracy losses(if at all)</strong>.To use a boxing term, <strong>pound for pound</strong>, the <strong>same hardware micro-designed by a Deep Reinforcement Learning Agent for carrying a specific load</strong>.
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        3 JUL 2021
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2021-07/Transformers-On-Chip" class="post-title" title="Artificial Intelligence based chip design: Transformers on Chip">Artificial Intelligence based chip design: Transformers on Chip</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/chipdesign/transformersonchip.png"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        This is departure from my usual tech blog. In the <strong><a href='/tags/#Artificial-Intelligence-based-chip-design'>current series</a></strong> titled <strong><a href='/tags/#Artificial-Intelligence-based-chip-design'>Artificial-Intelligence-based-chip-design</a></strong>, I/we present the use of <strong>Artificial Intelligence</strong> to <strong>design FPGA chip</strong> for <strong>Ultra-Low-Latency</strong> based speech transformers. This showcases our products as well as our skills in the respective domains. This started as an experiment to lower latency for a seq-to-seq LSTM based speech synthesis system. Post a thorough analysis, we broke it down into <strong>1. mapping the matrix multiplication of deep learning transformers into an FPGA chip</strong>(<strong><a href='articles/2021-07/Transformers-On-Chip'>Transformers On Chip(part-1)</a></strong>) and <strong>2. co-designing the FPGA chip for the appropriate workload</strong>(<strong><a href='articles/2021-07/'>ChipDesign(part-2)</a></strong>). In part-1 we look specifically at resources available on an FPGA chip and a few different strategies of <strong>hardware mapping</strong> of the workload(matrix multiplication) on the above chip.
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        29 JUL 2020
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2020-07/MeditatingProcessor-5" class="post-title" title="Meditating with microprocessors Series: Part-5: Appendix:Tools of the trade(part-5)">Meditating with microprocessors Series: Part-5: Appendix:Tools of the trade(part-5)</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/mwprocs/bpfsw2.png"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        The current article is part of a bigger <strong><a href='/tags/#Meditating-with-microprocessors'>series</a></strong> titled <strong><a href='/tags/#Meditating-with-microprocessors'>Meditating-with-microprocessors</a></strong>,in which I demonstrate the use of <strong>Artificial Intelligence</strong> to tune <strong>microprocessors</strong> for <strong>Ultra Low Latency</strong> and <strong>Realtime</strong> loads. There are 5 parts to the series <strong><a href='/articles/2020-07/MeditatingProcessor-1'>Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1) </a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-2'>A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-3'>Trading off power for UltraLowLatency(part-3)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-4'>Artificial Intelligence guided Predictive MicroProcessor tuning(part-4)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-5'>Appendix:Tools of the trade(part-5) </a></strong>. In the current article, we drill down into the inner workings of <strong>Ftrace</strong> and <strong>EBPF</strong>. With <strong>EBPF based techniques and frameworks</strong> Linux tooling has gained <strong>tracing superpowers</strong>. It has capabilities that now exceed that of Dtrace on Solaris. This is not an exhaustive section and it does not do a breadth-first scan of Linux tooling. I am highlighting a couple of tools, why they work for us, and what makes them special. Moreover, tools play a vital role in verifying or rebutting the theories we make about the systems we design. Building/designing a system is like building a beehive, little blocks you put together making it a whole. But these little blocks fit more snuggly if the theory is rock-solid. Tools provide us the evidence for that.
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        29 JUL 2020
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2020-07/MeditatingProcessor-4" class="post-title" title="Meditating with microprocessors Series: Part-4: Artificial Intelligence guided Predictive MicroProcessor tuning">Meditating with microprocessors Series: Part-4: Artificial Intelligence guided Predictive MicroProcessor tuning</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/mwprocs/asymmetrictransformer3.png"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        The current article is part of a bigger <strong><a href='/tags/#Meditating-with-microprocessors'>series</a></strong> titled <strong><a href='/tags/#Meditating-with-microprocessors'>Meditating-with-microprocessors</a></strong>,in which I demonstrate the use of <strong>Artificial Intelligence</strong> to tune <strong>microprocessors</strong> for <strong>Ultra Low Latency</strong> and <strong>Realtime</strong> loads. There are 5 parts to the series <strong><a href='/articles/2020-07/MeditatingProcessor-1'>Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1) </a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-2'>A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-3'>Trading off power for UltraLowLatency(part-3)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-4'>Artificial Intelligence guided Predictive MicroProcessor tuning(part-4)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-5'>Appendix:Tools of the trade (part-5) </a></strong>. In the current article, we explore the use of Artificial Intelligence to reduce latency and save power at the same time. As we have understood, left to itself, the microprocessor will sense the load and configure itself for it(suitable c-states, P-states, etc. ). <strong>However, this process is reactive at best, and if latency matters then it is already late. By reactive, I mean the arrival of a remote message triggers an interrupt and from there, one thing leads to another before the microprocessor configures itself for the load. Causal is another word for it.</strong> <strong>With an Artificial Intelligence based model can we reduce latency by predicting load and preconfigure the microprocessor for load and then postconfigure to save power once the load has tided over. In other words making the process more proactive and predictive.</strong>
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        29 JUL 2020
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2020-07/MeditatingProcessor-3" class="post-title" title="Meditating with microprocessors Series: Part-3: Trading off power for UltraLowLatency">Meditating with microprocessors Series: Part-3: Trading off power for UltraLowLatency</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/mwprocs/pcm-inf-graf2.png"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        The current article is part of a bigger <strong><a href='/tags/#Meditating-with-microprocessors'>series</a></strong> titled <strong><a href='/tags/#Meditating-with-microprocessors'>Meditating-with-microprocessors</a></strong>,in which I demonstrate the use of <strong>Artificial Intelligence</strong> to tune <strong>microprocessors</strong> for <strong>Ultra Low Latency</strong> and <strong>Realtime</strong> loads. There are 5 parts to the series <strong><a href='/articles/2020-07/MeditatingProcessor-1'>Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1) </a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-2'>A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-3'>Trading off power for UltraLowLatency(part-3)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-4'>Artificial Intelligence guided Predictive MicroProcessor tuning(part-4)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-5'>Appendix:Tools of the trade(part-5) </a></strong>. In the current article, I present irrefutable evidence that if the processor is configured correctly with a goal in mind(for e.g. UltraLowLatency), the OS jitter caused by the processor can nearly be eliminated. The system can be much more predictable. Furthermore, It has a huge impact on the latency of the system for good. A <strong>substantial</strong> improvement in latency due to configuration on the <strong>Core</strong>, but beyond substantial due to  <strong>Uncore</strong>. <strong>The improvement due to Uncore is to be expected because there is a whole lot more circuitry on the Uncore</strong>. However, this is a trade off that comes at the cost of expending more power.
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        29 JUL 2020
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2020-07/MeditatingProcessor-2" class="post-title" title="Meditating with microprocessors Series: Part-2: A crashcourse in Microarchitecture and Linux CPUIDLE interface">Meditating with microprocessors Series: Part-2: A crashcourse in Microarchitecture and Linux CPUIDLE interface</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/mwprocs/cpuidle-sub1.png"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        The current article is part of a bigger <strong><a href='/tags/#Meditating-with-microprocessors'>series</a></strong> titled <strong><a href='/tags/#Meditating-with-microprocessors'>Meditating-with-microprocessors</a></strong>,in which I demonstrate the use of <strong>Artificial Intelligence</strong> to tune <strong>microprocessors</strong> for <strong>Ultra Low Latency</strong> and <strong>Realtime</strong> loads. There are 5 parts to the series <strong><a href='/articles/2020-07/MeditatingProcessor-1'>Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1) </a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-2'>A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-3'>Trading off power for UltraLowLatency(part-3)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-4'>Artificial Intelligence guided Predictive MicroProcessor tuning(part-4)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-5'>Appendix:Tools of the trade(part-5) </a></strong>. In the current article I lay the  technical groundwork for later articles in the series to build on. The term's and technologies I'll be using later must be understood really well. So we look at 3 concepts primarily. Firstly, Architecture of a modern microprocessor in short ,really really short. Just enough to make a mental model of the microprocessor to work with. Secondly, How does does software(Linux) interface with the microprocessor. Again just enough to make sense of the data we gather form the microprocessor using various UltraLowLatency profilers and tracers. Thirdly, help a programmers like you to get started. 
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        29 JUL 2020
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2020-07/MeditatingProcessor-1" class="post-title" title="Meditating with microprocessors Series: Part-1: Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea">Meditating with microprocessors Series: Part-1: Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/mwprocs/c-states3.jpeg"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        In the <strong><a href='/tags/#Meditating-with-microprocessors'>current series</a></strong> titled <strong><a href='/tags/#Meditating-with-microprocessors'>Meditating-with-microprocessors</a></strong>, I demonstrate the use of <strong>Artificial Intelligence</strong> to tune <strong>microprocessors</strong> for <strong>Ultra-Low-Latency</strong> and <strong>Realtime</strong> loads. The techniques, in general, can be extended to other components of a computer system like storage devices, memory, etc. However, the article series and my work is currently restricted to <strong>Intel microprocessors only</strong>. <strong>In future</strong>, we may extend this to other hardware components of a computer system. This is a very <strong>specialized</strong> and <strong>intense</strong> field and hence I intend to break it down using the first-principles approach into simpler pieces of technology that are easy to understand. There are 5 parts to the series <strong><a href='/articles/2020-07/MeditatingProcessor-1'>Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-2'>A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-3'>Trading off power for UltraLowLatency (part-3)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-4'>Artificial Intelligence guided Predictive MicroProcessor tuning (part-4)</a></strong>, <strong><a href='/articles/2020-07/MeditatingProcessor-5'>Appendix:Tools of the trade(part-5) </a></strong>. <strong>In the balance then, this is a documentation of my journey navigating these utterly specialized fields ( microarchitecture and Artificial Intelligence ), how to marry them together, the issues I faced, the respective solutions, what (how much) are the benefits if any, and what to have in the toolbox.<strong> 
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        19 SEP 2019
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2019-09/NMTPart1" class="post-title" title="Natural Language Processing Series: Neural Machine Translation(NMT):Part-1: Highly Simplified, completely Pictorial understanding of Neural Machine Translation">Natural Language Processing Series: Neural Machine Translation(NMT):Part-1: Highly Simplified, completely Pictorial understanding of Neural Machine Translation</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/nmt/hitoenex11.jpg"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        In the current article, we look at <strong>Artificial Intelligence(AI)</strong> based translation systems from one <strong>Natural Language</strong> to another. Natural language, as in the way humans speak and how far have we come in <strong>designing machines that understand and act on it</strong>. The first of the <strong>multi-part series</strong> is <strong>highly simplified</strong>, <strong>completely pictorial</strong>, dressing down of <strong>Neural Machine Translation</strong> systems.  Once you have a solid understanding of systems like these, we take a look at where are we currently applying it to(case studies), to broaden the horizons. The rest of the parts will be a rigorous under-the-skin(math,code and logic) look of <a href='/comingsoon.html'><strong>Neural Machine Translators</strong></a>, <a href='/comingsoon.html'><strong>Attention</strong></a> and, the cherry on the cake <a href='/comingsoon.html'><strong>Googles Neural Machine Translators</strong></a>
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        22 JUL 2019
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2019-07/LSTMPart-4" class="post-title" title="RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs">RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs</a>
      
    </h4>

    

    </header>

    
    <div class="post-image-feature">
      <img class="feature-image" src=
      
      "http://localhost:4000/img/rnn/bilstm7.jpg"
      
      alt=" feature image">

      
    </div><!-- /.image-wrap -->
    

    
    <div class="post-description">
      <p>
        In this <a href='/tags/'> (<a href='/articles/2019-07/LSTMPart-1'>part-1</a>,<a href='/articles/2019-07/LSTMPart-2'>part-2</a>,<a href='/articles/2019-07/LSTMPart-3'>part-3</a>) we look into composing LSTM into multiple higher layers and its directionality. Though <strong>Multiple layers</strong> are compute-intensive, they have better accuracy and so does <strong>bidirectional connections.</strong> More importantly, a solid understanding of the above mentioned paves the way for concepts like <strong>Highway connections</strong>, <strong>Residual Connections</strong>, <strong>Pointer networks</strong>, <strong>Encoder-Decoder</strong> Architectures and so forth in a future article. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        15 JUL 2019
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2019-07/LSTMPart-3" class="post-title" title="RNN Series:LSTM internals:Part-3: The Backward Propagation">RNN Series:LSTM internals:Part-3: The Backward Propagation</a>
      
    </h4>

    

    </header>

    

    
    <div class="post-description">
      <p>
        In this multi-part series(<a href='/articles/2019-07/LSTMPart-1'>part-1</a>,<a href='/articles/2019-07/LSTMPart-2'>part-2</a>), we look inside LSTM Backward Propagation. This would usually involve <strong>lots and lots of math</strong>. I love math but I am <strong>not a trained mathematician</strong>. However, I have decent <strong>intuition</strong> and intuition by itself can only get you so far. So I used my <strong>programming skills</strong> to validate pure theoretical results often cited by <strong>pure/trained mathematicians.</strong> <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        9 JUL 2019
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2019-07/LSTMPart-2" class="post-title" title="RNN Series:LSTM internals:Part-2:The Forward pass">RNN Series:LSTM internals:Part-2:The Forward pass</a>
      
    </h4>

    

    </header>

    

    
    <div class="post-description">
      <p>
        <strong>In this multi-part series, we look inside LSTM forward pass.</strong> Read the <a href='/articles/2019-07/LSTMPart-1'>part-1's</a> before you come back here. Once you are back, in this article, we explore the meaning, math and the implementation of an LSTM cell. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        9 JUL 2019
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2019-07/LSTMPart-1" class="post-title" title="RNN Series:LSTM internals:Part-1:The Big Picture">RNN Series:LSTM internals:Part-1:The Big Picture</a>
      
    </h4>

    

    </header>

    

    
    <div class="post-description">
      <p>
        LSTMs make up everything directly or indirectly from Time Series(TS) to Neural Machine Translators(NMT) to Bidirectional Encoder Representation from Transformers(BERT) to Neural Turing Machine(NTM) to Differentiable Neural Computers(DNC) etc, and yet they are not very well understood. <strong>This is a multi-part series that will unravel the mystery behind LSTMs.</strong> Especially it's gradient calculation when participating in a more complex model like NMT, BERT, NTM, DNC, etc. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        3 MAY 2019
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="post-title" title="Softmax and Cross-entropy">Softmax and Cross-entropy</a>
      
    </h4>

    

    </header>

    

    
    <div class="post-description">
      <p>
        The marriage of Softmax and CrossEntropy. This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and predicted value(yhat). 
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        1 MAY 2019
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2019-05/softmax-and-its-gradient" class="post-title" title="Softmax and its Gradient">Softmax and its Gradient</a>
      
    </h4>

    

    </header>

    

    
    <div class="post-description">
      <p>
        From the perspective of Deep Neural networks, softmax is one the most important activation function, maybe the most important. I had trouble understanding it in the beginning, especially its why its chosen, its gradient, its relationship with cross-entropy loss and the combined gradient. In this article, I further dumb it down and add code to theory. <strong>But, there is another selfish reason for reproducing these here, I will be able to refer to these while explaining more complex concepts like LSTMs, NMTs, BERTs, XLNETs, etc.</strong>
      </p>
    </div>
    


</section>

<section class="post">
  <header class="post-header">
    <p class="post-meta">
      <span class="post-date">
        1 MAY 2019
      </span>
      
      •
      
        <a class="post-cat" href="http://localhost:4000/categories/#article">article</a>
        
      
      
    </p>
    <h4>
      <a href="http://localhost:4000/articles/2019-05/introduction" class="post-title" title="Introduction: Why another technical blog?">Introduction: Why another technical blog?</a>
      
    </h4>

    

    </header>

    

    
    <div class="post-description">
      <p>
        Pictures say a thousand words is a cliche, but I believe in it. I have made a career out of it. Complex programming or logical concepts should be understood with the help of pictures.
      </p>
    </div>
    


</section>


        <footer>
  <strong>MIT License</strong> &copy; 2021 Mohit Kumar.
  <div class="newsletter-container" >
                <h4 class="newsletter-title">Subscribe</h4>
                <!-- <p class="newsletter-text"></p> -->
                <script type="text/javascript">var submitted=false;</script>
                <iframe name="hidden_iframe" id="hidden_iframe" style="display:none;"onload="if(submitted) {window.location='thankyou.html';}"></iframe>
                <form class="newsletter-form" name="gform" id="gform" enctype="text/plain" action="https://docs.google.com/forms/d/e/1FAIpQLSftY1olpvfsyxItLcV6kNsRSUQf3NQfPSL-RyT191nhPrfguA/formResponse"
                                                   target="hidden_iframe" onsubmit="submitted=true">
                    <p class="newsletter-text">Get new posts to your inbox</p>
                    <input class="newsletter-email" type="text" name="entry.1045781291" placeholder="name@example.com" />
                    <input class="newsletter-submit" type="submit" value="Subscribe" />
                </form>
  </div>
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="http://localhost:4000/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="http://localhost:4000/js/main.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-142206738-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
