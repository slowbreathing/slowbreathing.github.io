<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="From the perspective of Deep Neural networks, softmax is one the most important activation function, maybe the most important. I had trouble understanding it...">
  <meta name="keywords" content="blog and jekyll">
  <meta name="author" content="Softmax and its Gradient | Slowbreathing">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Softmax and its Gradient | Slowbreathing">
  <meta name="twitter:description" content="From the perspective of Deep Neural networks, softmax is one the most important activation function, maybe the most important. I had trouble understanding it...">
  
    <meta property="twitter:image" content="http://localhost:4000/img/leonids-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="http://localhost:4000/articles/2019-05/softmax-and-its-gradient">
  <meta property="og:title" content="Softmax and its Gradient | Slowbreathing">
  <meta property="og:description" content="From the perspective of Deep Neural networks, softmax is one the most important activation function, maybe the most important. I had trouble understanding it...">
  
    <meta property="og:image" content="http://localhost:4000/img/leonids-logo.png">
  
  <title>Softmax and its Gradient | Slowbreathing</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="http://localhost:4000/css/font-awesome.min.css">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">

  <link rel="canonical" href="http://localhost:4000/articles/2019-05/softmax-and-its-gradient">
  <link rel="alternate" type="application/rss+xml" title="Slowbreathing" href="http://localhost:4000/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.png">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142206738-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-142206738-1');
  </script>-->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Slowbreathing" />
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="http://localhost:4000/">
    <img src="http://localhost:4000/img/leonids-logo.png" alt="" class="avatar">
  </a>
  
  <a href="http://localhost:4000/" class="author_name">Mohit Kumar</a>
  <span class="author_job">Researcher/Consultant/Trainer</span>
  <span class="author_bio mbm">Programming is more than just typing.</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="http://localhost:4000/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="http://localhost:4000/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="http://localhost:4000/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="http://localhost:4000/resume/">about me</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/tags/">Tags</a>
      </li>
                 
    </ul>
  </nav>
  <script type="text/javascript">
  // based on https://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('mohit.riverstone@gmail.com', 'Hello from website');</script>
      </li>
    
    <li><a href="https://twitter.com/stillwaters_ia" class="social-link-item" target="_blank"><i class="fa fa-fw fa-twitter"></i></a></li>
    <li><a href="https://facebook.com/mohit.kumar.965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-facebook"></i></a></li>
    
    <li><a href="https://linkedin.com/in/mohit-kumar-05621b62" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    <li><a href="https://instagram.com/slowbreathing.github.io" class="social-link-item" target="_blank"><i class="fa fa-fw fa-instagram"></i></a></li>
    
    <li><a href="https://github.com/Slowbreathing" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    <li><a href="https://www.pinterest.com/mohitkumar965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-pinterest"></i></a></li>
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "http://localhost:4000/" >
  Home
</a>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>





<div id="post">
  <header class="post-header">
    <h1 title="Softmax and its Gradient">Softmax and its Gradient</h1>
    <span class="post-meta">
      <span class="post-date">
        1 MAY 2019
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    7 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <p>From the perspective of Deep Neural networks, softmax is one the most important activation function, maybe the most important. I had trouble understanding it in the beginning, especially its why its chosen, its gradient, its relationship with cross-entropy loss and the combined gradient.
There are many softmax resources available on the internet. Among the many that are available, I found the link <a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">here</a> to be the most complete. In this article, I “dumb” it down further and add code to the theory. Code, when associated with theory, makes the explanation very precise. <strong>But, there is another selfish reason for reproducing these here, I will be able to refer to these while explaining more complex concepts like LSTMs, NMT, BERT, etc.</strong></p>

<p>Softmax is essentially a vector function. It takes n inputs and produces and n outputs. <strong>The out can be interpreted as a probabilistic output(summing up to 1). A multiway shootout if you will.<strong></strong></strong></p>

<script type="math/tex; mode=display">\Large softmax(a)=\begin{bmatrix}
a_1\\
a_2\\
\cdots\\
a_N
\end{bmatrix}\rightarrow \begin{bmatrix}
S_1\\
S_2\\
\cdots\\
S_N
\end{bmatrix}</script>

<p>And the actual per-element formula is:</p>

<script type="math/tex; mode=display">\Large softmax_j = \frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}</script>

<p>As one can see the output function can only be positive because of the exponential and the values range between 0 and 1. Also, the value appears in the denominator summed up with other positive numbers.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="c">#row represents num classes but they may be real numbers</span>
    <span class="c">#So the shape of input is important</span>
    <span class="c">#([[1, 3, 5, 7],</span>
    <span class="c">#  [1,-9, 4, 8]]</span>

    <span class="c">#softmax will be for each of the 2 rows</span>
    <span class="c">#[[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]</span>
    <span class="c">#[8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]</span>
    <span class="c">#respectively But if the input is Tranposed clearly the answer</span>
    <span class="c">#will be wrong.</span>

    <span class="c">#That needs to be converted to probability</span>
    <span class="c">#column represents the vocabulary size.</span>

    <span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">predsl</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">logits</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
        <span class="c">#print("inputs:",inputs)</span>
        <span class="n">predsl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">inputs</span><span class="p">))))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predsl</span><span class="p">)</span></code></pre></figure>

<p>Above is a <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/common.py#L18-L38">softmax</a> implementation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"x:"</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sm</span><span class="o">=</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"softmax:"</span><span class="p">,</span><span class="n">sm</span><span class="p">)</span>
    <span class="c">#prints out</span>
    <span class="c">#[[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01],</span>
    <span class="c">#[8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]</span></code></pre></figure>

<p>Above is a <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/softmaxtest.py#L10-L19">softmaxtest</a> implementation.</p>

<p>The softmax function is very similar to the Logistic regression cost function. The only difference being that the sigmoid makes the output binary interpretable whereas, softmax’s output can be interpreted as a multiway shootout. With the above two rows individually summing up to one.</p>

<h3 id="softmax-derivative">Softmax Derivative</h3>
<p>Before diving into computing the derivative of softmax, let’s start with some preliminaries from vector calculus.</p>

<p>Softmax is fundamentally a vector function. It takes a vector as input and produces a vector as output; in other words, it has multiple inputs and multiple outputs. Therefore, we cannot just ask for “the derivative of softmax”; We should instead specify:</p>

<ol>
  <li>Which component (output element) of softmax we’re seeking to find the derivative of.</li>
  <li>Since softmax has multiple inputs, with respect to which input element the partial derivative is being computed.
This is exactly why the notation of vector calculus was developed. What we’re looking for is the partial derivatives:</li>
</ol>

<script type="math/tex; mode=display">\Large \frac{\partial softmax_i}{\partial a_j}</script>

<p>This is the partial derivative of the i-th output w.r.t. the j-th input. A shorter way to write it that we’ll be using going forward is:
Since softmax is a  function, the most general derivative we compute for it is the Jacobian matrix:</p>

<script type="math/tex; mode=display">\Large Dsoftmax=\begin{bmatrix}
D_1 softmax_1\:\:\:\:\:  \cdots\:\:\:\:\: D_N softmax_1 \\
\vdots\:\:\:\:\: \ddots\:\:\:\:\: \vdots \\
D_1 softmax_N\:\:\:\:\: \cdots\:\:\:\:\: D_N softmax_N
\end{bmatrix}</script>

<p>Let’s compute <script type="math/tex">D_jsoftmax_i</script> for arbitrary i and j:</p>

<p><script type="math/tex">\large D_jsoftmax_i=\Large \:\:\frac{\partial softmax_i}{\partial a_j}\:\:= \frac{\partial \frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j}</script><br />
Using the quotient rule <script type="math/tex">\Large f(x) = \frac{g(x)}{h(x)}</script> , <script type="math/tex">\Large f'(x) = \frac{g'(x)h(x) - h'(x)g(x)}{h(x)^2}</script> <br />
in our case <script type="math/tex">\Large g_i = e^{a_i}</script><br />
                   <script type="math/tex">\Large h_i = \sum_{k=1}^{N} e^{a_k}</script> simplifying <script type="math/tex">\Large \sum</script> for <script type="math/tex">\Large \sum_{k=1}^{N} e^{a_k}</script></p>

<p>Note that no matter which <script type="math/tex">a_j</script> we compute the derivative of <script type="math/tex">h_i</script> for, the answer will always be <script type="math/tex">e^{a_j}</script>. This is not the case for <script type="math/tex">g_i</script>, howewer. The derivative of <script type="math/tex">g_i</script> w.r.t. <script type="math/tex">a_j</script> is <script type="math/tex">e^{a_j}</script> only if i=j, because only then <script type="math/tex">g_i</script> has <script type="math/tex">a_j</script> anywhere in it. Otherwise, the derivative is 0.</p>

<ol>
  <li>
    <p>Going back to our <script type="math/tex">\Large \frac{\partial softmax_i}{\partial a_j}</script> we’ll start with the <script type="math/tex">\Large i=j</script> case. <br />
Then, using the quotient rule we have:
<script type="math/tex">\Large \frac{\partial\frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j} = \frac{e^{a_i}.\sum-e^{a_j}.e^{a_i}}{\sum^2}</script><br />
                         <script type="math/tex">\Large = \frac{e^{a_i}}{\sum}.\frac{\sum-e^{a_j}}{\sum}</script><br />
                         <script type="math/tex">\Large = softmax_i(1-softmax_j)</script></p>
  </li>
  <li>
    <p>similarly for <script type="math/tex">\Large i \neq j</script><br />
<script type="math/tex">\Large \frac{\partial\frac{e^{a_j}}{\sum_{k=1}^{N} e^{a_k}}}{\partial a_j} = \frac{0- e^{a_j}e^{a_i}}{\sum^2}</script><br />
                         <script type="math/tex">\Large = \frac{- e^{a_j}}{\sum}.\frac{e^{a_i}}{\sum}</script><br />
                         <script type="math/tex">\Large = {-softmax_j}.{softmax_i}</script></p>
  </li>
</ol>

<p>To summarize:
<a name="eq-1"></a><br />
<script type="math/tex">% <![CDATA[
\Large D_jsoftmax_i=\begin{Bmatrix}
softmax_i(1-softmax_j) & i= j \\
{-softmax_j}.{softmax_i} & i\neq j
\end{Bmatrix} \:\:\:\: eq(1) %]]></script></p>

<h3 id="softmax-derivative-implementation">Softmax Derivative Implementation</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">_softmax_grad</span><span class="p">(</span><span class="n">sm</span><span class="p">):</span>
    <span class="c"># Below is the softmax value for [1, 3, 5, 7]</span>
    <span class="c"># [2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]</span>
    <span class="c"># initialize the 2-D jacobian matrix.</span>
    <span class="n">jacobian_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sm</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">jacobian_m</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">jacobian_m</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">j</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"equal:"</span><span class="p">,</span><span class="n">i</span><span class="p">,</span> <span class="n">sm</span><span class="p">[</span><span class="n">i</span><span class="p">],(</span><span class="mi">1</span><span class="o">-</span><span class="n">sm</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
                <span class="c">#equal: 0 0.002144008783584634 0.9978559912164153</span>
                <span class="c">#equal: 1 0.015842201178506925 0.9841577988214931</span>
                <span class="c">#equal: 2 0.11705891323853292 0.8829410867614671</span>
                <span class="c">#equal: 3 0.8649548767993754 0.13504512320062456</span>
                <span class="n">jacobian_m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sm</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sm</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"not equal:"</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">sm</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">sm</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
                <span class="c">#not equal: 0 1 0.002144008783584634 0.015842201178506925</span>
                <span class="c">#not equal: 0 2 0.002144008783584634 0.11705891323853292</span>
                <span class="c">#not equal: 0 3 0.002144008783584634 0.8649548767993754</span>

                <span class="c">#not equal: 1 0 0.015842201178506925 0.002144008783584634</span>
                <span class="c">#not equal: 1 2 0.015842201178506925 0.11705891323853292</span>
                <span class="c">#not equal: 1 3 0.015842201178506925 0.8649548767993754</span>

                <span class="c">#not equal: 2 0 0.11705891323853292 0.002144008783584634</span>
                <span class="c">#not equal: 2 1 0.11705891323853292 0.015842201178506925</span>
                <span class="c">#not equal: 2 3 0.11705891323853292 0.8649548767993754</span>

                <span class="c">#not equal: 3 0 0.8649548767993754 0.002144008783584634</span>
                <span class="c">#not equal: 3 1 0.8649548767993754 0.015842201178506925</span>
                <span class="c">#not equal: 3 2 0.8649548767993754 0.11705891323853292</span>
                <span class="n">jacobian_m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">sm</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">sm</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

    <span class="c">#finally resulting in</span>
    <span class="c">#[[ 2.13941201e-03 -3.39658185e-05 -2.50975338e-04 -1.85447085e-03]</span>
    <span class="c">#[-3.39658185e-05  1.55912258e-02 -1.85447085e-03 -1.37027892e-02]</span>
    <span class="c">#[-2.50975338e-04 -1.85447085e-03  1.03356124e-01 -1.01250678e-01]</span>
    <span class="c">#[-1.85447085e-03 -1.37027892e-02 -1.01250678e-01  1.16807938e-01]]</span>

    <span class="k">return</span> <span class="n">jacobian_m</span></code></pre></figure>

<p>The above is the softmax grad code. As you can see it initializes a diagonal matrix that is then populated with the right values. On the main diagonal it has the values for case (i=j) and (i!=j) elsewhere. This is illustrated in the picture below.</p>

<figure>
    
    <img src="/img/softmaxgrad.png" alt="Image: figure-1" hight="120%" width="120%" />
    
    <figcaption><center><span class="faded_text">figure-1</span></center></figcaption>
</figure>

<h3 id="summary">Summary</h3>
<p>As you can see the softmax gradient <strong>producers an nxn matrix for input size of n</strong>. Hopefully, you got a good idea of softmax and its implementation. Hopefully, you got a good idea of softmax’s gradient and its implementation. Softmax is usually used along with cross_entropy_loss, but not always. There are few a instances like “<a href="https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/attention.py">Attention</a>”. More on <a href="https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/attention.py">Attention</a> in a much later article. But for now, what is the relationship between softmax and cross_entropy_loss function. This will be illustrated in the next article.</p>


  </article>
</div>

<h3>Connect with me</h3>
Hi People, Thanks a ton for your feedback and response. If you find anything interesting or would want to connect, drop me line using the side bar.
As some of you had requested, Login has been removed from subscription form.
<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/articles/2019-05/softmax-and-its-gradient" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/articles/2019-05/softmax-and-its-gradient" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/articles/2019-05/softmax-and-its-gradient" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=http://localhost:4000/articles/2019-05/softmax-and-its-gradient" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=http://localhost:4000/articles/2019-05/softmax-and-its-gradient" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->
<script src="https://apis.google.com/js/plusone.js">
</script>

<script src="https://utteranc.es/client.js"
        repo="slowbreathing/slowbreathing.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



        <footer>
  <strong>MIT License</strong> &copy; 2021 Mohit Kumar.
  <div class="newsletter-container" >
                <h4 class="newsletter-title">Subscribe</h4>
                <!-- <p class="newsletter-text"></p> -->
                <script type="text/javascript">var submitted=false;</script>
                <iframe name="hidden_iframe" id="hidden_iframe" style="display:none;"onload="if(submitted) {window.location='thankyou.html';}"></iframe>
                <form class="newsletter-form" name="gform" id="gform" enctype="text/plain" action="https://docs.google.com/forms/d/e/1FAIpQLSftY1olpvfsyxItLcV6kNsRSUQf3NQfPSL-RyT191nhPrfguA/formResponse"
                                                   target="hidden_iframe" onsubmit="submitted=true">
                    <p class="newsletter-text">Get new posts to your inbox</p>
                    <input class="newsletter-email" type="text" name="entry.1045781291" placeholder="name@example.com" />
                    <input class="newsletter-submit" type="submit" value="Subscribe" />
                </form>
  </div>
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="http://localhost:4000/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="http://localhost:4000/js/main.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-142206738-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
