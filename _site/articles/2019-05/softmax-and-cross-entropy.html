<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="The marriage of Softmax and CrossEntropy. This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and ...">
  <meta name="keywords" content="blog and jekyll">
  <meta name="author" content="Softmax and Cross-entropy | Slowbreathing">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Softmax and Cross-entropy | Slowbreathing">
  <meta name="twitter:description" content="The marriage of Softmax and CrossEntropy. This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and ...">
  
    <meta property="twitter:image" content="http://localhost:4000/img/leonids-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="http://localhost:4000/articles/2019-05/softmax-and-cross-entropy">
  <meta property="og:title" content="Softmax and Cross-entropy | Slowbreathing">
  <meta property="og:description" content="The marriage of Softmax and CrossEntropy. This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and ...">
  
    <meta property="og:image" content="http://localhost:4000/img/leonids-logo.png">
  
  <title>Softmax and Cross-entropy | Slowbreathing</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="http://localhost:4000/css/font-awesome.min.css">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">

  <link rel="canonical" href="http://localhost:4000/articles/2019-05/softmax-and-cross-entropy">
  <link rel="alternate" type="application/rss+xml" title="Slowbreathing" href="http://localhost:4000/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.png">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142206738-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-142206738-1');
  </script>-->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Slowbreathing" />
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="http://localhost:4000/">
    <img src="http://localhost:4000/img/leonids-logo.png" alt="" class="avatar">
  </a>
  
  <a href="http://localhost:4000/" class="author_name">Mohit Kumar</a>
  <span class="author_job">Researcher/Consultant/Trainer</span>
  <span class="author_bio mbm">Programming is more than just typing.</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="http://localhost:4000/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="http://localhost:4000/archive/">Archive</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="http://localhost:4000/resume/">about me</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/tags/">Tags</a>
      </li>
               
    </ul>
  </nav>
  <script type="text/javascript">
  // based on https://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('mohit.riverstone@gmail.com', 'Hello from website');</script>
      </li>
    
    <li><a href="https://twitter.com/stillwaters_ia" class="social-link-item" target="_blank"><i class="fa fa-fw fa-twitter"></i></a></li>
    <li><a href="https://facebook.com/mohit.kumar.965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-facebook"></i></a></li>
    
    <li><a href="https://linkedin.com/in/mohit-kumar-36050a65" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    <li><a href="https://instagram.com/slowbreathing.github.io" class="social-link-item" target="_blank"><i class="fa fa-fw fa-instagram"></i></a></li>
    
    <li><a href="https://github.com/Slowbreathing" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    <li><a href="https://www.pinterest.com/mohitkumar965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-pinterest"></i></a></li>
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "http://localhost:4000/" >
  Home
</a>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>





<div id="post">
  <header class="post-header">
    <h1 title="Softmax and Cross-entropy">Softmax and Cross-entropy</h1>
    <span class="post-meta">
      <span class="post-date">
        3 MAY 2019
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    7 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <h3 id="introduction">Introduction</h3>
<p>This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and predicted value(yhat). Cross entropy is a loss function that is defined as <script type="math/tex">\Large E = -y .log ({\hat{Y}})</script> where <script type="math/tex">\Large E</script>, is defined as the error, <script type="math/tex">\Large y</script> is the label and <script type="math/tex">\Large \hat{Y}</script> is defined as the <script type="math/tex">\Large softmax_j (logits)</script> and logits are the weighted sum. One of the reasons to choose cross-entropy alongside softmax is that because softmax has an exponential element inside it. A cost function that has an element of the natural log will provide for a convex cost function. This is similar to logistic regression which uses sigmoid. Mathematically expressed as below.</p>

<script type="math/tex; mode=display">\Large \hat{Y}=softmax_j (logits)</script>

<script type="math/tex; mode=display">\Large E = -y .log ({\hat{Y}})</script>

<p>where <script type="math/tex">\Large y</script> is the label and Yhat is <script type="math/tex">\Large \hat{Y}</script> the predicted value.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
  <span class="s">"""
  One at a time.
  args:
      pred-(seq=1),input_size
      labels-(seq=1),input_size
  """</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">1</span></code></pre></figure>

<p>Above is a <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/common.py#L8-L16">loss</a> implementation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
                <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">sm</span><span class="o">=</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="c">#prints out 0.145</span>
  <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">sm</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">input_one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
  <span class="c">#prints out 17.01</span>
  <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">sm</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">input_one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">2</span></code></pre></figure>

<p>As you can see the loss function accepts softmaxed input and one-hot encoded labels which is being passed in <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/softmaxtest.py#L26-L38">Listing-2</a>.
The output is illustrated in figure-1 and 2 below.</p>

<blockquote>
  <ul>
    <li>Loss function: Example-1</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/cel.jpg" alt="Image: figure-1:Cost is low because, the prediction is closer to the truth." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-1:Cost is low because, the prediction is closer to the truth.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Loss function: Example-2</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/cel2.jpg" alt="Image: figure-2:Cost is high because, the prediction is far away from the truth." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-2:Cost is high because, the prediction is far away from the truth.</span></center></figcaption>
</figure>

<h3 id="crossentropyloss">CrossEntropyLoss</h3>
<p>CrossEntropyLoss Function is the same loss function above but simplified and adapted for calculating the loss for multiple time steps as is usually required in RNNs. In fact, it calls the same loss function internally. In the above example, we are making 2 comparisons because we are passing 2 sets of logits(x) and 2 labels(y). The labels further have to be adapted into a one-hot of 4 so that they can be compared. Assuming that the above 2 comparisons are for 2 timesteps, the above results can be achieved by calling the <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/common.py#L117-L136">CrossEntropyLoss</a> function that calculates the softmax internally.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
    <span class="s">"""
    Does an internal softmax before loss calculation.
    args:
        pred- batch,seq,input_size
        labels-batch,seq(has to be transformed before comparision with preds(line-133).)
    """</span>
    <span class="n">checkdatadim</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">checkdatadim</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">pred</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">yhat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">batnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">seqnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
            <span class="n">yhat</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">]</span><span class="o">=</span><span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">size</span><span class="p">]))</span>
    <span class="n">lossesa</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">batnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">seqnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
            <span class="n">lossesa</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">]</span><span class="o">=</span><span class="n">loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">yhat</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">input_one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">],</span><span class="n">size</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">yhat</span><span class="p">,</span><span class="n">lossesa</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">3</span></code></pre></figure>

<p>The point to keep in mind is, it accepts it’s 2 inputs in 3(batch,seq,input_size) and 2(batch,seq) dimensions respectively. Batch size usually indicates multiple parallel input sequences, can be ignored for now and be assumed as 1. The shape of pred in our case is batch=1,seq=2,input_size=4. And the shape of labels is batch=1, seq=2. This is illustrated in Listing-3 and Listing-4</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
                <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

  <span class="c">#prints array([[ 0.14507794, 17.01904505]]))</span>
  <span class="n">softmaxed</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"loss:"</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">4</span></code></pre></figure>

<p>As illustrated in <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/common.py#L117-L136">Listing-3</a> and <a href="https://github.com/slowbreathing/Deep-Breathe">Listing-4</a><a href="https://github.com/slowbreathing/Deep-Breathe">Deep-Breathe</a> version of cross_entropy_loss function returns a tuple of softmaxed output that it calculates internally(only for convenience) and the Loss. The calculated loss, as expected, is the same as before as was while calling the loss function directly.</p>

<h3 id="crossentropyloss-derivative">CrossEntropyLoss Derivative</h3>
<p>One of the tricks I have learnt to get back-propagation right is to write the equations backwards. This becomes especially useful when the model is more complex in later articles. A trick that I use a lot.</p>

<script type="math/tex; mode=display">\Large \hat{Y}=softmax_j (logits)</script>

<script type="math/tex; mode=display">\Large E = -y .log ({\hat{Y}})</script>

<p>The above equation is written like so below while calculating the gradients.</p>

<script type="math/tex; mode=display">\Large E = -y .log ({\hat{Y}})</script>

<script type="math/tex; mode=display">\Large \hat{Y}=softmax_j (logits)</script>

<blockquote>
  <ul>
    <li>Gradient: Example-3</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/cel3.jpg" alt="Image: figure-3:The red arrow follows the gradient." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-3:The red arrow follows the gradient.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Deriving the gradients now.</li>
  </ul>
</blockquote>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = \frac{\partial {E}}{\partial {\hat{Y}}}.\frac{\partial {\hat{Y}}}{\partial {logits}}</script>

<h1><a name="eq-2"></a></h1>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\frac{\partial {\hat{Y}}}{\partial {logits}} \:\:\:\: eq(2)</script>

<blockquote>
  <ul>
    <li>For calculating <script type="math/tex">\Large \frac{\partial {\hat{Y}}}{\partial {logits}}</script> where <script type="math/tex">\Large \hat{y}</script> is the softmax and its derivative is given by <a href="softmax-and-its-gradient#eq-1">eq-1</a>. Combining <a href="softmax-and-its-gradient#eq-1">eq-1</a> and <a href="softmax-and-cross-entropy#eq-2">eq-2</a>.</li>
  </ul>
</blockquote>

<script type="math/tex; mode=display">% <![CDATA[
\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\begin{Bmatrix}
softmax_i(1-softmax_j) & i= j \\
{-softmax_j}.{softmax_i} & i\neq j
\end{Bmatrix} %]]></script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\hat{y_t}.(1-\hat{y_t}) + \sum_{i\neq j}(\frac{-y_t}{\hat{y_t}})(-\hat{y_t}\hat{y_t})</script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = -y.(1-\hat{y_t}) + \sum_{i\neq j}y_t.\hat{y_t}</script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = -y+y.\hat{y_t} + \sum_{i\neq j}y_t.\hat{y_t}</script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = y.\hat{y_t} + \sum_{i\neq j}y_t.\hat{y_t} -y</script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = \hat{y_t}.(y + \sum_{i\neq j}y_t) -y</script>

<p>Since Y is a one hot vector, the term “<script type="math/tex">\Large (y + \sum_{i\neq j}y_t)</script>” sums up to one.</p>

<h1 id="-1"><a name="eq-3"></a></h1>
<p><script type="math/tex">\Large \frac{\partial {E}}{\partial {logits}} = (\hat{y_t} -y) \:\:\:\: eq(3)</script></p>

<h3 id="crossentropyloss-derivative-implementation">CrossEntropyLoss Derivative implementation</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="n">softmaxed</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"loss:"</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

  <span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">target_one_hot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>
  <span class="c"># Adapting the shape of Y</span>
  <span class="k">for</span> <span class="n">batnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
        <span class="n">target_one_hot</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">input_one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">i</span><span class="p">],</span><span class="n">size</span><span class="p">)</span>
  <span class="n">dy</span> <span class="o">=</span> <span class="n">softmaxed</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
  <span class="n">dy</span> <span class="o">=</span> <span class="n">dy</span> <span class="o">-</span> <span class="n">target_one_hot</span>
  <span class="c"># prints out gradient: [[[ 2.14400878e-03  1.58422012e-02  1.17058913e-01 -1.35045123e-01]</span>
  <span class="c">#                        [ 8.94679461e-04 -9.99999959e-01  1.79701173e-02  9.81135163e-01]]]</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"gradient:"</span><span class="p">,</span><span class="n">dy</span><span class="p">)</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">5</span></code></pre></figure>

<p><a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/softmaxtest.py#L52-L62">Listing-5</a></p>

<h3 id="summary">Summary</h3>
<p>As you can see the idea behind softmax and cross_entropy_loss and their combined use and implementation. Also, their combined gradient derivation is one of the most used formulas in deep learning. Implemented code often lends perspective into theory as you see the various shapes of input and output. Hopefully, cross_entropy_loss’s combined gradient in Listing-5 does the same.</p>


  </article>
</div>

Connect with Me:
If you find anything interesting, do connect with me.

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->
<script src="https://apis.google.com/js/plusone.js">
</script>

<script src="https://utteranc.es/client.js"
        repo="slowbreathing/slowbreathing.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



        <footer>
  <strong>MIT License</strong> &copy; 2019 Mohit Kumar.
  <div class="newsletter-container" >
                <h4 class="newsletter-title">Subscribe</h4>
                <!-- <p class="newsletter-text"></p> -->
                <script type="text/javascript">var submitted=false;</script>
                <iframe name="hidden_iframe" id="hidden_iframe" style="display:none;"onload="if(submitted) {window.location='thankyou.html';}"></iframe>
                <form class="newsletter-form" name="gform" id="gform" enctype="text/plain" action="https://docs.google.com/forms/d/e/1FAIpQLSftY1olpvfsyxItLcV6kNsRSUQf3NQfPSL-RyT191nhPrfguA/formResponse"
                                                   target="hidden_iframe" onsubmit="submitted=true">

                    <p class="newsletter-text">Get new posts to your inbox</p>

                    <input class="newsletter-email" type="text" name="entry.1045781291" placeholder="name@example.com" />

                    <input class="newsletter-submit" type="submit" value="Subscribe" />

                </form>

  </div>




</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="http://localhost:4000/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="http://localhost:4000/js/main.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-142206738-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
