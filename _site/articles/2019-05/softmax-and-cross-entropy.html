<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="The marriage of Softmax and CrossEntropy. This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and ...">
  <meta name="keywords" content="blog and jekyll">
  <meta name="author" content="Softmax and Cross-entropy | Slowbreathing">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Softmax and Cross-entropy | Slowbreathing">
  <meta name="twitter:description" content="The marriage of Softmax and CrossEntropy. This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and ...">
  
    <meta property="twitter:image" content="http://localhost:4000/img/leonids-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="http://localhost:4000/articles/2019-05/softmax-and-cross-entropy">
  <meta property="og:title" content="Softmax and Cross-entropy | Slowbreathing">
  <meta property="og:description" content="The marriage of Softmax and CrossEntropy. This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and ...">
  
    <meta property="og:image" content="http://localhost:4000/img/leonids-logo.png">
  
  <title>Softmax and Cross-entropy | Slowbreathing</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="http://localhost:4000/css/font-awesome.min.css">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">

  <link rel="canonical" href="http://localhost:4000/articles/2019-05/softmax-and-cross-entropy">
  <link rel="alternate" type="application/rss+xml" title="Slowbreathing" href="http://localhost:4000/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.png">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142206738-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-142206738-1');
  </script>-->

</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="http://localhost:4000/">
    <img src="http://localhost:4000/img/leonids-logo.png" alt="" class="avatar">
  </a>
  
  <a href="http://localhost:4000/" class="author_name">Mohit Kumar</a>
  <span class="author_job">Researcher/Consultant/Trainer</span>
  <span class="author_bio mbm">Programming is more than just typing.</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="http://localhost:4000/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="http://localhost:4000/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="http://localhost:4000/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="http://localhost:4000/resume/">about me</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/tags/">Tags</a>
      </li>
         
    </ul>
  </nav>
  <script type="text/javascript">
  // based on https://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('mohit.riverstone@gmail.com', 'Hello from website');</script>
      </li>
    
    
    <li><a href="https://facebook.com/mohit.kumar.965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-facebook"></i></a></li>
    
    <li><a href="https://linkedin.com/in/mohit-kumar-36050a65" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    
    
    <li><a href="https://github.com/Slowbreathing" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "http://localhost:4000/" >
  Home
</a>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>





<div id="post">
  <header class="post-header">
    <h1 title="Softmax and Cross-entropy">Softmax and Cross-entropy</h1>
    <span class="post-meta">
      <span class="post-date">
        3 MAY 2019
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    7 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <h3 id="introduction">Introduction</h3>
<p>This is a loss calculating function post the yhat(predicted value) that measures the difference between Labels and predicted value(yhat). Cross entropy is a loss function that is defined as <script type="math/tex">\Large E = -y .log ({\hat{Y}})</script> where <script type="math/tex">\Large E</script>, is defined as error, <script type="math/tex">\Large y</script> is the label and <script type="math/tex">\Large \hat{Y}</script> is defined as <script type="math/tex">\Large softmax_j (logits)</script> and logits are weighted sum. One of the reason to choose cross entropy alongside softmax is that because softmax has an exponential element inside it the, an element log provides for convex cost function. This is similar to logistic regression which uses sigmoid. Mathematically expressed as below.</p>

<script type="math/tex; mode=display">\Large \hat{Y}=softmax_j (logits)</script>

<script type="math/tex; mode=display">\Large E = -y .log ({\hat{Y}})</script>

<p>where <script type="math/tex">\Large y</script> is the label and Yhat <script type="math/tex">\Large \hat{Y}</script> the predicted value.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
  <span class="s">"""
  One at a time.
  args:
      pred-(seq=1),input_size
      labels-(seq=1),input_size
  """</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">1</span></code></pre></figure>

<p>Above is a <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/common.py#L8-L16">loss</a> implementation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
                <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">sm</span><span class="o">=</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="c">#prints out 0.145</span>
  <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">sm</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">input_one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
  <span class="c">#prints out 17.01</span>
  <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">sm</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">input_one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">2</span></code></pre></figure>

<p>As you can see the loss function accepts softmaxed input and one-hot encoded labels which is being passed in <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/softmaxtest.py#L26-L38">Listing-2</a>.
The output is illustated in figure-1 and 2 below.</p>

<blockquote>
  <ul>
    <li>Loss function: Example-1</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/cel.jpg" alt="Image: figure-1:Cost is low because, the prediction is closer to the truth." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-1:Cost is low because, the prediction is closer to the truth.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Loss function: Example-2</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/cel2.jpg" alt="Image: figure-2:Cost is high because, the prediction is far away from the truth." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-2:Cost is high because, the prediction is far away from the truth.</span></center></figcaption>
</figure>

<h3 id="crossentropyloss">CrossEntropyLoss</h3>
<p>CrossEntropyLoss Function is the same loss function above but simplified and adapted for calculating the loss for multiple time steps as is usually required in RNNs. Infact it calls the same loss function internally. In the above example we are making 2 comparisions because we are passing 2 sets of logits(x) and 2 labels(y). The labels further have to be adapted into a one-hot of 4 so that they can be compared. Assuming that the abobe 2 comparisions are for 2 time timesteps, the abobe results can be achieved by calling the the <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/common.py#L117-L136">CrossEntropyLoss</a> function that calculates the softmax internally.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
    <span class="s">"""
    Does an internal softmax before loss calculation.
    args:
        pred- batch,seq,input_size
        labels-batch,seq(has to be transformed before comparision with preds(line-133).)
    """</span>
    <span class="n">checkdatadim</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">checkdatadim</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">pred</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">yhat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">batnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">seqnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
            <span class="n">yhat</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">]</span><span class="o">=</span><span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">size</span><span class="p">]))</span>
    <span class="n">lossesa</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">batnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">seqnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
            <span class="n">lossesa</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">]</span><span class="o">=</span><span class="n">loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">yhat</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">input_one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">seqnum</span><span class="p">],</span><span class="n">size</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">yhat</span><span class="p">,</span><span class="n">lossesa</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">3</span></code></pre></figure>

<p>The point to keep in mind is, it accepts it’s 2 inputs in 3(batch,seq,input_size) and 2(batch,seq) dimensions respectively. Batch size usually indicates multiple parallel input sequences, can be ignored for now and be assumed as 1. The shape of pred in our case is batch=1,seq=2,input_size=4. And the shape of labels is batch=1, seq=2. This is illustrated in Listing-3 and Listing-4</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
                <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

  <span class="c">#prints array([[ 0.14507794, 17.01904505]]))</span>
  <span class="n">softmaxed</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"loss:"</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">4</span></code></pre></figure>

<p>As illustrated in <a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/common.py#L117-L136">Listing-3</a> and <a href="https://github.com/slowbreathing/Deep-Breathe">Listing-4</a><a href="https://github.com/slowbreathing/Deep-Breathe">Deep-Breathe</a> version of cross_entropy_loss function returns a tuple of softmaxed output that it calculates internally(only for convenience) and the Loss. The calculated loss, as expected, is the same as before as was while calling the loss function directly.</p>

<h3 id="crossentropyloss-derivative">CrossEntropyLoss Derivative</h3>
<p>One of the tricks I have learnt to get back propagation right is to write the equations backwards. This becomes especially useful when the model is more complex in later articles. A trick that I use a lot.</p>

<script type="math/tex; mode=display">\Large \hat{Y}=softmax_j (logits)</script>

<script type="math/tex; mode=display">\Large E = -y .log ({\hat{Y}})</script>

<p>The above equation is writen like so below while calculating the gradients.</p>

<script type="math/tex; mode=display">\Large E = -y .log ({\hat{Y}})</script>

<script type="math/tex; mode=display">\Large \hat{Y}=softmax_j (logits)</script>

<blockquote>
  <ul>
    <li>Gradient: Example-3</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/cel3.jpg" alt="Image: figure-3:The red arrow follows the gradient." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-3:The red arrow follows the gradient.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Deriving the gradients now.</li>
  </ul>
</blockquote>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = \frac{\partial {E}}{\partial {\hat{Y}}}.\frac{\partial {\hat{Y}}}{\partial {logits}}</script>

<h1><a name="eq-2"></a></h1>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\frac{\partial {\hat{Y}}}{\partial {logits}} \:\:\:\: eq(2)</script>

<blockquote>
  <ul>
    <li>For calculating <script type="math/tex">\Large \frac{\partial {\hat{Y}}}{\partial {logits}}</script> where <script type="math/tex">\Large \hat{y}</script> is the softmax and its derivative is given by <a href="softmax-and-its-gradient#eq-1">eq-1</a>. Combining <a href="softmax-and-its-gradient#eq-1">eq-1</a> and <a href="softmax-and-cross-entropy#eq-2">eq-2</a>.</li>
  </ul>
</blockquote>

<script type="math/tex; mode=display">% <![CDATA[
\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\begin{Bmatrix}
softmax_i(1-softmax_j) & i= j \\
{-softmax_j}.{softmax_i} & i\neq j
\end{Bmatrix} %]]></script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = -\frac{y}{\hat{y}}.\hat{y_t}.(1-\hat{y_t}) + \sum_{i\neq j}(\frac{-y_t}{\hat{y_t}})(-\hat{y_t}\hat{y_t})</script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = -y.(1-\hat{y_t}) + \sum_{i\neq j}y_t.\hat{y_t}</script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = -y+y.\hat{y_t} + \sum_{i\neq j}y_t.\hat{y_t}</script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = y.\hat{y_t} + \sum_{i\neq j}y_t.\hat{y_t} -y</script>

<script type="math/tex; mode=display">\Large \frac{\partial {E}}{\partial {logits}} = \hat{y_t}.(y + \sum_{i\neq j}y_t) -y</script>

<p>Since Y is a one hot vector, the term “<script type="math/tex">\Large (y + \sum_{i\neq j}y_t)</script>” sums up to one.</p>

<h1 id="-1"><a name="eq-3"></a></h1>
<p><script type="math/tex">\Large \frac{\partial {E}}{\partial {logits}} = (\hat{y_t} -y) \:\:\:\: eq(3)</script></p>

<h3 id="crossentropyloss-derivative-implementation">CrossEntropyLoss Derivative implementation</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="n">softmaxed</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"loss:"</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

  <span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">target_one_hot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span><span class="n">seq</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>
  <span class="c"># Adapting the shape of Y</span>
  <span class="k">for</span> <span class="n">batnum</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
        <span class="n">target_one_hot</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">input_one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">batnum</span><span class="p">][</span><span class="n">i</span><span class="p">],</span><span class="n">size</span><span class="p">)</span>
  <span class="n">dy</span> <span class="o">=</span> <span class="n">softmaxed</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
  <span class="n">dy</span> <span class="o">=</span> <span class="n">dy</span> <span class="o">-</span> <span class="n">target_one_hot</span>
  <span class="c"># prints out gradient: [[[ 2.14400878e-03  1.58422012e-02  1.17058913e-01 -1.35045123e-01]</span>
  <span class="c">#                        [ 8.94679461e-04 -9.99999959e-01  1.79701173e-02  9.81135163e-01]]]</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"gradient:"</span><span class="p">,</span><span class="n">dy</span><span class="p">)</span>

<span class="n">Listing</span><span class="o">-</span><span class="mi">5</span></code></pre></figure>

<p><a href="https://github.com/slowbreathing/Deep-Breathe/blob/0c61d509643cd9abce816a9db388708fe3dc642f/org/mk/training/dl/softmaxtest.py#L52-L62">Listing-5</a></p>

<h3 id="summary">Summary</h3>
<p>As you can see idea behind softmax and cross_entropy_loss and their combined use and implementation. Also their combined gradient derivation is one the most used formulas in deep learning. Implemented code often lends perspective into theory as you see the various shapes of input and output. Hopefully cross_entropy_loss’s combined gradient in Listing-5 does the same.</p>


  </article>
</div>

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=http://localhost:4000/articles/2019-05/softmax-and-cross-entropy" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->



        <footer>
  <strong>MIT License</strong> &copy; 2019 Mohit Kumar.
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="http://localhost:4000/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="http://localhost:4000/js/main.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-142206738-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
