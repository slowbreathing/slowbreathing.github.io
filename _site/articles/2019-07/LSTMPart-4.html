<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="In this multi-part series(part-1,part-2,part-3) we look into composing LSTM into multiple higher layers and its directionality. Though Multiple layers are co...">
  <meta name="keywords" content="blog and jekyll">
  <meta name="author" content="RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs | Slowbreathing">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs | Slowbreathing">
  <meta name="twitter:description" content="In this multi-part series(part-1,part-2,part-3) we look into composing LSTM into multiple higher layers and its directionality. Though Multiple layers are co...">
  
    <meta property="twitter:image" content="http://localhost:4000/img/leonids-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="http://localhost:4000/articles/2019-07/LSTMPart-4">
  <meta property="og:title" content="RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs | Slowbreathing">
  <meta property="og:description" content="In this multi-part series(part-1,part-2,part-3) we look into composing LSTM into multiple higher layers and its directionality. Though Multiple layers are co...">
  
    <meta property="og:image" content="http://localhost:4000/img/leonids-logo.png">
  
  <title>RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs | Slowbreathing</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="http://localhost:4000/css/font-awesome.min.css">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">

  <link rel="canonical" href="http://localhost:4000/articles/2019-07/LSTMPart-4">
  <link rel="alternate" type="application/rss+xml" title="Slowbreathing" href="http://localhost:4000/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.png">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142206738-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-142206738-1');
  </script>-->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Slowbreathing" />
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="http://localhost:4000/">
    <img src="http://localhost:4000/img/leonids-logo.png" alt="" class="avatar">
  </a>
  
  <a href="http://localhost:4000/" class="author_name">Mohit Kumar</a>
  <span class="author_job">Researcher/Consultant/Trainer</span>
  <span class="author_bio mbm">Programming is more than just typing.</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="http://localhost:4000/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="http://localhost:4000/archive/">Archive</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="http://localhost:4000/resume/">about me</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/tags/">Tags</a>
      </li>
               
    </ul>
  </nav>
  <script type="text/javascript">
  // based on https://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('mohit.riverstone@gmail.com', 'Hello from website');</script>
      </li>
    
    <li><a href="https://twitter.com/stillwaters_ia" class="social-link-item" target="_blank"><i class="fa fa-fw fa-twitter"></i></a></li>
    <li><a href="https://facebook.com/mohit.kumar.965" class="social-link-item" target="_blank"><i class="fa fa-fw fa-facebook"></i></a></li>
    
    <li><a href="https://linkedin.com/in/mohit-kumar-36050a65" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    
    
    <li><a href="https://github.com/Slowbreathing" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "http://localhost:4000/" >
  Home
</a>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>





<div id="post">
  <header class="post-header">
    <h1 title="RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs">RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs</h1>
    <span class="post-meta">
      <span class="post-date">
        22 JUL 2019
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    9 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <h3 id="introduction">Introduction</h3>
<p>In this <a href="/tags/#LSTM">multi-part series</a> (<a href="/articles/2019-07/LSTMPart-1">part-1</a>,<a href="/articles/2019-07/LSTMPart-2">part-2</a>,<a href="/articles/2019-07/LSTMPart-3">part-3</a>) we look into composing LSTM into multiple higher layers and its directionality. Though <strong>Multiple layers</strong> are compute-intensive, they have better accuracy and so does <strong>bidirectional connections.</strong> More importantly, a solid understanding of the above mentioned paves the way for concepts like <strong>Highway connections</strong>, <strong>Residual Connections</strong>, <strong>Pointer networks</strong>, <strong>Encoder-Decoder</strong> Architectures and so forth in future article. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models</strong>.</p>

<h3 id="what-this-article-is-not-about">What this article is not about?</h3>
<blockquote>
  <ul>
    <li>This article will not talk about the conceptual model of LSTM, on which there is some great existing material here and here in the order of difficulty.</li>
    <li>This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome, if a somewhat <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">difficult post</a>, by Andrej.</li>
    <li>This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts <a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html">here</a> and <a href="https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577">here</a> in the order of difficulty</li>
  </ul>
</blockquote>

<h3 id="what-this-article-is-about">What this article is about?</h3>
<blockquote>
  <ul>
    <li>Using the <a href="https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0">first principles</a> we picturize/visualize <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a> and <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a> and their backward propagation.</li>
    <li>Then we use a heady mixture of intuition, Logic, and programming to prove mathematical results.</li>
  </ul>
</blockquote>

<h3 id="context">Context</h3>
<p>This is the same example and the context is the same as described in <a href="/articles/2019-07/LSTMPart-1">part-1</a>. <strong>The focus. however, this time is on <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a> and <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a></strong>.</p>

<p>There are 2 parts to this article.</p>
<ol>
  <li><a href="LSTMPart-4#Multi layer RNNs">Multi layer RNNs</a></li>
  <li><a href="LSTMPart-4#Bidirectional RNNs section">Bidirectional RNNs</a></li>
  <li><a href="LSTMPart-4#Combining Bidirectional and MultiLayerRNNs">Combining Bidirectional and MultiLayerRNNs</a></li>
</ol>

<h1><a name="Multi layer RNNs"></a></h1>
<h3 id="multi-layer-rnns">Multi layer RNNs</h3>
<ol>
  <li><a href="LSTMPart-4#Recurent depth">Recurent depth</a></li>
  <li><a href="LSTMPart-4#Feed Forward depth">Feed Forward depth</a></li>
  <li><a href="LSTMPart-4#Multi layer RNNs Forward pass">Multi layer RNNs Forward pass</a></li>
  <li><a href="LSTMPart-4#Multi layer RNNs backward propagation">Multi layer RNNs backward propagation</a></li>
</ol>

<h1 id="-1"><a name="Recurent depth"></a></h1>
<h4 id="recurent-depth">Recurent depth</h4>
<p>A quick recap, LSTM encapsulates the internal cell logic. There are many variations to Cell logic, VanillaRNN, LSTMs, GRUs, etc. What we have seen so far is they can be fed with sequential time series data. We feed in the sequence, compare with the labels, calculate errors back-propagate and adjust the weights. The length of the fed in the sequence is informally called recurrent depth.</p>

<figure>
    
    <img src="/img/rnn/multilayercell.jpg" alt="Image: figure-1: &lt;strong&gt;Recurrent depth&lt;/strong&gt;=3" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-1: <strong>Recurrent depth</strong>=3</span></center></figcaption>
</figure>

<p>Figure-1 shows the recurrent depth. Formally stated, <strong>Recurrent depth is the Longest path between the same hidden state in successive time-steps</strong>. The recurrent depth is amply clear.</p>

<h1 id="-2"><a name="Feed Forward depth"></a></h1>
<h4 id="feed-forward-depth">Feed Forward depth</h4>
<p>The topic of this article is Feed-forward depth, more akin to the depth we have in vanilla neural networks. It is the depth of a network that is generally attributed to the success of deep learning as a technique. There are downsides too, too much compute budget for one if done indiscriminately. That being said, well look at the inner workings of Multiple layers and how we set it up in code.</p>

<figure>
    
    <img src="/img/rnn/multilayercell2.jpg" alt="Image: figure-2: &lt;strong&gt;Feed forward depth&lt;/strong&gt;=3" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-2: <strong>Feed forward depth</strong>=3</span></center></figcaption>
</figure>

<p>Figure-2 shows the Feed-Forward depth. Formally stated the longest path between an input and output at the same timestep.</p>

<h1 id="-3"><a name="Multi layer RNNs Forward pass"></a></h1>
<h4 id="multi-layer-rnns-forward-pass">Multi layer RNNs Forward pass</h4>

<p>For the most part this straight forward as you have already seen in the <a href="/articles/2019-07/LSTMPart-3">previous article</a> in this series. The only difference is that there are multiple cells(2 in this example) now and how the state flows forward and gradient flows backwards.</p>

<blockquote>
  <ul>
    <li><strong><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a></strong> forward pass summary.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/multilayercell7.jpg" alt="Image: figure-3: &lt;strong&gt;Multi layer RNNs Forward pass summary&lt;/strong&gt;" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-3: <strong>Multi layer RNNs Forward pass summary</strong></span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Multi-layer RNNs Forward pass</li>
    <li>Notice the state being passed(yellow) from the first layer to the second.</li>
    <li>The softmax and cross_entropy_loss are done on the second layer output expectedly.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/multilayercell3.jpg" alt="Image: figure-4: &lt;strong&gt;Multi layer RNNs Forward pass&lt;/strong&gt;" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-4: <strong>Multi layer RNNs Forward pass</strong></span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Complete code of <strong><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a></strong> from <a href="https://github.com/slowbreathing/Deep-Breathe">DEEP-Breathe</a> helps in looking at the internals</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/multilayercell4.jpg" alt="Image: figure-5: &lt;strong&gt;DHt&lt;/strong&gt;" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-5: <strong>DHt</strong></span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a> <strong>outputs</strong> returned</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/multilayercell5.jpg" alt="Image: figure-6: &lt;strong&gt;outputs&lt;/strong&gt; returned." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-6: <strong>outputs</strong> returned.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a> <strong>states</strong> returned</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/multilayercell6.jpg" alt="Image: figure-7: &lt;strong&gt;states&lt;/strong&gt; returned." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-7: <strong>states</strong> returned.</span></center></figcaption>
</figure>

<p>Also when using <a href="https://github.com/slowbreathing/Deep-Breathe">Deep-Breathe</a> you could enable forward or backward logging for MultiRNNCell like so</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="n">cell</span><span class="o">=</span> <span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell1</span><span class="p">,</span> <span class="n">cell2</span><span class="p">],</span><span class="n">debug</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">backpassdebug</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">Listing</span><span class="o">-</span><span class="mi">1</span></code></pre></figure>

<h1 id="-4"><a name="Multi layer RNNs backward propagation"></a></h1>
<h4 id="multi-layer-rnns-backward-propagation">Multi layer RNNs backward propagation</h4>
<p>Again, for the most part, this is almost identical to standard LSTM Backward Propagation which we went through in detail in <a href="/articles/2019-07/LSTMPart-3">part-3</a>. So if you have got a good hang of it, only a few things change. But before we go on, refresh <a href="LSTMPart-3#DHX">DHX</a> and the complete <a href="LSTMPart-3#Backward propagation through LSTMCell(Step-2)">section</a> before you resume here. Here is what we have to calculate.</p>

<blockquote>
  <ul>
    <li><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a> Backward propagation summary.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/multilayercell8.jpg" alt="Image: figure-8: MultiRNNCell Backward propagation summary." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-8: MultiRNNCell Backward propagation summary.</span></center></figcaption>
</figure>

<p><strong>I’ll reproduce DHX here just to set the context and note down the changes.</strong></p>
<h6 id="standard-dhx-for-regular-lstm-cell">Standard DHX for Regular LSTM Cell</h6>
<blockquote>
  <ul>
    <li><strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>.</li>
    <li>Complete listing can be found at <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344">code-1</a></li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/lstmback26.jpg" alt="Image: figure-9: &lt;strong&gt;Dhx&lt;/strong&gt;, &lt;strong&gt;Dh_next&lt;/strong&gt;, &lt;strong&gt;dxt&lt;/strong&gt;." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-9: <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>.</span></center></figcaption>
</figure>

<h6 id="dhx-for-multirnncell-composing-lstm-cell">DHX for MultiRNNCell composing LSTM Cell</h6>
<blockquote>
  <ul>
    <li>Dhx, Dh_next, dxt as usual and then <strong>dh_next_recurr</strong> which is passed back to MultiRNNCell to be applied to the next layer.</li>
    <li>Complete listing can be found at <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344">code-1</a>. Pay careful attention to the weights shapes.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/multilayercell9.jpg" alt="Image: figure-10: &lt;strong&gt;dh_next_recurr&lt;/strong&gt; which is passed back to MultiRNNCell to be applied to the next layer." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-10: <strong>dh_next_recurr</strong> which is passed back to MultiRNNCell to be applied to the next layer.</span></center></figcaption>
</figure>

<h6 id="standard-dhx-schematically-regular-lstm-cell">Standard DHX schematically Regular LSTM Cell</h6>
<blockquote>
  <ul>
    <li>Many models like Neural Machine Translators(NMT), Bidirectional Encoder Representation from Transformers(BERT) use word embeddings as their inputs(Xs) which oftentimes need learning and that is where we need <strong>dxt</strong></li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/lstmback27.jpg" alt="Image: figure-30: &lt;strong&gt;Dhx&lt;/strong&gt;, &lt;strong&gt;Dh_next&lt;/strong&gt;, &lt;strong&gt;dxt&lt;/strong&gt;" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-30: <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong></span></center></figcaption>
</figure>

<h6 id="dhx-schematically-for-multirnncell-composing-lstm-cell">DHX schematically for MultiRNNCell composing LSTM Cell</h6>
<blockquote>
  <ul>
    <li>Dhx, Dh_next, dxt as usual and then <strong>dh_next_recurr</strong> which is passed back to MultiRNNCell to be applied to the next layer.</li>
    <li>The Complete listing can be found at <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344">code-1</a>, in particular, the <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L146-L153">MultiRNNCell “compute_gradient()”</a>. Pay careful attention to the weights’ shapes.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/multilayercell10.jpg" alt="Image: figure-10: &lt;strong&gt;dh_next_recurr&lt;/strong&gt; which is passed back to MultiRNNCell to be applied to next layer." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-10: <strong>dh_next_recurr</strong> which is passed back to MultiRNNCell to be applied to next layer.</span></center></figcaption>
</figure>

<h1 id="-5"><a name="Bidirectional RNNs section"></a></h1>
<h3 id="bidirectional-rnns">Bidirectional RNNs</h3>
<p>The bidirectional RNNs are great for accuracy but the compute budget goes up. Most advanced architectures use bidirectional for better accuracy. It has 2 cells or 2 multi-cells where the sequence is fed in forwards as well as backwards. For many sequences, language models for instance, when the sequence is fed in-reverse then it provides a bit more context of what is being said. Consider the below statements.</p>

<blockquote>
  <ul>
    <li>One of the greatest American Richard Feynman said. “If I could explain it to the average person, it wouldn’t have been worth the Nobel Prize.”</li>
    <li>One of the greatest American Richard Pryor said. “I became a performer because it was what I enjoyed doing.”</li>
  </ul>
</blockquote>

<p>The first six words of the 2 sentences are identical. But if this sequence was fed in backwards as well, then the context would have been much clearer. Setting it up much easier as illustrated in the next few figures.</p>

<blockquote>
  <ul>
    <li><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a>.</li>
    <li>Simple 2 separate cells fed in with the same sequence in opposite directions.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/bilstm0.jpg" alt="Image: figure-11: Bidirectional RNNs.&lt;strong&gt;forward cell&lt;/strong&gt;" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-11: Bidirectional RNNs.<strong>forward cell</strong></span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a></li>
    <li>The State looks identical but reversed, and that is because they have been initialized identically for comparison purposes. <strong>In practice we don’t do that</strong>.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/bilstm1.jpg" alt="Image: figure-12: Bidirectional RNNs.&lt;strong&gt;backward cell&lt;/strong&gt;" hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-12: Bidirectional RNNs.<strong>backward cell</strong></span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a> Output</li>
    <li>Tupled h-states for both the cells.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/bilstm2.jpg" alt="Image: figure-13: Bidirectional RNNs:Output." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-13: Bidirectional RNNs:Output.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a> States</li>
    <li><strong>Tupled and last (c-state and h-state) for both the cells</strong>.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/bilstm3.jpg" alt="Image: figure-14: MultiRNNCell Backward propagation summary." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-14: MultiRNNCell Backward propagation summary.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a> Pred Calculation</li>
    <li>Pred calculation is similar except the last h-state is concatenated changing the shape of Wy.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/bilstm4.jpg" alt="Image: figure-15: MultiRNNCell Backward propagation summary." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-15: MultiRNNCell Backward propagation summary.</span></center></figcaption>
</figure>

<h1 id="-6"><a name="Combining Bidirectional and MultiLayerRNNs"></a></h1>
<h3 id="combining-bidirectional-and-multilayerrnns">Combining Bidirectional and MultiLayerRNNs</h3>
<blockquote>
  <ul>
    <li>Combining <strong><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a> and <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a></strong> <strong>Output</strong>.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/bilstm5.jpg" alt="Image: figure-16: Combining Bidirectional RNNs and MultiRNNCell." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-16: Combining Bidirectional RNNs and MultiRNNCell.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Combining <strong><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a> and <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a></strong> <strong>State</strong>.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/bilstm6.jpg" alt="Image: figure-17: Combining Bidirectional RNNs and MultiRNNCell." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-17: Combining Bidirectional RNNs and MultiRNNCell.</span></center></figcaption>
</figure>

<blockquote>
  <ul>
    <li>Combining <strong><a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416">Bidirectional RNNs</a> and <a href="https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193">MultiRNNCell</a></strong> Pred Calculation.</li>
  </ul>
</blockquote>

<figure>
    
    <img src="/img/rnn/bilstm7.jpg" alt="Image: figure-18: Combining Bidirectional RNNs and MultiRNNCell." hight="110%" width="110%" />
    
    <figcaption><center><span class="faded_text">figure-18: Combining Bidirectional RNNs and MultiRNNCell.</span></center></figcaption>
</figure>

<h3 id="summary">Summary</h3>
<p>Finally, that would be the complete documentation of LSTMs inner workings. Maybe a comparison with RNNs for vanishing gradient improvement will complete it. But that apart, once we have a drop on LSTMs like we did, using them in more complex architectures will become a lot easier. In the immediate next article, we’ll look at the improvements in propagation because of vanishing and exploding gradients, if any, brought about by LSTMs over RNNs.</p>


  </article>
</div>

<h3>Connect with me</h3>
If you find anything interesting or would want to connect, drop me line using the side bar.  

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/articles/2019-07/LSTMPart-4" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/articles/2019-07/LSTMPart-4" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/articles/2019-07/LSTMPart-4" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=http://localhost:4000/articles/2019-07/LSTMPart-4" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=http://localhost:4000/articles/2019-07/LSTMPart-4" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->
<script src="https://apis.google.com/js/plusone.js">
</script>

<script src="https://utteranc.es/client.js"
        repo="slowbreathing/slowbreathing.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



        <footer>
  <strong>MIT License</strong> &copy; 2019 Mohit Kumar.
  <div class="newsletter-container" >
                <h4 class="newsletter-title">Subscribe</h4>
                <!-- <p class="newsletter-text"></p> -->
                <form class="newsletter-form" action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open('https://feedburner.google.com/fb/a/mailverify?uri=Slowbreathing', 'popupwindow', 'scrollbars=yes,width=550,height=220');return true">

                    <p class="newsletter-text">Get new posts to your inbox</p>

                    <input type="hidden" value="Slowbreathing" name="uri" />

                    <input type="hidden" name="loc" value="en_US" />

                    <input class="newsletter-email" type="text" name="email" placeholder="name@example.com" />

                    <input class="newsletter-submit" type="submit" value="Subscribe" />

                </form>
  </div>




</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="http://localhost:4000/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="http://localhost:4000/js/main.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-142206738-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
