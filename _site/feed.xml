<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-08T15:42:48+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Slowbreathing</title><subtitle>Programming is more than just typing.</subtitle><entry><title type="html">Meditating with microprocessors Series: Part-1: Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea</title><link href="http://localhost:4000/articles/2020-07/MeditatingProcessor-1" rel="alternate" type="text/html" title="Meditating with microprocessors Series: Part-1: Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea" /><published>2020-07-29T15:07:19+00:00</published><updated>2020-07-29T15:07:19+00:00</updated><id>http://localhost:4000/articles/2020-07/MeditatingProcessor-1</id><content type="html" xml:base="http://localhost:4000/articles/2020-07/MeditatingProcessor-1">&lt;h3 id=&quot;meditating-with-microprocessors&quot;&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating with microprocessors&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;part-1&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Meditation and microprocessors: An extremely simple idea&quot;&gt;Meditation and microprocessors: An extremely simple idea&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#MicroProcessor C-states&quot;&gt;MicroProcessor C-states&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#A quick Summary of wakeup_latency&quot;&gt;A quick Summary of wakeup_latency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Artificial Intelligence based Hardware(Miroprocessor) tuning&quot;&gt;Artificial Intelligence based Hardware(Miroprocessor) tuning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;part-2&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#MicroProcessor Components&quot;&gt;MicroProcessor Components&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Core&quot;&gt;Core&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&quot;&gt;Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Execution&quot;&gt;Sandy Bridge Pipeline:Execution&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Backend (Data load and Store)&quot;&gt;Sandy Bridge Pipeline:Backend (Data load and Store)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Haswell Pipeline&quot;&gt;Haswell Pipeline&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Uncore&quot;&gt;Uncore&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Linux Inteface to CPUIDLE&quot;&gt;Linux Inteface to CPUIDLE&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem&quot;&gt;CPUIDLE subsystem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Driver load&quot;&gt;CPUIDLE subsystem:Driver load&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Call the Driver&quot;&gt;CPUIDLE subsystem:Call the Driver&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Governor&quot;&gt;CPUIDLE subsystem:Governor&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;CPUIDLE subsystem:Gathering and undertanding latency data&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Trading off power for UltraLowLatency&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;part-3&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Core&quot;&gt;Core and Uncore:Core&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power&quot;&gt;Power&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things off&quot;&gt;Power:Turn things off&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:c-states&quot;&gt;Power:c-states&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned&quot;&gt;Power:Tuned&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:c-states requests&quot;&gt;Power:Tuned:c-states requests&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware State Residency&quot;&gt;Power:Tuned:Hardware State Residency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:c-states&quot;&gt;Power:Tuned:Influx:Grafana:c-states&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Measuring Latency&quot;&gt;Power:Tuned:Measuring Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware Latency&quot;&gt;Power:Tuned:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Wakeup Latency&quot;&gt;Power:Tuned:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:latency&quot;&gt;Power:Tuned:Influx:Grafana:latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:PMQOS&quot;&gt;Power:PMQOS&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down&quot;&gt;Power:Turn things down&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down:P-states:Hardware Latency&quot;&gt;Power:Turn things down:P-states:Hardware Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore&quot;&gt;Core and Uncore:Uncore&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:How much power can be saved&quot;&gt;Core and Uncore:How much power can be saved&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Artificial Intelligence guided Predictive MicroProcessor tuning&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;part-4&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a solution?&quot;&gt;Is there a solution?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a smarter solution?: Artificial Intelligence model based proactive decision making&quot;&gt;Is there a smarter solution?: Artificial Intelligence model based proactive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Recognizing the pattern&quot;&gt;Recognizing the pattern&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transformer as the backbone&quot;&gt;Transformer as the backbone&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transfer Learning inspiration from NLP&quot;&gt;Transfer Learning inspiration from NLP&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Fine tuning pre-trained networks&quot;&gt;Fine tuning pre-trained networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Artificial Intelligence model based proactive and predictive decision making&quot;&gt;Artificial Intelligence model based proactive and predictive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#The Final Cut&quot;&gt;The Final Cut&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools of the trade&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;part-5&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:An incisive but limited view&quot;&gt;Linux Tools:An incisive but limited view&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:uprobes and kprobes&quot;&gt;Linux Tools:Event Sources:uprobes and kprobes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:Tracepoints&quot;&gt;Linux Tools:Event Sources:Tracepoints&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools&quot;&gt;Linux Tools:Extraction Tools&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&quot;&gt;Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Engineering&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Engineering&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&quot;&gt;Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&quot;&gt;Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&quot;&gt;Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:Stack Trace Walking&quot;&gt;Linux Tools:Extraction Tools:BPF:Stack Trace Walking&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:A defining example&quot;&gt;Linux Tools:Extraction Tools:BPF:A defining example&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;&lt;a name=&quot;Introduction&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In the &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;multi-part series&lt;/a&gt;&lt;/strong&gt; titled &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating-with-microprocessors&lt;/a&gt;&lt;/strong&gt;, I demonstrate the use of &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; to tune &lt;strong&gt;microprocessors&lt;/strong&gt; for &lt;strong&gt;Ultra-Low-Latency&lt;/strong&gt; and &lt;strong&gt;Realtime&lt;/strong&gt; loads. The techniques, in general, can be extended to other components of a computer system like storage devices, memory, etc. However, the article series and my work is currently restricted to &lt;strong&gt;Intel microprocessors only&lt;/strong&gt;. &lt;strong&gt;In future&lt;/strong&gt;, we may extend this to other hardware components of a computer system. This is a very &lt;strong&gt;specialized&lt;/strong&gt; and &lt;strong&gt;intense&lt;/strong&gt; field and hence I intend to break it down using the first-principles approach into simpler pieces of technology that are easy to understand. There are 5 parts to the series, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Meditating with Microprocessors: An essentially simple idea(part-1)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Trading off power for UltraLowLatency (part-3) (part-3)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Artificial Intelligence guided Predictive MicroProcessor tuning (part-4)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools of the trade (part-5)&lt;/a&gt;&lt;/strong&gt;. &lt;strong&gt;In the balance then, this is a documentation of my journey navigating these utterly specialized fields ( microarchitecture and Artificial Intelligence ), how to marry them together, the issues I faced, the respective solutions, what (how much) are the benefits if any, and what to have in the toolbox.&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;-1&quot;&gt;&lt;a name=&quot;Meditation and microprocessors: An extremely simple idea&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;meditation-and-microprocessors-an-extremely-simple-idea&quot;&gt;Meditation and microprocessors: An extremely simple idea&lt;/h4&gt;
&lt;p&gt;The idea is so simple, it is actually ridiculous. Also, it pays to know of an analogy to compare it to just in case the technology sounds complicated. In our daily lives, we go through cycles of work time and relaxation ( I’ll call it meditation, you may call it meditation or sleep ). The work time is our livelihood, pays our bills but also stresses us out. The meditation time is the recuperation time. It helps us stay sane.&lt;/p&gt;

&lt;p&gt;As it turns out, microprocessors, for lack of a better phrase, have a similar sleeping pattern. In fact, modern microprocessors have a very sophisticated sleeping pattern. When they have work to do (execute instructions) they are in a waking state (duh) getting vital work done but expending much energy. On the other hand, when there is a lack of work (no instructions to be executed) they spiral into deeper and deeper sleeping states with the intention of saving power and conserving energy. The catch is that the deeper they go into sleep states, the more is the time they take to come back to full awareness to execute instructions again.&lt;/p&gt;

&lt;h1 id=&quot;-2&quot;&gt;&lt;a name=&quot;MicroProcessor C-states&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;microprocessor-c-states&quot;&gt;MicroProcessor C-states&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This is c-states as documented in the &lt;a href=&quot;[https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/8th-gen-core-family-datasheet-vol-1.pdf]&quot;&gt;Intel 8th and 9th datasheeet manual&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;The actual states may differ depending on the exact model of the Microprocessor.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/c-states.png&quot; alt=&quot;Image: figure-1: &amp;lt;strong&amp;gt;C-states from Intel's Documentation&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: &lt;strong&gt;C-states from Intel's Documentation&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This can be queried using the “/sys/devices/system” interface on Linux&lt;/li&gt;
    &lt;li&gt;The actual states may differ depending on the exact model of the Microprocessor.&lt;/li&gt;
    &lt;li&gt;Names can be confusing too. So there are 9 c-states(c-state0-c-state8) and they are called C0,C1,C1E,C3,C6,C7s,C8,C9,C10.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/c-states2.png&quot; alt=&quot;Image: figure-2: &amp;lt;strong&amp;gt;c-states on my current microprocessor&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: &lt;strong&gt;c-states on my current microprocessor&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Each of the green lines ending on a positive number represents entry into a c-state.&lt;/li&gt;
    &lt;li&gt;Again, Each one of the green line ending on a negative number (-5) represents the exit from the previous c-state.&lt;/li&gt;
    &lt;li&gt;The value returned to denote an exit is actually 4294967295 which is -1. size_t is an unsigned integral type, it is the largest possible representable value for this type&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-3&quot;&gt;&lt;a name=&quot;c-state transitions&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/c-states3.png&quot; alt=&quot;Image: figure-3: &amp;lt;strong&amp;gt;c-states transitions&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: &lt;strong&gt;c-states transitions&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The C0 is the state where the code is executing. So there is O ‘wakeup latency’. ‘Wakeup latency’ refers to the time it takes for the microprocessor to wake up from a sleep state. What you see marked in red is application latency.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The rest of the picture c-state1(C1) to c-state8(C10) refers to the ‘wakeup latency’ and it goes up as the microprocessor goes into deeper c-states&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/c-states_latency.png&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;Latency(us) on x axis C-state on y axis&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;Latency(us) on x axis C-state on y axis&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-4&quot;&gt;&lt;a name=&quot;A quick Summary of wakeup_latency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;a-quick-summary-of-wakeup_latency&quot;&gt;A quick Summary of wakeup_latency:&lt;/h4&gt;
&lt;p&gt;Hopefully, the above illustrations were helpful in understanding the crux of the matter, and references to meditation were not distracting. Here is a quick summary, modern Microprocessors switch states(deeper c-states as illustrated in figure-{1,2,3,4}) to save energy when there are no instructions to execute. And fling back to C0 when there are instructions to execute. This flinging back time is called &lt;strong&gt;‘wakeup_latency’&lt;/strong&gt; and is higher for deeper states as illustrated in figure-4. And this ‘wakeup_latency’ is problematic for Ultra-Low-Latency applications. The source of the problem&lt;/p&gt;

&lt;h1 id=&quot;-5&quot;&gt;&lt;a name=&quot;Artificial Intelligence based Hardware(Miroprocessor) tuning&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;artificial-intelligence-based-hardwaremiroprocessor-tuning&quot;&gt;Artificial Intelligence based Hardware(Miroprocessor) tuning&lt;/h4&gt;
&lt;p&gt;If you consider figure-3 there were close to 25000 transitions (give or take) in a stretch of 15 seconds. Each of those costing valuable hundreds of microseconds. The effect can also get magnified because there might be multiple application threads vying for microprocessor resources. It can get worse. There might be a &lt;a href=&quot;https://en.wikipedia.org/wiki/Domino_effect&quot;&gt;Domino&lt;/a&gt; effect of these occurrences. There is a specific term coined for the Domino effect in the software world. I believe it is called &lt;a href=&quot;https://www.azul.com/files/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf&quot;&gt;coordinated omission&lt;/a&gt; and &lt;a href=&quot;https://www.azul.com/files/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf&quot;&gt;here is a great reference to it&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Is there a microprocessor setting that restricts spiraling into deeper c-states so that wakeup latency is effectively ‘0’? Yes, there is, but it will drain a lot of energy/power and may be detrimental to the microprocessor’s health in the long run ( Similar to life without meditation ).&lt;/li&gt;
    &lt;li&gt;Can we look at the historical load and predict when it might occur again. If yes, then can we configure it to a setting which is suited to Ultra-Low-Latency or Realtime workloads. &lt;strong&gt;The answer to that is a huge yes.&lt;/strong&gt; Understandably this has to be done on the fly at runtime using the software controls provided by the Hardware and the OS.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;This is one of the things that our software intends to do&lt;/strong&gt;. So once we have the load being predicted by Artificial Intelligence based model, we tune the microprocessor to the settings which reduces latency if the goal is to lower latency.&lt;/li&gt;
    &lt;li&gt;We reset the settings to power save(normal mode) when the high load had tided over. &lt;strong&gt;As of now, the implementation is only for Intel microprocessors.&lt;/strong&gt; But in the future, we believe we can extend it to Memory chips, HDDs and SSDs, and so forth.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;There is something else.&lt;/strong&gt; Just like there is a unique signature that every person has, in a similar manner there is a unique signature for a particular load. This signature can be recognized using pre-training using techniques similar to &lt;strong&gt;transfer-learning&lt;/strong&gt; in NLP and BERT.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a name=&quot;Summary&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;In summary then, modern microprocessors have power saving states they enter into when there is no work to do. All very good, till you consider that they have to get back to “full click” when they have to perform work. Ordinarily, this is not a problem but for some latency(performance) sensitive application. This approach is reactive or causal. Can we make this proactive based on some Artificial Intelligence based load prediction. Rest of the articles in the series is an under-the-hood look at how the processors interface with software(OS) and if there is a case to be made for Artificial Intelligence. Also there are things to explore for performance minded programmers. So get your hands dirty.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/250967/&quot;&gt;&lt;strong&gt;What every programmer should know about memory:&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(This is a definitive 9 part(the links for the rest of the parts are embedded in the first one.) article on how the hardware works and, how software and data structure design can exploit it. It had a huge impact on the way I thought and still do think about design. The article first came out in 2007 but it is still relevant which is proof that basics don’t change very often.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html&quot;&gt;&lt;strong&gt;Intels documenation:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Intel’s documentation is the authentic source here. But, it is incredibly difficult to read. It is as if Intel’s employees were given a raise to make it “impossible to comprehend” kind of document.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.agner.org/optimize/&quot;&gt;&lt;strong&gt;Agner Fog:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (He benchmarks microprocessors using forward and reverse engineering techniques. My bible.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/torvalds/linux&quot;&gt;&lt;strong&gt;Linux Source:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (If your going to trace your programmes/applications then having the Linux source is must. Tracers will tell you half the story, the other half will come from here. )&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;&lt;strong&gt;Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Modern Natural Language processing, in general, must be indebted to Transformer architecture. We, however, use an asymmetric transformer setup.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.14794.pdf&quot;&gt;&lt;strong&gt;Performer-A Sparse Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (This is as cutting edge as it can get. The transformer at the heart of it, is a stacked multi-headed-attention unit. As the sequences(of words or System events or stock prices or vehicle positions e.t.c.) get longer the quadratic computation and quadratic memory for matric cannot keep up. Performer, a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=93uE_kWWQjs&amp;amp;t=1178s&quot;&gt;&lt;strong&gt;Ftrace: The Inner workings&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( I dont think there is a better explaination of Ftrace’s inner workings.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/608497/&quot;&gt;&lt;strong&gt;Ftrace: The hidden light switch:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( This article demonstrates the tools based on Ftrace.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.brendangregg.com/ebpf.html&quot;&gt;&lt;strong&gt;BPF:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( eBPF or just BPF is changing the way programming is done on Linux. Linux now has observability superpowers beyond most OSes. A detailed post on BPF is need of the hour and I am planning as much. In the meantime, the attached link can be treated as virtual BPF homepage. )&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!--Series--&gt;

&lt;!--Doc1--&gt;

&lt;!--Doc2--&gt;

&lt;!--Doc3--&gt;

&lt;!--Doc4--&gt;

&lt;!--Doc5--&gt;
&lt;!--External references--&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="NLP" /><category term="Microarchitecture" /><category term="UltraLowLatency" /><category term="RealTime" /><category term="Performance" /><category term="Meditating-with-microprocessors" /><summary type="html">In the current series titled Meditating-with-microprocessors, I demonstrate the use of Artificial Intelligence to tune microprocessors for Ultra-Low-Latency and Realtime loads. The techniques, in general, can be extended to other components of a computer system like storage devices, memory, etc. However, the article series and my work is currently restricted to Intel microprocessors only. In future, we may extend this to other hardware components of a computer system. This is a very specialized and intense field and hence I intend to break it down using the first-principles approach into simpler pieces of technology that are easy to understand. There are 5 parts to the series Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1), A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2), Trading off power for UltraLowLatency (part-3), Artificial Intelligence guided Predictive MicroProcessor tuning (part-4), Appendix:Tools of the trade(part-5) . In the balance then, this is a documentation of my journey navigating these utterly specialized fields ( microarchitecture and Artificial Intelligence ), how to marry them together, the issues I faced, the respective solutions, what (how much) are the benefits if any, and what to have in the toolbox.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22feature%22=%3E%22mwprocs/c-states3.jpeg%22%7D" /></entry><entry><title type="html">Meditating with microprocessors Series: Part-2: A crashcourse in Microarchitecture and Linux CPUIDLE interface</title><link href="http://localhost:4000/articles/2020-07/MeditatingProcessor-2" rel="alternate" type="text/html" title="Meditating with microprocessors Series: Part-2: A crashcourse in Microarchitecture and Linux CPUIDLE interface" /><published>2020-07-29T15:07:19+00:00</published><updated>2020-07-29T15:07:19+00:00</updated><id>http://localhost:4000/articles/2020-07/MeditatingProcessor-2</id><content type="html" xml:base="http://localhost:4000/articles/2020-07/MeditatingProcessor-2">&lt;h3 id=&quot;meditating-with-microprocessors&quot;&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating with microprocessors&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;part-1&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Meditation and microprocessors: An extremely simple idea&quot;&gt;Meditation and microprocessors: An extremely simple idea&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#MicroProcessor C-states&quot;&gt;MicroProcessor C-states&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#A quick Summary of wakeup_latency&quot;&gt;A quick Summary of wakeup_latency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Artificial Intelligence based Hardware(Miroprocessor) tuning&quot;&gt;Artificial Intelligence based Hardware(Miroprocessor) tuning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;part-2&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#MicroProcessor Components&quot;&gt;MicroProcessor Components&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Core&quot;&gt;Core&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&quot;&gt;Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Execution&quot;&gt;Sandy Bridge Pipeline:Execution&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Backend (Data load and Store)&quot;&gt;Sandy Bridge Pipeline:Backend (Data load and Store)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Haswell Pipeline&quot;&gt;Haswell Pipeline&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Uncore&quot;&gt;Uncore&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Linux Inteface to CPUIDLE&quot;&gt;Linux Inteface to CPUIDLE&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem&quot;&gt;CPUIDLE subsystem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Driver load&quot;&gt;CPUIDLE subsystem:Driver load&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Call the Driver&quot;&gt;CPUIDLE subsystem:Call the Driver&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Governor&quot;&gt;CPUIDLE subsystem:Governor&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;CPUIDLE subsystem:Gathering and undertanding latency data&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Trading off power for UltraLowLatency&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;part-3&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Core&quot;&gt;Core and Uncore:Core&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power&quot;&gt;Power&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things off&quot;&gt;Power:Turn things off&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:c-states&quot;&gt;Power:c-states&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned&quot;&gt;Power:Tuned&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:c-states requests&quot;&gt;Power:Tuned:c-states requests&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware State Residency&quot;&gt;Power:Tuned:Hardware State Residency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:c-states&quot;&gt;Power:Tuned:Influx:Grafana:c-states&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Measuring Latency&quot;&gt;Power:Tuned:Measuring Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware Latency&quot;&gt;Power:Tuned:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Wakeup Latency&quot;&gt;Power:Tuned:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:latency&quot;&gt;Power:Tuned:Influx:Grafana:latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:PMQOS&quot;&gt;Power:PMQOS&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down&quot;&gt;Power:Turn things down&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down:P-states:Hardware Latency&quot;&gt;Power:Turn things down:P-states:Hardware Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore&quot;&gt;Core and Uncore:Uncore&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:How much power can be saved&quot;&gt;Core and Uncore:How much power can be saved&lt;/a&gt;&lt;br /&gt;
      * &lt;a href=&quot;MeditatingProcessor-3#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Artificial Intelligence guided Predictive MicroProcessor tuning&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;part-4&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a solution?&quot;&gt;Is there a solution?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a smarter solution?: Artificial Intelligence model based proactive decision making&quot;&gt;Is there a smarter solution?: Artificial Intelligence model based proactive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Recognizing the pattern&quot;&gt;Recognizing the pattern&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transformer as the backbone&quot;&gt;Transformer as the backbone&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transfer Learning inspiration from NLP&quot;&gt;Transfer Learning inspiration from NLP&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Fine tuning pre-trained networks&quot;&gt;Fine tuning pre-trained networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Artificial Intelligence model based proactive and predictive decision making&quot;&gt;Artificial Intelligence model based proactive and predictive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#The Final Cut&quot;&gt;The Final Cut&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools of the trade&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;part-5&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:An incisive but limited view&quot;&gt;Linux Tools:An incisive but limited view&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:uprobes and kprobes&quot;&gt;Linux Tools:Event Sources:uprobes and kprobes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:Tracepoints&quot;&gt;Linux Tools:Event Sources:Tracepoints&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools&quot;&gt;Linux Tools:Extraction Tools&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&quot;&gt;Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Engineering&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Engineering&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&quot;&gt;Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&quot;&gt;Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&quot;&gt;Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:Stack Trace Walking&quot;&gt;Linux Tools:Extraction Tools:BPF:Stack Trace Walking&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:A defining example&quot;&gt;Linux Tools:Extraction Tools:BPF:A defining example&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;&lt;a name=&quot;Introduction&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The current article is part of a bigger &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;series&lt;/a&gt;&lt;/strong&gt; titled &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating-with-microprocessors&lt;/a&gt;&lt;/strong&gt;, in which I demonstrate the use of &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; to tune &lt;strong&gt;microprocessors&lt;/strong&gt; for &lt;strong&gt;Ultra Low Latency&lt;/strong&gt; and &lt;strong&gt;Realtime&lt;/strong&gt; loads. There are 5 parts to the series, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Meditating with Microprocessors: An essentially simple idea(part-1)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Trading off power for UltraLowLatency (part-3)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Artificial Intelligence guided Predictive MicroProcessor tuning (part-4)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools of the trade (part-5)&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In the current article I lay the  technical groundwork for later articles in the series to build on. The terms and technologies I’ll be using later must be understood really well. So we look at 3 concepts primarily.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Firstly, the architecture of a modern microprocessor in short, really really short. Just enough to make a mental model of the microprocessor to work with.&lt;/li&gt;
    &lt;li&gt;Secondly, How does the software(Linux) interface with the microprocessor. Again, just enough to make sense of the data we gather from the CPU using various UltraLowLatency profilers and tracers.&lt;/li&gt;
    &lt;li&gt;Thirdly, help programmers like you to get started quickly, explore further, and help us.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-1&quot;&gt;&lt;a name=&quot;MicroProcessor Components&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;microprocessor-components&quot;&gt;MicroProcessor Components&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Core:&lt;/strong&gt; The modules/agents that compute are called the cores in a CPU. They may be general-purpose CPU cores or special purpose GPGPU cores. Cores take instructions from a software program and execute them loads, stores, “arithmetic and control flow”.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Cache:&lt;/strong&gt; Caches frequently used data so that expensive memory access can be avoided. There is usually a hierarchy of multiple levels of cache. L1 and L2 are quick access (static RAM chips) small in size. L3 and L4, slower to access (dynamic RAM chips) but comparatively larger in size. Also, L3 and L4 are usually on the die as the core.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;On-chip-fabric:&lt;/strong&gt; Interconnects exist on the CPU dies that are commonly called on-die or on-chip fabrics. These are not to be confused with fabrics that connect multiple CPU dies together at the data center level.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Memory controller:&lt;/strong&gt; Memory controllers provide an interface to main memory (DDR in many recent processor generations).&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;PCIe:&lt;/strong&gt; PCIe provides a mechanism to connect external devices such as network cards into the CPU.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Chipset:&lt;/strong&gt; The chipset can be thought of as a support entity to the CPU. In addition to supporting the boot process, it can also provide additional capabilities such as PCIe, hard drive access, networking, and manageability. Chipset functionality is integrated into the same die or package as the cores in the microserver space.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-2&quot;&gt;&lt;a name=&quot;Core&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;core&quot;&gt;Core&lt;/h4&gt;
&lt;p&gt;Full-length books and courses have been dedicated to microarchitecture, so to illustrate the idea in a few pictures is foolhardy, to say the least. The complexity of this subject is second to none. And yet, I’ll give it a try, because the essence is not that difficult and worth understanding. Also, I have marked some links that do an excellent treatment of the subject in the references below. &lt;strong&gt;The first thing we look at is the flow of instruction and data inside the core at a relatively high level.&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cores can be designed for general purpose usage where they work well for a wide range of workloads. The trade of with general purpose cores is a relatively higher cost of design and construction. As a result, there are special-purpose cores, that trade-off floating-point performance, or 64-bit precision e.t.c. for smaller form factor or cost. In this section, we look at general-purpose cores. Also, we look at &lt;strong&gt;Sandy Bridge&lt;/strong&gt; as it is relatively modern and Intel makes evolutionary changes moving forward to &lt;strong&gt;Haswell&lt;/strong&gt; and &lt;strong&gt;*Lake pipelines&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Follow the color code.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Core:&lt;/strong&gt; Identifying various modules/units in the core.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/core.png&quot; alt=&quot;Image: figure-1: &amp;lt;strong&amp;gt;Oncore modules/units&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: &lt;strong&gt;Oncore modules/units&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-3&quot;&gt;&lt;a name=&quot;Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;sandy-bridge-pipelinefrontendinstruction-load-decode-cache&quot;&gt;Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&lt;/h4&gt;
&lt;p&gt;This details the frontend of the core. How the &lt;strong&gt;INSTRUCTIONS(NOT DATA)&lt;/strong&gt; are loaded, decoded and cached.
They have been designed in such a way that multiple μop(micro-operations) are available for execution at every cycle to increase parallelism.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Instruction fetch(figure-2(yellow)):&lt;/strong&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;Instruction codes are fetched from the code cache(L1i-cache) in aligned 16-byte chunks into a double buffer that can hold two 16-byte chunks. The purpose of the double buffer is to make it possible to decode an instruction that crosses a 16-byte boundary.&lt;/span&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Instruction decoding(figure-2(yellow)):&lt;/strong&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;This is a very critical stage in the pipeline because it limits the degree of parallelism that can be achieved. We want to fetch more than one instruction per clock cycle, decode more than one instruction per clock cycle, and execute more than one μop per clock cycle in order to gain speed. But decoding instructions in parallel is difficult when instructions have different lengths. You need to decode the first instruction in order to know how long it is and where the second instruction begins before you can start to decode the second instruction. So a simple instruction length decoder would only be able to handle one instruction per clock cycle. There are four decoders, which can handle instructions generating one or more ]=ops(Micro-operation) according to certain patterns.&lt;/span&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Micro-operation Cache(figure-2(yellow)):&lt;/strong&gt;&lt;span style=&quot;color:#ff9900&quot;&gt; The decoded μops(Micro-operation) are cached in a μops(Micro-operation) cache after the decoders in Sandy Bridge. This is useful because the limitation of 16 bytes per clock cycle in the fetch/decode units is a serious bottleneck if the average instruction length is more than four bytes. The throughput is doubled to 32 bytes per clock for code that fits into the μop(Micro-operation) cache.&lt;/span&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-4&quot;&gt;&lt;a name=&quot;Sandy Bridge Pipeline:Execution&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;sandy-bridge-pipelineexecution&quot;&gt;Sandy Bridge Pipeline:Execution&lt;/h4&gt;
&lt;p&gt;This is where the execution happens. This is the critical source of performance in modern microprocessors. AFAIK, &lt;a href=&quot;https://en.wikipedia.org/wiki/P5_(microarchitecture)&quot;&gt;P5&lt;/a&gt; is the first model from Intel that had multiple ports and an &lt;strong&gt;out-of-order( also called dynamic execution)&lt;/strong&gt; scheduler. This scheme is used in most high-performance CPUs. &lt;strong&gt;In this scheme, the order of execution of instructions is determined by the availability of data and execution units and not the source order of the program(Thread).&lt;/strong&gt; What this means is instructions within a thread may be reordered to suit the availability of data and execution units. The capacity(parallelism) of Sandy Bridge’s execution unit is quite high. Many μops have two or three execution ports to choose between and each unit.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Register Alias Table(RAT)(figure-2(green)):&lt;/strong&gt;&lt;span style=&quot;color:#669900&quot;&gt; The assembly code references logical/architectural registers(Think of them as place holders). Every logical register has a set of physical registers associated with it. The processor re-maps/transposes this name(for e.g. %rax) to one specific physical register on the fly. The physical registers are opaque and cannot be referenced directly but only via the canonical names.&lt;/span&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Register allocation and renaming(figure-1(green)):&lt;/strong&gt;&lt;span style=&quot;color:#669900&quot;&gt; Register renaming is controlled by the register alias table (RAT) and the reorder buffer (ROB).&lt;/span&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Execution Units(EU)(figure-2(green)):&lt;/strong&gt;&lt;span style=&quot;color:#669900&quot;&gt; The Sandy Bridge and Ivy Bridge have six execution ports. Port 0, 1 and 5 are for arithmetic and logic operations (ALU). There are two identical memory read ports on port 2 and 3 where previous processors had only one. Port 4 is for memory write. The memory write unit on port 4 has no address calculation. All write operations use port 2 or 3 for address calculation. The maximum throughput is one unfused μop on each port per clock cycle.&lt;/span&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-5&quot;&gt;&lt;a name=&quot;Sandy Bridge Pipeline:Backend (Data load and Store)&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;sandy-bridge-pipelinebackend-data-load-and-store&quot;&gt;Sandy Bridge Pipeline:Backend (Data load and Store)&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Memory access(figure-2(pink)):&lt;/strong&gt; &lt;span style=&quot;color:#cc0099&quot;&gt;The Sandy Bridge has two memory read ports where previous processors have only one. Cache bank conflicts are quite common when the maximum memory bandwidth is utilized.&lt;/span&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/microarchflow.png&quot; alt=&quot;Image: figure-2: &amp;lt;strong&amp;gt;Core&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: &lt;strong&gt;Core&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;You can generate the below SOC layout using &lt;a href=&quot;[https://linux.die.net/man/1/lstopo]&quot;&gt;hwloc lstopo tool&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/microarchflow3.png&quot; alt=&quot;Image: figure-3: &amp;lt;strong&amp;gt;Entire SOC(System on chip)&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: &lt;strong&gt;Entire SOC(System on chip)&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-6&quot;&gt;&lt;a name=&quot;Haswell Pipeline&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;haswell-pipeline&quot;&gt;Haswell Pipeline&lt;/h4&gt;

&lt;p&gt;Intel followed up Sandy and Ivy bridge with Haswell. The Haswell has several &lt;strong&gt;important but evolutionary&lt;/strong&gt; improvements over previous designs. With a few changes, everything has been increased a little bit. Like number of execution units are 8. Compare figure-2 and figure-4 side by side.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Evolutionary changes in Haswell. &lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/microarchflow4.png&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;Core&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;Core&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Evolutionary changes in Haswell. This SOC(System on Chip) view generated by &lt;a href=&quot;[https://linux.die.net/man/1/lstopo]&quot;&gt;hwloc lstopo tool&lt;/a&gt;.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/microarchflow5.png&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;Entire SOC(System on chip)&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;Entire SOC(System on chip)&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-7&quot;&gt;&lt;a name=&quot;Uncore&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;uncore&quot;&gt;Uncore&lt;/h4&gt;
&lt;p&gt;All logic that is on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Die_(integrated_circuit)&quot;&gt;die&lt;/a&gt; is referred to as Uncore by Intel. As you may have guessed, that is quite a flimsy definition. But it makes sense because, as we integrate more into the single &lt;strong&gt;die&lt;/strong&gt; or &lt;strong&gt;package&lt;/strong&gt;, it results in a denser design and consumes much less power.&lt;/p&gt;

&lt;p&gt;In the Nehalem generation(preceding Sandy bridge ), this included the &lt;strong&gt;L3 cache&lt;/strong&gt;, &lt;strong&gt;integrated memory controller&lt;/strong&gt;, &lt;strong&gt;QuickPath Interconnect (QPI; for multi-socket communication)&lt;/strong&gt;, and an &lt;strong&gt;interconnect&lt;/strong&gt; that tied it all together. In the Sandy Bridge generation over and above Sandy Bridge, &lt;strong&gt;PCIe&lt;/strong&gt; was integrated into the CPU uncore.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;L3 cache&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;integrated memory controller&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;QuickPath Interconnect (QPI; for multi-socket communication)&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;interconnect&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;PCIe&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Integrated Grafics&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/uncore.png&quot; alt=&quot;Image: figure-6: &amp;lt;strong&amp;gt;Uncore&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: &lt;strong&gt;Uncore&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-8&quot;&gt;&lt;a name=&quot;Linux Inteface to CPUIDLE&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-inteface-to-cpuidle&quot;&gt;Linux Inteface to CPUIDLE&lt;/h4&gt;
&lt;p&gt;The idea here is to control the deeper c-states( deeper meditative states if you recall ) dynamically at run time. But before that, we need to understand Linux’s interface to a microprocessor. The reason why this is important is that it will help us understand the data gathered by the tracers on the microprocessor’s latency. Understanding this process of interaction allows us to visualize data in more flexible ways. Like mapping c-states to latencies etc.&lt;/p&gt;

&lt;p&gt;On most systems the microprocessor is doing some task, editing documents, sending or receiving emails, etc. But what does a microprocessor do when there is nothing to execute.
As it turns out, doing nothing is quite complicated. In the olden days idling(doing nothing) was merely the lowest priority thread in a system on a busy spin. Until a new task popped in.
Sooner rather than later, someone was going to realize that it is quite inefficient to expend power without any logical work being done. And this is where deeper c-states come in.&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/c-states-desc.png&quot; alt=&quot;Image: figure-7: &amp;lt;strong&amp;gt;c-states description&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: &lt;strong&gt;c-states description&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-9&quot;&gt;&lt;a name=&quot;CPUIDLE subsystem&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;cpuidle-subsystem&quot;&gt;CPUIDLE subsystem&lt;/h4&gt;

&lt;p&gt;Different processors may have different idle-state characteristics. In fact, even for the same manufacturer, there may be differences from one model to another. “cpuidle.h”(“include/linux/cpuidle.h”) is the generic framework defined for the purpose. “cpuidle.h” acts as an interface to separate platform-independent(rest of the kernel) code from manufacture-dependant(CPU Drivers) ones.&lt;/p&gt;

&lt;h1 id=&quot;-10&quot;&gt;&lt;a name=&quot;CPUIDLE subsystem:Driver load&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;cpuidle-subsystemdriver-load&quot;&gt;CPUIDLE subsystem:Driver load&lt;/h4&gt;
&lt;p&gt;The implementation has been handled in &lt;strong&gt;2 different ways.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;For a hardware platform (for e.g. x86), a generic set of idle states may be supported in a manufacture independent way. This is implemented in &lt;strong&gt;“acpi_idle”(“drivers/acpi/processor_idle.c”)&lt;/strong&gt; available at implements this behavior for all/most x86 vendors(Intel/AMD)&lt;/li&gt;
    &lt;li&gt;The other choice is to use the &lt;strong&gt;Intel driver&lt;/strong&gt; which is implemented in &lt;strong&gt;“intel_idle”(“drivers/idle/intel_idle.c”)&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/idle.png&quot; alt=&quot;Image: figure-8: &amp;lt;strong&amp;gt;CPUIdle_driver&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-8: &lt;strong&gt;CPUIdle_driver&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;The figure-9&lt;/strong&gt; shows the implementation of &lt;strong&gt;“intel_idle”&lt;/strong&gt; which is implemented in &lt;strong&gt;“intel_idle.c”(“drivers/idle/intel_idle.c”)&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/intel_idle.png&quot; alt=&quot;Image: figure-9: &amp;lt;strong&amp;gt;cpuidle_state&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-9: &lt;strong&gt;cpuidle_state&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;CPUIDLE driver&lt;/strong&gt; from Intel &lt;strong&gt;initialization function.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/cpuinit.png&quot; alt=&quot;Image: figure-10: &amp;lt;strong&amp;gt;CPU driver initialization(intel_idle)-1&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10: &lt;strong&gt;CPU driver initialization(intel_idle)-1&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;CPUIDLE driver&lt;/strong&gt; from Intel initialization function. &lt;strong&gt;Figure-(8,9,10,11,12)&lt;/strong&gt; are to be looked at &lt;strong&gt;together&lt;/strong&gt; to make sense.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/cpuinit2.png&quot; alt=&quot;Image: figure-11: &amp;lt;strong&amp;gt;cpuidle_device&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-11: &lt;strong&gt;cpuidle_device&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Finally the result of initialzation can be seen. The idle-states of a modern &lt;strong&gt;Intel processor&lt;/strong&gt; through &lt;strong&gt;Linux ‘sysfs’ interface.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-11&quot;&gt;&lt;a name=&quot;State of initialized device through sysfs:Residency States&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/cpuinit3.png&quot; alt=&quot;Image: figure-12: &amp;lt;strong&amp;gt;State of initialized device through sysfs&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-12: &lt;strong&gt;State of initialized device through sysfs&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-12&quot;&gt;&lt;a name=&quot;CPUIDLE subsystem:Call the Driver&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;cpuidle-subsystemcall-the-driver&quot;&gt;CPUIDLE subsystem:Call the Driver&lt;/h4&gt;
&lt;p&gt;The kernel’s scheduler cannot find a task to run on a CPU so it goes ahead and calls “do_idle”(“kernel/sched/idle.c”).
Which idle-state is chosen is a matter of policy, but how then is communicated to the processor is illustrated next.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;When the kernel scheduler cannot find work to do, it calls &lt;strong&gt;“do_idle()” from “kernel/sched/idle.c”.&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;The &lt;strong&gt;picture illustrates&lt;/strong&gt; the interaction.&lt;/li&gt;
    &lt;li&gt;At the bottom right, is a trace generated from &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt; for “intel_idle”&lt;/strong&gt;. It shows the stack trace leading upto &lt;strong&gt;intel_idle&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt; is almost a magical tracer.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/cpuidle-sub1.png&quot; alt=&quot;Image: figure-13: &amp;lt;strong&amp;gt;CPUIDLE:Tracing intel_idle&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-13: &lt;strong&gt;CPUIDLE:Tracing intel_idle&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This one shows the entry to &lt;strong&gt;processor specific&lt;/strong&gt; code of &lt;strong&gt;“intel_idle”&lt;/strong&gt; from &lt;strong&gt;“cpuidle_enter_state” (“drivers/cpuidle/cpuidle.c”)&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;The target state accessed from driver(struct drv). This state structure is of type &lt;strong&gt;“cpuidle_state” shown earlier.&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;I traced &lt;strong&gt;“sched_idle_set_state”&lt;/strong&gt; into which &lt;strong&gt;“target_state”&lt;/strong&gt; is getting passed.&lt;/li&gt;
    &lt;li&gt;I traced using &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&quot;&gt;EBPF&lt;/a&gt;&lt;/strong&gt;. &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&quot;&gt;EBPF&lt;/a&gt;&lt;/strong&gt; allows great &lt;strong&gt;flexibility in tracing&lt;/strong&gt;. As you can see I am accessing the struct &lt;strong&gt;“cpuidle_state”&lt;/strong&gt; at run time on a running kernel and printing it out.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/cpuidle-intelidle1.png&quot; alt=&quot;Image: figure-14: &amp;lt;strong&amp;gt;CPUIDLE:Tracing device_state&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-14: &lt;strong&gt;CPUIDLE:Tracing device_state&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Finally, call to &lt;strong&gt;“intel_idle”&lt;/strong&gt; from &lt;strong&gt;“cpuidle_enter_state” (“drivers/cpuidle/cpuidle.c”)&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/cpuidle-intelidle2.png&quot; alt=&quot;Image: figure-15: &amp;lt;strong&amp;gt;CPUIDLE:intel_idle&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-15: &lt;strong&gt;CPUIDLE:intel_idle&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Rubber hits the road or the code hits the metal?&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/cpuidle-intelidle4.png&quot; alt=&quot;Image: figure-16: &amp;lt;strong&amp;gt;CPUIDLE: code hits the metal&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-16: &lt;strong&gt;CPUIDLE: code hits the metal&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;“mwait_idle_with_hints”&lt;/strong&gt; and &lt;strong&gt;__mwait&lt;/strong&gt; are implemented in &lt;strong&gt;inline assembly.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mwait_idle_with_hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;static_cpu_has_bug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X86_BUG_MONITOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_set_polling_and_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;static_cpu_has_bug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X86_BUG_CLFLUSH_MONITOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;mb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;clflush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_thread_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;mb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;__monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_thread_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;need_resched&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;__mwait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;current_clr_polling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__mwait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;cm&quot;&gt;/* &quot;mwait %eax, %ecx;&quot; */&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;asm&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;volatile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;.byte 0x0f, 0x01, 0xc9;&quot;&lt;/span&gt;
         &lt;span class=&quot;o&quot;&gt;::&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;c&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h1 id=&quot;-13&quot;&gt;&lt;a name=&quot;CPUIDLE subsystem:Call the Driver&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;cpuidle-subsystemgovernor&quot;&gt;CPUIDLE subsystem:Governor&lt;/h4&gt;

&lt;p&gt;Governor is a part of the CPUIDLE subsystem and implements the policy part. Policies can be expressed in many ways.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;One policy can be, restrict deeper idle-states for lower latency. These policies are more direct in nature.&lt;/li&gt;
    &lt;li&gt;Others can be based on some QOS requiremment. For example, respond within a latency of a few 100 microseconds or powersave.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Many governors may exist in a kernel but only one of them can be in control.&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/governor.png&quot; alt=&quot;Image: figure-17: &amp;lt;strong&amp;gt;CPUIDLE:Governor&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17: &lt;strong&gt;CPUIDLE:Governor&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-14&quot;&gt;&lt;a name=&quot;CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;cpuidle-subsystemgathering-and-undertanding-latency-data&quot;&gt;CPUIDLE subsystem:Gathering and undertanding latency data&lt;/h4&gt;

&lt;p&gt;The next step is to gather and understand latency data. There are two challenges here. First,  to find a tracer that can gather CPUIDLE latency data without a huge observer effect. This can be achieved by using “&lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt;”. &lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;“ftrace” is the coolest tracing dude on the planet&lt;/a&gt;. Second, is to make sense of the data.  &lt;strong&gt;At a very high level, what we want to understand is the impact of deep idle-states on latency.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt; event tracing&lt;/strong&gt; for &lt;strong&gt;cpu_idle&lt;/strong&gt;. Which state a &lt;strong&gt;cpu enters and exits&lt;/strong&gt; with respect to time lapse as duration.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/latency1.png&quot; alt=&quot;Image: figure-18: &amp;lt;strong&amp;gt;CPUIDLE:ftrace raw data&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-18: &lt;strong&gt;CPUIDLE:ftrace raw data&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Filtered by cpu.&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/latency2.png&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;CPUIDLE:ftrace:Filtered by cpu&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;CPUIDLE:ftrace:Filtered by cpu&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;What the numbers mean, and &lt;strong&gt;corresponding latency calculation.&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/latency3.png&quot; alt=&quot;Image: figure-20: &amp;lt;strong&amp;gt;CPUIDLE:ftrace:latency calculation&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-20: &lt;strong&gt;CPUIDLE:ftrace:latency calculation&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Latency by &lt;strong&gt;state exit and entry&lt;/strong&gt; respectively in CSV form.&lt;/li&gt;
    &lt;li&gt;The &lt;strong&gt;time series property&lt;/strong&gt; has been retained. That will tell us if a &lt;strong&gt;configuration change has effected latency.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/latency4.png&quot; alt=&quot;Image: figure-17: &amp;lt;strong&amp;gt;CPUIDLE:ftrace:csv&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17: &lt;strong&gt;CPUIDLE:ftrace:csv&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Pushed to &lt;strong&gt;influxdb and then visualized by grafana.&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/latency5.png&quot; alt=&quot;Image: figure-17: &amp;lt;strong&amp;gt;CPUIDLE:ftrace:influx:grafana:visualization&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17: &lt;strong&gt;CPUIDLE:ftrace:influx:grafana:visualization&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Summary&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;We looked at 2 primary concepts. Firstly, a sketch of what units a CPU Core has and how it executes code and processes data. The pipeline of the core should provide a high-level idea of the flow instruction and data. Secondly, the interaction of software(Linux) with the CPU. This information is really interesting and in the field of performance absolutely indispensable. Once understood well, debugging becomes a lot easier. Fancy profilers are all very good but have a huge observer effect. ‘Ftrace’ like tools are built for this purpose. However, the data they generate may not be easily digestible. Now that we have a basic understanding, we can use these tools to measure after-effects of performance configurations in the rest of the &lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;series&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/250967/&quot;&gt;&lt;strong&gt;What every programmer should know about memory:&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(This is a definitive 9 part(the links for the rest of the parts are embedded in the first one.) article on how the hardware works and, how software and data structure design can exploit it. It had a huge impact on the way I thought and still do think about design. The article first came out in 2007 but it is still relevant which is proof that basics don’t change very often.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html&quot;&gt;&lt;strong&gt;Intels documenation:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Intel’s documentation is the authentic source here. But, it is incredibly difficult to read. It is as if Intel’s employees were given a raise to make it “impossible to comprehend” kind of document.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.agner.org/optimize/&quot;&gt;&lt;strong&gt;Agner Fog:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (He benchmarks microprocessors using forward and reverse engineering techniques. My bible.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/torvalds/linux&quot;&gt;&lt;strong&gt;Linux Source:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (If you are going to trace your programs/applications then having the Linux source is a must. Tracers will tell you half the story, the other half will come from here. )&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;&lt;strong&gt;Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Modern Natural Language processing, in general, must be indebted to Transformer architecture. We however, use an asymmetric transformer setup.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.14794.pdf&quot;&gt;&lt;strong&gt;Performer-A Sparse Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (This is as cutting edge as it can get. The transformer at the heart of it is a stacked multi-headed-attention unit. As the sequences(of words or System events or stock prices or vehicle positions e.t.c.) get longer, the quadratic computation and quadratic memory for matrix cannot keep up. Performer, a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=93uE_kWWQjs&amp;amp;t=1178s&quot;&gt;&lt;strong&gt;Ftrace: The Inner workings&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( I dont think there is a better explaination of Ftrace’s inner workings.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/608497/&quot;&gt;&lt;strong&gt;Ftrace: The hidden light switch:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( This article demonstrates the tools based on Ftrace.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.brendangregg.com/ebpf.html&quot;&gt;&lt;strong&gt;BPF:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( eBPF or just BPF is changing the way programming is done on Linux. Linux now has observability superpowers beyond most OSes. A detailed post on BPF is the need of the hour and I am planning as much. In the meantime, the attached link can be treated as a virtual BPF homepage.)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!--Series--&gt;

&lt;!--Doc1--&gt;

&lt;!--Doc2--&gt;

&lt;!--Doc3--&gt;

&lt;!--Doc4--&gt;

&lt;!--Doc5--&gt;

&lt;!--External--&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="NLP" /><category term="Microarchitecture" /><category term="UltraLowLatency" /><category term="RealTime" /><category term="Performance" /><category term="Meditating-with-microprocessors" /><summary type="html">The current article is part of a bigger series titled Meditating-with-microprocessors,in which I demonstrate the use of Artificial Intelligence to tune microprocessors for Ultra Low Latency and Realtime loads. There are 5 parts to the series Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1) , A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2), Trading off power for UltraLowLatency(part-3), Artificial Intelligence guided Predictive MicroProcessor tuning(part-4), Appendix:Tools of the trade(part-5) . In the current article I lay the technical groundwork for later articles in the series to build on. The term's and technologies I'll be using later must be understood really well. So we look at 3 concepts primarily. Firstly, Architecture of a modern microprocessor in short ,really really short. Just enough to make a mental model of the microprocessor to work with. Secondly, How does does software(Linux) interface with the microprocessor. Again just enough to make sense of the data we gather form the microprocessor using various UltraLowLatency profilers and tracers. Thirdly, help a programmers like you to get started.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22feature%22=%3E%22mwprocs/cpuidle-sub1.png%22%7D" /></entry><entry><title type="html">Meditating with microprocessors Series: Part-3: Trading off power for UltraLowLatency</title><link href="http://localhost:4000/articles/2020-07/MeditatingProcessor-3" rel="alternate" type="text/html" title="Meditating with microprocessors Series: Part-3: Trading off power for UltraLowLatency" /><published>2020-07-29T15:07:19+00:00</published><updated>2020-07-29T15:07:19+00:00</updated><id>http://localhost:4000/articles/2020-07/MeditatingProcessor-3</id><content type="html" xml:base="http://localhost:4000/articles/2020-07/MeditatingProcessor-3">&lt;h3 id=&quot;meditating-with-microprocessors&quot;&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating with microprocessors&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;part-1&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Meditation and microprocessors: An extremely simple idea&quot;&gt;Meditation and microprocessors: An extremely simple idea&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#MicroProcessor C-states&quot;&gt;MicroProcessor C-states&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#A quick Summary of wakeup_latency&quot;&gt;A quick Summary of wakeup_latency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Artificial Intelligence based Hardware(Miroprocessor) tuning&quot;&gt;Artificial Intelligence based Hardware(Miroprocessor) tuning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;part-2&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#MicroProcessor Components&quot;&gt;MicroProcessor Components&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Core&quot;&gt;Core&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&quot;&gt;Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Execution&quot;&gt;Sandy Bridge Pipeline:Execution&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Backend (Data load and Store)&quot;&gt;Sandy Bridge Pipeline:Backend (Data load and Store)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Haswell Pipeline&quot;&gt;Haswell Pipeline&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Uncore&quot;&gt;Uncore&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Linux Inteface to CPUIDLE&quot;&gt;Linux Inteface to CPUIDLE&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem&quot;&gt;CPUIDLE subsystem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Driver load&quot;&gt;CPUIDLE subsystem:Driver load&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Call the Driver&quot;&gt;CPUIDLE subsystem:Call the Driver&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Governor&quot;&gt;CPUIDLE subsystem:Governor&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;CPUIDLE subsystem:Gathering and undertanding latency data&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Trading off power for UltraLowLatency&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;part-3&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Core&quot;&gt;Core and Uncore:Core&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power&quot;&gt;Power&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things off&quot;&gt;Power:Turn things off&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:c-states&quot;&gt;Power:c-states&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned&quot;&gt;Power:Tuned&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:c-states requests&quot;&gt;Power:Tuned:c-states requests&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware State Residency&quot;&gt;Power:Tuned:Hardware State Residency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:c-states&quot;&gt;Power:Tuned:Influx:Grafana:c-states&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Measuring Latency&quot;&gt;Power:Tuned:Measuring Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware Latency&quot;&gt;Power:Tuned:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Wakeup Latency&quot;&gt;Power:Tuned:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:latency&quot;&gt;Power:Tuned:Influx:Grafana:latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:PMQOS&quot;&gt;Power:PMQOS&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down&quot;&gt;Power:Turn things down&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down:P-states:Hardware Latency&quot;&gt;Power:Turn things down:P-states:Hardware Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore&quot;&gt;Core and Uncore:Uncore&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:How much power can be saved&quot;&gt;Core and Uncore:How much power can be saved&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Artificial Intelligence guided Predictive MicroProcessor tuning&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;part-4&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a solution?&quot;&gt;Is there a solution?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a smarter solution?: Artificial Intelligence model based proactive decision making&quot;&gt;Is there a smarter solution?: Artificial Intelligence model based proactive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Recognizing the pattern&quot;&gt;Recognizing the pattern&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transformer as the backbone&quot;&gt;Transformer as the backbone&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transfer Learning inspiration from NLP&quot;&gt;Transfer Learning inspiration from NLP&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Fine tuning pre-trained networks&quot;&gt;Fine tuning pre-trained networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Artificial Intelligence model based proactive and predictive decision making&quot;&gt;Artificial Intelligence model based proactive and predictive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#The Final Cut&quot;&gt;The Final Cut&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools of the trade&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;part-5&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:An incisive but limited view&quot;&gt;Linux Tools:An incisive but limited view&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:uprobes and kprobes&quot;&gt;Linux Tools:Event Sources:uprobes and kprobes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:Tracepoints&quot;&gt;Linux Tools:Event Sources:Tracepoints&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools&quot;&gt;Linux Tools:Extraction Tools&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&quot;&gt;Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Engineering&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Engineering&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&quot;&gt;Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&quot;&gt;Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&quot;&gt;Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:Stack Trace Walking&quot;&gt;Linux Tools:Extraction Tools:BPF:Stack Trace Walking&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:A defining example&quot;&gt;Linux Tools:Extraction Tools:BPF:A defining example&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;&lt;a name=&quot;Introduction&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The current article is part of a bigger &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;series&lt;/a&gt;&lt;/strong&gt; titled &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating-with-microprocessors&lt;/a&gt;&lt;/strong&gt;, in which I demonstrate the use of &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; to tune &lt;strong&gt;microprocessors&lt;/strong&gt; for &lt;strong&gt;Ultra Low Latency&lt;/strong&gt; and &lt;strong&gt;Realtime&lt;/strong&gt; loads. There are 5 parts to the series, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Meditating with Microprocessors: An essentially simple idea(part-1)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Trading off power for UltraLowLatency (part-3)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Artificial Intelligence guided Predictive MicroProcessor tuning (part-4)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools of the trade (part-5)&lt;/a&gt;&lt;/strong&gt;.
In the current article, I present irrefutable evidence that if the processor is configured correctly with a goal in mind(for e.g. UltraLowLatency), the OS jitter caused by the processor can nearly be eliminated. The system can be much more predictable. Furthermore, It has a huge impact on the latency of the system for good. A &lt;strong&gt;substantial&lt;/strong&gt; improvement in latency due to configuration on the &lt;strong&gt;Core&lt;/strong&gt;, but beyond substantial due to  &lt;strong&gt;Uncore&lt;/strong&gt;. &lt;strong&gt;The improvement due to Uncore is to be expected because there is a whole lot more circuitry on the Uncore&lt;/strong&gt;. However, this is a trade off that comes at the cost of expending more power.&lt;/p&gt;

&lt;h1 id=&quot;-1&quot;&gt;&lt;a name=&quot;Core and Uncore:Core&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;core-and-uncorecore&quot;&gt;Core and Uncore:Core&lt;/h4&gt;
&lt;p&gt;We’ll look at &lt;a href=&quot;MeditatingProcessor-2#Core&quot;&gt;Core&lt;/a&gt; and &lt;a href=&quot;MeditatingProcessor-2#Uncore&quot;&gt;Uncore&lt;/a&gt; again but this time from the perspective of the power interface. How much
Power can be saved and how much is the tradeoff in terms of latency.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;On the core we have L1(data), L1(instruction), L2, Execution-units, and a whole lot of buffers. Maybe, you could quickly refresh the discussion on &lt;a href=&quot;MeditatingProcessor-2#Core&quot;&gt;Core&lt;/a&gt; before you proceed.&lt;/li&gt;
    &lt;li&gt;On the uncore we have L3 cache, integrated memory controller, QuickPath Interconnect (QPI; for multi-socket communication), and an interconnect that tied it all together. In the Sandy Bridge generation over and above Sandy Bridge, PCIe was integrated into the CPU uncore.&lt;/li&gt;
    &lt;li&gt;The complete package is called a package(core + uncore). &lt;strong&gt;core + uncore=package&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-2&quot;&gt;&lt;a name=&quot;Power&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;power&quot;&gt;Power&lt;/h4&gt;
&lt;p&gt;Active power of the CPU is when executing logic it transitions between ‘0’ and ‘1’. The transistors are charging and discharging to represent states. There are 2 primary components&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The power that runs the clock of the CPU&lt;/li&gt;
    &lt;li&gt;The power consumed by execution of logic&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are 2 ways to save power.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things off&quot;&gt;Turn things off&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down&quot;&gt;Turn things down&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-3&quot;&gt;&lt;a name=&quot;Power:Turn things off&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powerturn-things-off&quot;&gt;Power:Turn things off&lt;/h4&gt;
&lt;p&gt;Within the CPU there are 2 ways of doing this. &lt;strong&gt;(1)&lt;/strong&gt; Clock gating stops the clock, saving active power. The latency incurred is approximately 10ns-1μs. &lt;strong&gt;(2)&lt;/strong&gt; Power gating removes all power, saving both leakage and active power. The latency incurred is approximately 1μs-10μs.&lt;/p&gt;

&lt;h1 id=&quot;-4&quot;&gt;&lt;a name=&quot;Power:c-states&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powerc-states&quot;&gt;Power:c-states&lt;/h4&gt;
&lt;p&gt;Core c-states and package(core+uncore) c-states. Abbreviated as cc-states(c-states) and pc-states. So CPU designers have developed ways for the processor to go into a lower-power state when there is nothing for it to do. Typically, when put into this state, the CPU will stop clocks and power down part or all of its circuitry until the next interrupt arrives. Intel servers commonly target a worst-case of about 40 microseconds in order to restore the path to the main memory for PCIe devices.&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/ccpc.png&quot; alt=&quot;Image: figure-1: &amp;lt;strong&amp;gt;core c-states and package c-states&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: &lt;strong&gt;core c-states and package c-states&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-5&quot;&gt;&lt;a name=&quot;Power:Tuned&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powertuned&quot;&gt;Power:Tuned&lt;/h4&gt;
&lt;p&gt;Tuned-adm, developed by Redhat, allows tuning of certain settings in the operating system to suit certain use-cases. There are many predefined profiles to cater to common use cases. Custom profiles can also be defined from scratch or built on existing profiles. Few example profiles are “Low latency for storage and network”,  “high throughput for storage and network” etc.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;List of profiler.&lt;/li&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;spindown-disk&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;“/dev/cpu_dma_latency”&lt;/strong&gt; is the response time for the cpu in the current configuration.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned1.png&quot; alt=&quot;Image: figure-2: &amp;lt;strong&amp;gt;Tuned:Active:Spindown-disk&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: &lt;strong&gt;Tuned:Active:Spindown-disk&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;latency-performance&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;“/dev/cpu_dma_latency”&lt;/strong&gt; is the response time for the cpu in the current configuration.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned2.png&quot; alt=&quot;Image: figure-3: &amp;lt;strong&amp;gt;Tuned:Active:latency-performance&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: &lt;strong&gt;Tuned:Active:latency-performance&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-6&quot;&gt;&lt;a name=&quot;Power:Tuned:c-states requests&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powertunedc-states-requests&quot;&gt;Power:Tuned:c-states requests&lt;/h4&gt;
&lt;p&gt;But does this change have an impact on c-state request. Measurements are done using an excellent tool called &lt;a href=&quot;https://www.linux.org/docs/man8/turbostat.html&quot;&gt;turbostat&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;spindown-disk&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;c-state residency&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate1.png&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;Tuned:Active:Spindown-disk&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;Tuned:Active:Spindown-disk&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;spindown-disk&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;c-state residency&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;c-state residency: C10 has the highest number of requests. It is the lowest powersave mode or borderline Mokshaa.&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-3#Glossary of terms&quot;&gt;Glossary of terms&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate2.png&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;Tuned:Active:Spindown-disk&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;Tuned:Active:Spindown-disk&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;latency-performance&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;c-state residency&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate3.png&quot; alt=&quot;Image: figure-6: &amp;lt;strong&amp;gt;Tuned:Active:latency-performance&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: &lt;strong&gt;Tuned:Active:latency-performance&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-7&quot;&gt;&lt;a name=&quot;Power:Tuned:Hardware State Residency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powertunedhardware-state-residency&quot;&gt;Power:Tuned:Hardware State Residency&lt;/h4&gt;
&lt;p&gt;The hardware c-state are requests made by the operating system on behalf of software control policies. However, not all of them may be granted by hardware. Part of the reason for this is 2 logical processors ( Hyper Threads on the same physical core ) may not agree upon the c-states requested. The lower(C1 for e.g.) of the 2 c-state may be granted and the higher one(C6) would report as not granted. The core after all is the same unit. And unlike quantum physics, there is no duality here.&lt;/p&gt;

&lt;p&gt;The important thing is how do we measure hardware state residency. Well, there are &lt;strong&gt;MSRs (model-specific registers )&lt;/strong&gt; on the CPU that can be queried for this.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;MSRs (model-specific registers )&lt;/strong&gt; introduced in Sandy Bridge carried through to later microarchitecture pipelines.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Some register for measuring these values may change.&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html&quot;&gt;Intel’s documentation&lt;/a&gt; is the authentic source here. But, it is incredibly difficult to read. It is as if Intel’s employees were given a raise to make it “impossible to comprehend” kind of document.&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Prior to &lt;a href=&quot;https://en.wikipedia.org/wiki/Cannon_Lake_(microarchitecture)&quot;&gt;Cannonlake&lt;/a&gt; there was no MSR to measure C1 and had to be measure via softawre means. Post &lt;a href=&quot;https://en.wikipedia.org/wiki/Cannon_Lake_(microarchitecture)&quot;&gt;Cannonlake&lt;/a&gt; it can be measured 0x660H register.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate-hsr1.png&quot; alt=&quot;Image: figure-7: &amp;lt;strong&amp;gt;Tuned:Active:Hardware-State-Residency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: &lt;strong&gt;Tuned:Active:Hardware-State-Residency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;spindown-disk&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hardware State Residency&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-8&quot;&gt;&lt;a name=&quot;figure-8&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate-hsr2.png&quot; alt=&quot;Image: figure-8: &amp;lt;strong&amp;gt;Tuned:Active:spindown-disk:Tuned:Active::spindown-disk:Hardware-State-Residency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-8: &lt;strong&gt;Tuned:Active:spindown-disk:Tuned:Active::spindown-disk:Hardware-State-Residency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;latency-performance&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hardware State Residency&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-9&quot;&gt;&lt;a name=&quot;figure-9&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate-hsr3.png&quot; alt=&quot;Image: figure-9: &amp;lt;strong&amp;gt;Tuned:Active:spindown-disk:Tuned:Active::latency-performance:Hardware-State-Residency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-9: &lt;strong&gt;Tuned:Active:spindown-disk:Tuned:Active::latency-performance:Hardware-State-Residency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-10&quot;&gt;&lt;a name=&quot;Power:Tuned:Influx:Grafana:c-states&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powertunedinfluxgrafanac-states&quot;&gt;Power:Tuned:Influx:Grafana:c-states&lt;/h4&gt;

&lt;p&gt;This is visualizing &lt;strong&gt;c-states transitions&lt;/strong&gt; as a time series. We looked at this in the &lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;CPUIDLE subsystem:Gathering and undertanding latency data&lt;/a&gt; section. The processed CSV files are then moved to influxdb and visualized with grafana.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Tuned Low latency setting applied midway through the 30 second &lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt; recording&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/influx-graf.png&quot; alt=&quot;Image: figure-10: &amp;lt;strong&amp;gt;Tuned:ftrace-influx-grafana&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10: &lt;strong&gt;Tuned:ftrace-influx-grafana&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;This is a recording with &lt;a href=&quot;https://github.com/opcm/pcm&quot;&gt;PCM&lt;/a&gt; from Intel now open-sourced.&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;It has many dashboards showing various metrics. I am showing a couple of relevant ones like c-state Hardware Residency and L2/L3 Misses&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;These experiments have been done on my laptop with coffeelake client.(I have not been going to office because of covid19. The point is, that the difference is more stark on Xeon CPUs).&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/pcm-inf-graf1.png&quot; alt=&quot;Image: figure-11: &amp;lt;strong&amp;gt;Tuned:PCM-influx-grafana&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-11: &lt;strong&gt;Tuned:PCM-influx-grafana&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;With Low latency setting applied.&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/pcm-inf-graf2.png&quot; alt=&quot;Image: figure-12: &amp;lt;strong&amp;gt;Tuned:PCM-influx-grafana&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-12: &lt;strong&gt;Tuned:PCM-influx-grafana&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-11&quot;&gt;&lt;a name=&quot;Power:Tuned:Measuring Latency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powertunedmeasuring-latency&quot;&gt;Power:Tuned:Measuring Latency&lt;/h4&gt;

&lt;p&gt;Measuring latency is relatively easy. Just so that we are on the same page. Latency is the elapsed time between when an event should occur and it actually occurs. What is relatively more difficult is apportioning it to a subsystem. In modern computer systems, how or what do we apportion the measured latency to? Is it the Hardware, Operating System, or the Application causing it. And yet, there are underrated tools(ftrace) that are exactly for that purpose. But mind you, ftrace is a low-level tool, and post-processing of data may be required.&lt;/p&gt;

&lt;h1 id=&quot;-12&quot;&gt;&lt;a name=&quot;Power:Tuned:Hardware Latency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powertunedhardware-latency&quot;&gt;Power:Tuned:Hardware Latency&lt;/h4&gt;

&lt;p&gt;This is &lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt;’s hwlat tracer from the ftrace collection of tracers. What is does is simple yet elegant. It hogs a CPU in a busy loop taking timestamps for a configured amount of time. It does so with interrupts disabled. It then reports the gaps in the timestamp. Not all code paths are covered obviously but give us a fair idea of hardware latency.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;spindown-disk&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hardware Latency of the system&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate-hl1.png&quot; alt=&quot;Image: figure-13: &amp;lt;strong&amp;gt;Tuned:Active:spindown-disk:Tuned:Active:spindown-disk:Hardware Latency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-13: &lt;strong&gt;Tuned:Active:spindown-disk:Tuned:Active:spindown-disk:Hardware Latency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;latency-performance&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hardware Latency of the system&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate-hl2.png&quot; alt=&quot;Image: figure-14: &amp;lt;strong&amp;gt;Tuned:Active:spindown-disk:Tuned:Active:latency-performance:Hardware Latency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-14: &lt;strong&gt;Tuned:Active:spindown-disk:Tuned:Active:latency-performance:Hardware Latency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-13&quot;&gt;&lt;a name=&quot;Power:Tuned:Wakeup Latency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powertunedwakeup-latency&quot;&gt;Power:Tuned:Wakeup Latency&lt;/h4&gt;

&lt;p&gt;Wakeup tracer from &lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt; traces the elapsed time from a task being woken up to actually waking up. There are too many variables here, for e.g. other tasks in the queue. In order for this to make sense, one has to look at this relatively and pick the latency numbers for the same task.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;spindown-disk&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Wakeup Latency of the system&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Notice the latency of “__schedule”, you can interpret that as the latency of the OS scheduler.&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate-wul1.png&quot; alt=&quot;Image: figure-15: &amp;lt;strong&amp;gt;Tuned:Active:spindown-disk:Tuned:Active:spindown-disk:Wakeup Latency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-15: &lt;strong&gt;Tuned:Active:spindown-disk:Tuned:Active:spindown-disk:Wakeup Latency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Active Profile is &lt;strong&gt;latency-performance&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Wakeup Latency of the system&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Notice the latency of “__schedule”, you can interpret that as the latency of the OS scheduler.&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate-wul2.png&quot; alt=&quot;Image: figure-16: &amp;lt;strong&amp;gt;Tuned:Active:spindown-disk:Tuned:Active:latency-performance:Wakeup Latency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-16: &lt;strong&gt;Tuned:Active:spindown-disk:Tuned:Active:latency-performance:Wakeup Latency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-14&quot;&gt;&lt;a name=&quot;Power:Tuned:Influx:Grafana:latency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powertunedinfluxgrafanalatency&quot;&gt;Power:Tuned:Influx:Grafana:latency&lt;/h4&gt;

&lt;p&gt;This is visualizing &lt;strong&gt;latency&lt;/strong&gt; as a time series. We looked at this in the &lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;CPUIDLE subsystem:Gathering and undertanding latency data&lt;/a&gt; section. The processed CSV files are then moved to influxdb and visualized with grafana.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Tuned Low latency setting applied midway through the 22 second &lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt; recording&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/influx-graf2.png&quot; alt=&quot;Image: figure-17: &amp;lt;strong&amp;gt;Tuned:ftrace-influx-grafana:latency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17: &lt;strong&gt;Tuned:ftrace-influx-grafana:latency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-15&quot;&gt;&lt;a name=&quot;Power:PMQOS&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powerpmqos&quot;&gt;Power:PMQOS&lt;/h4&gt;

&lt;p&gt;This interface provides a kernel and user mode interface for registering performance expectations by drivers, subsystems, and user space applications on one of the parameters. Two different PM QoS(Power Management Quality of Service) frameworks are available:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;PM QoS classes for &lt;strong&gt;cpu_dma_latency, network_latency, network_throughput, memory_bandwidth.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The per-device PM QoS framework provides the API to manage the &lt;strong&gt;per-device latency constraints and PM QoS flags&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here are the benefits&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Provides flexibility, variety and degree of options&lt;/li&gt;
  &lt;li&gt;Uses available tools and infrastructure&lt;/li&gt;
  &lt;li&gt;Scalable and can be easily included early in application design&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let me simplify this. Refer back to &lt;a href=&quot;MeditatingProcessor-3#figure-8&quot;&gt;figure-8&lt;/a&gt; and  &lt;a href=&quot;MeditatingProcessor-3#figure-9&quot;&gt;figure-9&lt;/a&gt;. You’ll notice that the settings take effect for all the cores in the system. “pm_qos” is a much more fine-grained interface. Now let’s look at figure-10 below especially virtual cores 3 and 9.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Hardware-State-Residency with “pm_qos”&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Notice the latency of “__schedule”, you can interpret that as the latency of the OS scheduler.&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Allows user to specify a resume latency constraint&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;CPU idle governor limits C states with exit latencies lower than the constraint&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Application can change constraint at different phases&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;C states can be controlled in each core independently&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/tuned-cstate-pmqos.png&quot; alt=&quot;Image: figure-18: &amp;lt;strong&amp;gt;pm_qos:Hardware-State-Residency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-18: &lt;strong&gt;pm_qos:Hardware-State-Residency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-16&quot;&gt;&lt;a name=&quot;Power:Turn things down&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powerturn-things-down&quot;&gt;Power:Turn things down&lt;/h4&gt;
&lt;p&gt;Changing the frequency and voltage of a subset of the system: Traditionally these states have been focused on the cores, but changing the frequencies of other components of the CPU is also possible (such as a shared L3 cache). Execution can continue at varied performance and power levels when using &lt;strong&gt;P-states&lt;/strong&gt;. Voltage/frequency scaling If high frequency is not required, it can be dynamically reduced in order to achieve a lower power level. When frequency is reduced, it may also be possible to reduce the voltage. Voltage/frequency(vret) required to maintain state in a circuit can be lower than the voltage required to operate that circuit.&lt;/p&gt;

&lt;h1 id=&quot;-17&quot;&gt;&lt;a name=&quot;Power:Turn things down:P-states:Hardware Latency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;powerturn-things-downp-stateshardware-latency&quot;&gt;Power:Turn things down:P-states:Hardware Latency&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Frequency can be changed using tools like &lt;a href=&quot;https://github.com/RRZE-HPC/likwid&quot;&gt;likwid&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/pstate-lw1.png&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;likwid-setfrequency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;likwid-setfrequency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Low Frequency&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hardware Latency of the system&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/pstate-lw2.png&quot; alt=&quot;Image: figure-20: &amp;lt;strong&amp;gt;Hardware Latency: Low frequency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-20: &lt;strong&gt;Hardware Latency: Low frequency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;High Frequency&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hardware Latency of the system&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/pstate-lw3.png&quot; alt=&quot;Image: figure-21: &amp;lt;strong&amp;gt;Hardware Latency: High frequency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-21: &lt;strong&gt;Hardware Latency: High frequency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-18&quot;&gt;&lt;a name=&quot;Core and Uncore:Uncore&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;core-and-uncoreuncore&quot;&gt;Core and Uncore:Uncore&lt;/h4&gt;
&lt;p&gt;We’ll look at &lt;a href=&quot;MeditatingProcessor-2#Core&quot;&gt;Core&lt;/a&gt; and &lt;a href=&quot;MeditatingProcessor-2#Uncore&quot;&gt;Uncore&lt;/a&gt; again but this time from the perspective of the power interface. How much
power can be saved and how much is the tradeoff in terms of latency.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;On the uncore we have L3 cache, integrated memory controller, QuickPath Interconnect (QPI; for multi-socket communication), and an interconnect that tied it all together. In the Sandy Bridge generation over and above Sandy Bridge, PCIe was integrated into the CPU uncore.&lt;/li&gt;
    &lt;li&gt;However, just bu looking at the size and number or units on the &lt;a href=&quot;MeditatingProcessor-2#Uncore&quot;&gt;Uncore&lt;/a&gt;, the potential of latency improvements here are huge&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-19&quot;&gt;&lt;a name=&quot;Core and Uncore:Uncore:monitoring and Tuning&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;core-and-uncoreuncoremonitoring-and-tuning&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning&lt;/h4&gt;
&lt;p&gt;Unlike core performance monitoring, the performance monitoring architecture in the uncore is not standardized across all generations and product lines. A common architecture is used on Xeon E5/E7 products starting in the Sandy Bridge generation. Very few &lt;strong&gt;uncore&lt;/strong&gt; performance monitoring capabilities have been productized on other Intel products. We read from and write to the &lt;strong&gt;MSRs&lt;/strong&gt; directly. This is one of the reasons why less importance is paid to &lt;strong&gt;uncore&lt;/strong&gt;. The interface to &lt;strong&gt;uncore&lt;/strong&gt; is not standard. But good news is on the horizon, Intel’s UNCORE frequency driver may be part of mainline kernel as soon as &lt;strong&gt;kernel version 5.6&lt;/strong&gt;. Till such time &lt;strong&gt;MSRs&lt;/strong&gt; are our only hope as we are still on &lt;strong&gt;kernel version 4.15&lt;/strong&gt;. But we are itching to get on to 5 series.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;uncore&lt;/strong&gt; makes a far bigger difference because of it’s sheer size.&lt;/p&gt;

&lt;h1 id=&quot;-20&quot;&gt;&lt;a name=&quot;Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;core-and-uncoreuncoremonitoring-and-tuninghardware-latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Low Frequency&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hardware Latency of the system&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/uncore1.png&quot; alt=&quot;Image: figure-22: &amp;lt;strong&amp;gt;Uncore:Hardware Latency:Low frequency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-22: &lt;strong&gt;Uncore:Hardware Latency:Low frequency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;High Frequency&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hardware Latency of the system&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/uncore2.png&quot; alt=&quot;Image: figure-23: &amp;lt;strong&amp;gt;Uncore:Hardware Latency:High frequency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-23: &lt;strong&gt;Uncore:Hardware Latency:High frequency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-21&quot;&gt;&lt;a name=&quot;Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;core-and-uncoreuncoremonitoring-and-tuningwakeup-latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Low Frequency&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Wakeup Latency of the system&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/uncore3.png&quot; alt=&quot;Image: figure-24: &amp;lt;strong&amp;gt;Uncore:Wakeup Latency:Low frequency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-24: &lt;strong&gt;Uncore:Wakeup Latency:Low frequency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;High Frequency&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Wakeup Latency of the system&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/uncore4.png&quot; alt=&quot;Image: figure-25: &amp;lt;strong&amp;gt;Uncore:Wakeup Latency:High frequency&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-25: &lt;strong&gt;Uncore:Wakeup Latency:High frequency&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-22&quot;&gt;&lt;a name=&quot;Core and Uncore:How much power can be saved&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;core-and-uncorehow-much-power-can-be-saved&quot;&gt;Core and Uncore:How much power can be saved&lt;/h4&gt;

&lt;p&gt;But what about power are we saving? The whole premise here is that once the load has passed over, post-configure the setting back to power save settings again based on load predictions.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This has been measured in quad socket system using &lt;a href=&quot;https://github.com/opcm/pcm&quot;&gt;PCM&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/power.png&quot; alt=&quot;Image: figure-26: &amp;lt;strong&amp;gt;Power Save&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-26: &lt;strong&gt;Power Save&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Summary&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;Based on the experiments that I ran with my team, our conclusions are simple. If we consider latency, the OS jitter caused by the microprocessor per se can be almost completely eliminated. I only presented the evidence related to c-states and p-states. But this must be combined with &lt;strong&gt;isolcpus, affinity, irqaffinity, nohz_full, rcu_nocb&lt;/strong&gt;, etc to have a holistic approach to eliminating jitter. A similar approach can be used for storage and network devices.&lt;br /&gt;
We can meditate with the whole computer system, but that would be a much bigger article series than what it already is. Last but not least, latency is the goal for this article series and the most crucial type of load we handle. But it is not the only one, it could very well be throughput or something more custom.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Glossary of terms&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;glossary-of-terms&quot;&gt;Glossary of terms&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;PLL:&lt;/strong&gt;A phase locked loop, or PLL, is a circuit that is used to generate a stable frequency that has a specific mathematical relationship to some reference frequenc&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;VCCSA:&lt;/strong&gt; Starting with the second-generation Core i processors (“Sandy Bridge”), the VTT voltage was renamed to VCCSA, and is called “system agent(SA).”It feeds the integrated PCI Express controller, memory controller, and display engine (i.e., the “2D” part of the graphics engine)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;VCCIO:&lt;/strong&gt; Available starting with the second-generation Core i CPUs (“Sandy Bridge”), this voltage is used for feeding all input/output (I/O) pins of the CPU, except memory-related pins.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GT:&lt;/strong&gt; Integrated Grafics unit&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;IA:&lt;/strong&gt; Core CPU&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PS4:&lt;/strong&gt; Low power mode&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;VR:&lt;/strong&gt;  Voltage regulators&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/250967/&quot;&gt;&lt;strong&gt;What every programmer should know about memory:&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(This is a definitive 9 part(the links for the rest of the parts are embedded in the first one.) article on how the hardware works and, how software and data structure design can exploit it. It had a huge impact on the way I thought and still do think about design. The article first came out in 2007 but it is still relevant which is proof that basics don’t change very often.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html&quot;&gt;&lt;strong&gt;Intels documenation:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Intel’s documentation is the authentic source here. But, it is incredibly difficult to read. It is as if Intel’s employees were given a raise to make it “impossible to comprehend” kind of document.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.agner.org/optimize/&quot;&gt;&lt;strong&gt;Agner Fog:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (He benchmarks microprocessors using forward and reverse engineering techniques. My bible.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/torvalds/linux&quot;&gt;&lt;strong&gt;Linux Source:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (If you are going to trace your programs/applications then having the Linux source is a must. Tracers will tell you half the story, the other half will come from here. )&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;&lt;strong&gt;Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Modern Natural Language processing, in general, must be indebted to Transformer architecture. We however, use an asymmetric transformer setup.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.14794.pdf&quot;&gt;&lt;strong&gt;Performer-A Sparse Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (This is as cutting edge as it can get. The transformer at the heart of it is a stacked multi-headed-attention unit. As the sequences(of words or System events or stock prices or vehicle positions e.t.c.) get longer, the quadratic computation and quadratic memory for matrix cannot keep up. Performer, a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=93uE_kWWQjs&amp;amp;t=1178s&quot;&gt;&lt;strong&gt;Ftrace: The Inner workings&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( I dont think there is a better explaination of Ftrace’s inner workings.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/608497/&quot;&gt;&lt;strong&gt;Ftrace: The hidden light switch:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( This article demonstrates the tools based on Ftrace.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.brendangregg.com/ebpf.html&quot;&gt;&lt;strong&gt;BPF:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( eBPF or just BPF is changing the way programming is done on Linux. Linux now has observability superpowers beyond most OSes. A detailed post on BPF is the need of the hour and I am planning as much. In the meantime, the attached link can be treated as a virtual BPF homepage.)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!--Series--&gt;

&lt;!--Doc1--&gt;

&lt;!--Doc2--&gt;

&lt;!--Doc3--&gt;

&lt;!--Doc4--&gt;

&lt;!--Doc5--&gt;

&lt;!--External--&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="NLP" /><category term="Microarchitecture" /><category term="UltraLowLatency" /><category term="RealTime" /><category term="Performance" /><category term="Meditating-with-microprocessors" /><summary type="html">The current article is part of a bigger series titled Meditating-with-microprocessors,in which I demonstrate the use of Artificial Intelligence to tune microprocessors for Ultra Low Latency and Realtime loads. There are 5 parts to the series Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1) , A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2), Trading off power for UltraLowLatency(part-3), Artificial Intelligence guided Predictive MicroProcessor tuning(part-4), Appendix:Tools of the trade(part-5) . In the current article, I present irrefutable evidence that if the processor is configured correctly with a goal in mind(for e.g. UltraLowLatency), the OS jitter caused by the processor can nearly be eliminated. The system can be much more predictable. Furthermore, It has a huge impact on the latency of the system for good. A substantial improvement in latency due to configuration on the Core, but beyond substantial due to Uncore. The improvement due to Uncore is to be expected because there is a whole lot more circuitry on the Uncore. However, this is a trade off that comes at the cost of expending more power.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22feature%22=%3E%22mwprocs/pcm-inf-graf2.png%22%7D" /></entry><entry><title type="html">Meditating with microprocessors Series: Part-4: Artificial Intelligence guided Predictive MicroProcessor tuning</title><link href="http://localhost:4000/articles/2020-07/MeditatingProcessor-4" rel="alternate" type="text/html" title="Meditating with microprocessors Series: Part-4: Artificial Intelligence guided Predictive MicroProcessor tuning" /><published>2020-07-29T15:07:19+00:00</published><updated>2020-07-29T15:07:19+00:00</updated><id>http://localhost:4000/articles/2020-07/MeditatingProcessor-4</id><content type="html" xml:base="http://localhost:4000/articles/2020-07/MeditatingProcessor-4">&lt;h3 id=&quot;meditating-with-microprocessors&quot;&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating with microprocessors&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;part-1&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Meditation and microprocessors: An extremely simple idea&quot;&gt;Meditation and microprocessors: An extremely simple idea&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#MicroProcessor C-states&quot;&gt;MicroProcessor C-states&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#A quick Summary of wakeup_latency&quot;&gt;A quick Summary of wakeup_latency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Artificial Intelligence based Hardware(Miroprocessor) tuning&quot;&gt;Artificial Intelligence based Hardware(Miroprocessor) tuning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;part-2&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#MicroProcessor Components&quot;&gt;MicroProcessor Components&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Core&quot;&gt;Core&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&quot;&gt;Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Execution&quot;&gt;Sandy Bridge Pipeline:Execution&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Backend (Data load and Store)&quot;&gt;Sandy Bridge Pipeline:Backend (Data load and Store)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Haswell Pipeline&quot;&gt;Haswell Pipeline&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Uncore&quot;&gt;Uncore&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Linux Inteface to CPUIDLE&quot;&gt;Linux Inteface to CPUIDLE&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem&quot;&gt;CPUIDLE subsystem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Driver load&quot;&gt;CPUIDLE subsystem:Driver load&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Call the Driver&quot;&gt;CPUIDLE subsystem:Call the Driver&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Governor&quot;&gt;CPUIDLE subsystem:Governor&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;CPUIDLE subsystem:Gathering and undertanding latency data&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Trading off power for UltraLowLatency&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;part-3&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Core&quot;&gt;Core and Uncore:Core&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power&quot;&gt;Power&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things off&quot;&gt;Power:Turn things off&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:c-states&quot;&gt;Power:c-states&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned&quot;&gt;Power:Tuned&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:c-states requests&quot;&gt;Power:Tuned:c-states requests&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware State Residency&quot;&gt;Power:Tuned:Hardware State Residency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:c-states&quot;&gt;Power:Tuned:Influx:Grafana:c-states&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Measuring Latency&quot;&gt;Power:Tuned:Measuring Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware Latency&quot;&gt;Power:Tuned:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Wakeup Latency&quot;&gt;Power:Tuned:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:latency&quot;&gt;Power:Tuned:Influx:Grafana:latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:PMQOS&quot;&gt;Power:PMQOS&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down&quot;&gt;Power:Turn things down&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down:P-states:Hardware Latency&quot;&gt;Power:Turn things down:P-states:Hardware Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore&quot;&gt;Core and Uncore:Uncore&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:How much power can be saved&quot;&gt;Core and Uncore:How much power can be saved&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Artificial Intelligence guided Predictive MicroProcessor tuning&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;part-4&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a solution?&quot;&gt;Is there a solution?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a smarter solution?: Artificial Intelligence model based proactive decision making&quot;&gt;Is there a smarter solution?: Artificial Intelligence model based proactive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Recognizing the pattern&quot;&gt;Recognizing the pattern&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transformer as the backbone&quot;&gt;Transformer as the backbone&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transfer Learning inspiration from NLP&quot;&gt;Transfer Learning inspiration from NLP&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Fine tuning pre-trained networks&quot;&gt;Fine tuning pre-trained networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Artificial Intelligence model based proactive and predictive decision making&quot;&gt;Artificial Intelligence model based proactive and predictive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#The Final Cut&quot;&gt;The Final Cut&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools of the trade&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;part-5&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:An incisive but limited view&quot;&gt;Linux Tools:An incisive but limited view&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:uprobes and kprobes&quot;&gt;Linux Tools:Event Sources:uprobes and kprobes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:Tracepoints&quot;&gt;Linux Tools:Event Sources:Tracepoints&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools&quot;&gt;Linux Tools:Extraction Tools&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&quot;&gt;Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Engineering&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Engineering&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&quot;&gt;Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&quot;&gt;Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&quot;&gt;Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:Stack Trace Walking&quot;&gt;Linux Tools:Extraction Tools:BPF:Stack Trace Walking&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:A defining example&quot;&gt;Linux Tools:Extraction Tools:BPF:A defining example&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;&lt;a name=&quot;Introduction&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The current article is part of a bigger &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;series&lt;/a&gt;&lt;/strong&gt; titled &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating-with-microprocessors&lt;/a&gt;&lt;/strong&gt;, in which I demonstrate the use of &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; to tune &lt;strong&gt;microprocessors&lt;/strong&gt; for &lt;strong&gt;Ultra Low Latency&lt;/strong&gt; and &lt;strong&gt;Realtime&lt;/strong&gt; loads. There are 5 parts to the series, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Meditating with Microprocessors: An essentially simple idea(part-1)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Core (part-3)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Uncore (part-4)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools (part-5)&lt;/a&gt;&lt;/strong&gt;.
In the current article, we explore the use of Artificial Intelligence to reduce latency and save power at the same time. As we have understood, left to itself, the microprocessor will sense the load and configure itself for it(suitable c-states, P-states, etc. ). &lt;strong&gt;However, this process is reactive at best, and if latency matters then it is already late. By reactive, I mean the arrival of a remote message triggers an interrupt and from there, one thing leads to another before the microprocessor configures itself for the load. Causal is another word for it.&lt;/strong&gt; Let me put this in perspective. On a modern microprocessor with an average clock speed of roughly 2.5 gigahertz, there are 2500000000 cycles executed per second or 2.5 cycles per nanoseconds. A single instruction can take anywhere between a single cycle to a few tens of cycles depending upon the complexity of the instruction. At this granularity, the moment the CPU can spare a few hundred to a few thousands of nanoseconds of no work, it looks for an opportunity to get into a deep c-state or a low p-state. This is all guided by the policy of the system enforced by the OS. As a result, there are far too many transitions in a short time, costing valuable time in terms of latency and inducing jitter. We are not even talking about a millisecond granularity. Here is &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-1#c-state transitions&quot;&gt;an example&lt;/a&gt;&lt;/strong&gt; of roughly 25000 transitions in a period of 15 seconds. &lt;strong&gt;With an Artificial Intelligence based model can we reduce latency by predicting load and preconfigure the microprocessor for load and then postconfigure it to save power once the load has tided over. In other words making the process more proactive and predictive.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Is there a solution?&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;is-there-a-solution&quot;&gt;Is there a solution?&lt;/h4&gt;

&lt;p&gt;Well, one solution can be the other extreme. Configure the CPU such that it can never transition out to deeper c-states or lower p-states. This is what I did in the earlier articles. However, this causes a power drain, especially if done for a long time.
The other issue is crossing the Thermal Design Point(TDP). A TDP condition identifies this (power, frequency) pair. This is the limit, operating under which the CPU will never exceed its thermal envelope. TDP marks the worst-case real workload that a customer may run without causing thermal throttling. However, TDP is not a standard that every manufacturer agrees on and is often underestimated. This results in applications causing the CPU to exceed its TDP oftentimes. On average, everything still works fine but occasionally things may go wrong(I burnt one of my laptops(Not the CPU) 9-10 years ago testing things out).
Lastly, long term wear and tear may be an issue if the CPU deepest state is C0 and the CPU is overclocked for a long time or permanently. That being said, most modern devices are fairly tolerant of overclocking.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Is there a smarter solution?: Artificial Intelligence model based proactive decision making&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;is-there-a-smarter-solution-artificial-intelligence-model-based-proactive-decision-making&quot;&gt;Is there a smarter solution?: Artificial Intelligence model based proactive decision making&lt;/h4&gt;

&lt;p&gt;The idea is quite simple. Use Artificial Intelligence to predict an oncoming load, and change the microprocessor settings to high performance to suit the load. Isolate the high-performance settings on a fraction of cores depending on the requirement and not the complete system. Reset it back to normal once the load has passed over. If an Artificial Intelligence model can recognize a pattern of load much before it occurs then we have a winner here. &lt;strong&gt;And we can then call it proactive instead of reactive or causal.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Recognizing the pattern&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;recognizing-the-pattern&quot;&gt;Recognizing the pattern&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;This part is a bit proprietary so the details will be sketchy but the ideas will be clear.&lt;strong&gt; Just like a signature or fingerprint is unique to a person, is there a unique signature to a load. And if a particular load has a unique signature, is there a way to identify it? Last but not least, the identification process must be the least interfering. Remember, the idea is to reduce latency. If the whole process of identification of load has a huge observer effect, then it may not be worth the effort. &lt;strong&gt;As it turns out, there are ways to capture the pattern of load in a non-intrusive way. And I take solid inspiration from NLP for that.&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Transformer as the backbone&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;transformer-as-the-backbone&quot;&gt;Transformer as the backbone&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;/tags/#NMT&quot;&gt;NeuralMachineTranslators&lt;/a&gt; (&lt;a href=&quot;/tags/#NMT&quot;&gt;NMT&lt;/a&gt;) based on &lt;a href=&quot;/tags/#LSTM&quot;&gt;LSTM&lt;/a&gt; had excellent results but had few major challenges. Foremost was the &lt;strong&gt;inability&lt;/strong&gt; to feed data parallelly that impacted not just the speed of training these models but also the amount of data fed in.
Transformer’s architecture in many ways changed the course of NLP with a lot of improvements over the previous generation &lt;a href=&quot;/tags/#NMT&quot;&gt;NMT&lt;/a&gt; architectures that were based on &lt;a href=&quot;/tags/#LSTM&quot;&gt;LSTM&lt;/a&gt;s. While Transformers retained the core encoder-decoder setup there were some vital changes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;First improvement was &lt;strong&gt;positional encoding&lt;/strong&gt;. Allowing sequential data to be fed in parallelly and yet retain positional information is one of the greatest simple ideas in AI in along time.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Using &lt;strong&gt;sinusoidal wave embeddings as position markers&lt;/strong&gt; is a solid idea. This allowed data to be fed in parallelly, but even more crucially a bidirectional context could be built without physically feeding the data in both directions.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;In terms of &lt;strong&gt;training speed alone this was light years faster especially with GPUs.&lt;strong&gt;With much faster training speeds, there are other positive side effects like a much bigger corpus of data could be trained with.&lt;/strong&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/nlpmp0.png&quot; alt=&quot;Image: figure-1: &amp;lt;strong&amp;gt;Transformer:Encoder-Decoder&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: &lt;strong&gt;Transformer:Encoder-Decoder&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Second big change was &lt;strong&gt;Multi-Headed-Attention&lt;/strong&gt;. Attention by itself has been in the industry for quite some time. However with Multi-Headed-Attention, the model can fixate on multiple patterns within a sequence. And the number of patterns depends on the configurable number of &lt;strong&gt;AttentionHeads&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/nlpmp2.png&quot; alt=&quot;Image: figure-2: &amp;lt;strong&amp;gt;Multi-headed Attention visualization(One head being highlighted)&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: &lt;strong&gt;Multi-headed Attention visualization(One head being highlighted)&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Transfer Learning inspiration from NLP&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;transfer-learning-inspiration-from-nlp&quot;&gt;Transfer Learning inspiration from NLP&lt;/h4&gt;
&lt;p&gt;I have taken a lot of inspiration from the use of Transfer Learning in Artificial Intelligence and especially in Natural Language Processing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Here are the key ideas&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Annotated and labeled data are difficult to find but &lt;strong&gt;unlabelled data is plentiful&lt;/strong&gt;. Wikipedia, Online books are just some examples.&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://ruder.io/transfer-learning/&quot;&gt;Transfer Learning&lt;/a&gt; exemplifies learning with unlabelled data.&lt;/li&gt;
    &lt;li&gt;The NLP breakthroughs of 2018 and 2019 were triggered in a big way by Transfer Learning’s application on &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;&gt;Transformers&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt; was a breakthrough performer in NLP when it had come out in 2018.&lt;/li&gt;
    &lt;li&gt;The idea of &lt;strong&gt;masking&lt;/strong&gt; was a breath of fresh air, inspiring other techniques like &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;&gt;SpecAugment&lt;/a&gt; used in speech processing.&lt;/li&gt;
    &lt;li&gt;One of the ideas behind Random masking is that the context(self-attention calculations) can be bidirectional. This is one of the major reasons for &lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt; to outperform &lt;a href=&quot;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;GPT&lt;/a&gt; which uses causal masking.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a name=&quot;BERT:A Pretrained Tranformers&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/nlpmp1.png&quot; alt=&quot;Image: figure-3: &amp;lt;strong&amp;gt;BERT:A Pretrained Tranformers&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: &lt;strong&gt;BERT:A Pretrained Tranformers&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The recent success of transfer learning was ignited in 2018 by &lt;strong&gt;&lt;a href=&quot;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;GPT&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;&gt;ULMFiT&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.05365&quot;&gt;ELMo&lt;/a&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt;&lt;/strong&gt;, and 2019 saw the development of a huge diversity of new methods like &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.08237&quot;&gt;XLNet&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;&gt;RoBERTa&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html&quot;&gt;ALBERT&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.11504&quot;&gt;MT-DNN&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;The rate of progress in the field has made it difficult to evaluate which improvements are most meaningful and how effective they are when combined.&lt;/li&gt;
    &lt;li&gt;Most &lt;strong&gt;pre-trained models drop either encoder or decoder from the transformer stack&lt;/strong&gt;. &lt;strong&gt;&lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt; drops the decoder&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;GPT&lt;/a&gt; drops the encoder&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;However, our experiments have shown that a complete stack &lt;strong&gt;(encoder-decoder) generally outperforms a decoder-only or an encoder-only stack&lt;/strong&gt;. &lt;strong&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;&gt;T5&lt;/a&gt;&lt;/strong&gt; from google also supports a similar architecture for generally better results.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;The above transformer networks are called pretrained networks&lt;/strong&gt;. BERT, GPT, T5 are examples of pretrained networks.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a name=&quot;Fine tuning pre-trained networks&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;fine-tuning-pre-trained-networks&quot;&gt;Fine tuning pre-trained networks&lt;/h4&gt;

&lt;p&gt;Maybe it is time for another quick analogy. Let’s say that I need a support staff to handle queries from our clients. When we do hire the support staff we assume a &lt;strong&gt;basic knowledge of conversational languages and current affairs&lt;/strong&gt;. Post hiring, however, we need to &lt;strong&gt;train them on our specific processes&lt;/strong&gt;. For e.g. if a particular issue has been solved for another client, this information is likely to be found in JIRA, LOGs, mailtrail e.t.c.  &lt;br /&gt;
The &lt;strong&gt;pre-trained transformer&lt;/strong&gt; of the previous section is analogous to &lt;strong&gt;basic knowledge of conversational languages&lt;/strong&gt;.
The &lt;strong&gt;fine-tuned transformer&lt;/strong&gt; is analogous to &lt;strong&gt;post hiring training specific to our processes&lt;/strong&gt;. The fine-tuning takes for granted pre-training and does not have to start from scratch. Also, the flexibility to train the interns for different specific tasks.&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/finetune.png&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;BERT:A Fine tuning example&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;BERT:A Fine tuning example&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Artificial Intelligence model based proactive and predictive decision making&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;artificial-intelligence-model-based-proactive-and-predictive-decision-making&quot;&gt;Artificial Intelligence model based proactive and predictive decision making&lt;/h4&gt;

&lt;p&gt;This journey has been exhausting but fruitful. Finally, We treat the sequence of events captured by a tracer similar to a sequence of words in a sentence in NLP. Apart from that, from an Artificial Intelligence perspective, the setup is nearly identical.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;As discussed in the previous section, our transformer setup is asymmetric. We have more encoders than decoders.&lt;/li&gt;
    &lt;li&gt;The reason being, we wanted to exploit encoder-decoder attention, without paying the price of a complete decoder stack. I believe &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;&gt;T5&lt;/a&gt; has a similar setup. It is a good trade-off in my opinion.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/asymmetrictransformer2.png&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;Custom Transformer setup&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;Custom Transformer setup&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The pre-training is standard. We have been doing performance work for quite some time. So we have lots of data. And the great thing about pre-training is it needs to be done off-line. This is the beauty of &lt;a href=&quot;https://ruder.io/transfer-learning/&quot;&gt;Transfer Learning&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Elapsed time is the implicit time signal&lt;/strong&gt; and &lt;strong&gt;no explicit positional encoding is applied&lt;/strong&gt; unlike NLP.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;The only possible trap is running pre-training with less data that there are chances of overfit.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/asymmetrictransformer3.png&quot; alt=&quot;Image: figure-6: &amp;lt;strong&amp;gt;System event sequence: Pretraining&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: &lt;strong&gt;System event sequence: Pretraining&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;The Final Cut&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;the-final-cut&quot;&gt;The Final Cut&lt;/h4&gt;

&lt;p&gt;Regardless of the fine-tuning process, the pre-trained checkpoint remains the same. The fine tuning process itself can have many variations. The variations may be in terms of the goal, regression or classification with both having their pros and cons. I’ll illustrate a regression version here.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The idea here is to select a state based on estimated idle time.&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;MeditatingProcessor-2#State of initialized device through sysfs:Residency States&quot;&gt;target residency&lt;/a&gt; is the minimum time the hardware must spend in the given state, including the time needed to enter it (which may be substantial), in order to save more energy than it would save by entering one of the shallower idle states instead.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/asymmetrictransformer5.png&quot; alt=&quot;Image: figure-7: &amp;lt;strong&amp;gt;System event sequence: Fine-tuning for regression&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: &lt;strong&gt;System event sequence: Fine-tuning for regression&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Summary&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;In summary, &lt;strong&gt;this is the bird’s-eye view of the working of our system&lt;/strong&gt;. We kept our focus on the core CPU for now but we plan to extend it to other components of the system. The Artificial Intelligence bit is quite interesting and we were helped a great deal by work done in the past on NLP. &lt;strong&gt;To the best of our knowledge, this has not been attempted before&lt;/strong&gt;.
Transformers are not the only model we have tried. We tried LSTMs with very encouraging results. However, as the data grew bigger, and with an inherent lack of parallelism in training, it got a tad bit slow. Inferencing was OK though. Last but not least, Deep Reinforcement Learning is another approach that we are currently integrating. We will post results in time to come.
Last year has been phenomenal in terms of Artificial Intelligence in general and Natural Language in Particular. But many of these models can be used in cross-discipline domains. They are just waiting to be tried out.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/250967/&quot;&gt;&lt;strong&gt;What every programmer should know about memory:&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(This is a definitive 9 part(the links for the rest of the parts are embedded in the first one.) article on how the hardware works and, how software and data structure design can exploit it. It had a huge impact on the way I thought and still do think about design. The article first came out in 2007 but it is still relevant which is proof that basics don’t change very often.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html&quot;&gt;&lt;strong&gt;Intels documenation:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Intel’s documentation is the authentic source here. But, it is incredibly difficult to read. It is as if Intel’s employees were given a raise to make it “impossible to comprehend” kind of document.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.agner.org/optimize/&quot;&gt;&lt;strong&gt;Agner Fog:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (He benchmarks microprocessors using forward and reverse engineering techniques. My bible.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/torvalds/linux&quot;&gt;&lt;strong&gt;Linux Source:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (If you are going to trace your programs/applications then having the Linux source is a must. Tracers will tell you half the story, the other half will come from here. )&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;&lt;strong&gt;Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Modern Natural Language processing, in general, must be indebted to Transformer architecture. We however, use an asymmetric transformer setup.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.14794.pdf&quot;&gt;&lt;strong&gt;Performer-A Sparse Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (This is as cutting edge as it can get. The transformer at the heart of it is a stacked multi-headed-attention unit. As the sequences(of words or System events or stock prices or vehicle positions e.t.c.) get longer, the quadratic computation and quadratic memory for matrix cannot keep up. Performer, a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=93uE_kWWQjs&amp;amp;t=1178s&quot;&gt;&lt;strong&gt;Ftrace: The Inner workings&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( I dont think there is a better explaination of Ftrace’s inner workings.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/608497/&quot;&gt;&lt;strong&gt;Ftrace: The hidden light switch:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( This article demonstrates the tools based on Ftrace.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.brendangregg.com/ebpf.html&quot;&gt;&lt;strong&gt;BPF:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( eBPF or just BPF is changing the way programming is done on Linux. Linux now has observability superpowers beyond most OSes. A detailed post on BPF is the need of the hour and I am planning as much. In the meantime, the attached link can be treated as a virtual BPF homepage.)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!--Series--&gt;

&lt;!--Doc1--&gt;

&lt;!--Doc2--&gt;

&lt;!--Doc3--&gt;

&lt;!--Doc4--&gt;

&lt;!--Doc5--&gt;

&lt;!-- NMT LSTM --&gt;

&lt;!--External--&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="NLP" /><category term="Microarchitecture" /><category term="UltraLowLatency" /><category term="RealTime" /><category term="Performance" /><category term="Meditating-with-microprocessors" /><summary type="html">The current article is part of a bigger series titled Meditating-with-microprocessors,in which I demonstrate the use of Artificial Intelligence to tune microprocessors for Ultra Low Latency and Realtime loads. There are 5 parts to the series Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1) , A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2), Trading off power for UltraLowLatency(part-3), Artificial Intelligence guided Predictive MicroProcessor tuning(part-4), Appendix:Tools of the trade (part-5) . In the current article, we explore the use of Artificial Intelligence to reduce latency and save power at the same time. As we have understood, left to itself, the microprocessor will sense the load and configure itself for it(suitable c-states, P-states, etc. ). However, this process is reactive at best, and if latency matters then it is already late. By reactive, I mean the arrival of a remote message triggers an interrupt and from there, one thing leads to another before the microprocessor configures itself for the load. Causal is another word for it. With an Artificial Intelligence based model can we reduce latency by predicting load and preconfigure the microprocessor for load and then postconfigure to save power once the load has tided over. In other words making the process more proactive and predictive.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22feature%22=%3E%22mwprocs/asymmetrictransformer3.png%22%7D" /></entry><entry><title type="html">Meditating with microprocessors Series: Part-5: Appendix:Tools of the trade(part-5)</title><link href="http://localhost:4000/articles/2020-07/MeditatingProcessor-5" rel="alternate" type="text/html" title="Meditating with microprocessors Series: Part-5: Appendix:Tools of the trade(part-5)" /><published>2020-07-29T15:07:19+00:00</published><updated>2020-07-29T15:07:19+00:00</updated><id>http://localhost:4000/articles/2020-07/MeditatingProcessor-5</id><content type="html" xml:base="http://localhost:4000/articles/2020-07/MeditatingProcessor-5">&lt;h3 id=&quot;meditating-with-microprocessors&quot;&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating with microprocessors&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;part-1&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Meditation and microprocessors: An extremely simple idea&quot;&gt;Meditation and microprocessors: An extremely simple idea&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#MicroProcessor C-states&quot;&gt;MicroProcessor C-states&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#A quick Summary of wakeup_latency&quot;&gt;A quick Summary of wakeup_latency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Artificial Intelligence based Hardware(Miroprocessor) tuning&quot;&gt;Artificial Intelligence based Hardware(Miroprocessor) tuning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-1#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;part-2&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#MicroProcessor Components&quot;&gt;MicroProcessor Components&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Core&quot;&gt;Core&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&quot;&gt;Sandy Bridge Pipeline:Frontend(Instruction load, decode, cache)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Execution&quot;&gt;Sandy Bridge Pipeline:Execution&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Sandy Bridge Pipeline:Backend (Data load and Store)&quot;&gt;Sandy Bridge Pipeline:Backend (Data load and Store)&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Haswell Pipeline&quot;&gt;Haswell Pipeline&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Uncore&quot;&gt;Uncore&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Linux Inteface to CPUIDLE&quot;&gt;Linux Inteface to CPUIDLE&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem&quot;&gt;CPUIDLE subsystem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Driver load&quot;&gt;CPUIDLE subsystem:Driver load&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Call the Driver&quot;&gt;CPUIDLE subsystem:Call the Driver&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Governor&quot;&gt;CPUIDLE subsystem:Governor&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;CPUIDLE subsystem:Gathering and undertanding latency data&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-2#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Trading off power for UltraLowLatency&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;part-3&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Core&quot;&gt;Core and Uncore:Core&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power&quot;&gt;Power&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things off&quot;&gt;Power:Turn things off&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:c-states&quot;&gt;Power:c-states&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned&quot;&gt;Power:Tuned&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:c-states requests&quot;&gt;Power:Tuned:c-states requests&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware State Residency&quot;&gt;Power:Tuned:Hardware State Residency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:c-states&quot;&gt;Power:Tuned:Influx:Grafana:c-states&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Measuring Latency&quot;&gt;Power:Tuned:Measuring Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Hardware Latency&quot;&gt;Power:Tuned:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Wakeup Latency&quot;&gt;Power:Tuned:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Tuned:Influx:Grafana:latency&quot;&gt;Power:Tuned:Influx:Grafana:latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:PMQOS&quot;&gt;Power:PMQOS&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down&quot;&gt;Power:Turn things down&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Power:Turn things down:P-states:Hardware Latency&quot;&gt;Power:Turn things down:P-states:Hardware Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore&quot;&gt;Core and Uncore:Uncore&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Hardware Latency&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&quot;&gt;Core and Uncore:Uncore:monitoring and Tuning:Wakeup Latency&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Core and Uncore:How much power can be saved&quot;&gt;Core and Uncore:How much power can be saved&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-3#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Artificial Intelligence guided Predictive MicroProcessor tuning&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;part-4&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a solution?&quot;&gt;Is there a solution?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Is there a smarter solution?: Artificial Intelligence model based proactive decision making&quot;&gt;Is there a smarter solution?: Artificial Intelligence model based proactive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Recognizing the pattern&quot;&gt;Recognizing the pattern&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transformer as the backbone&quot;&gt;Transformer as the backbone&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Transfer Learning inspiration from NLP&quot;&gt;Transfer Learning inspiration from NLP&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Fine tuning pre-trained networks&quot;&gt;Fine tuning pre-trained networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Artificial Intelligence model based proactive and predictive decision making&quot;&gt;Artificial Intelligence model based proactive and predictive decision making&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#The Final Cut&quot;&gt;The Final Cut&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-4#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools of the trade&lt;/a&gt; &lt;strong&gt;(&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;part-5&lt;/a&gt;)&lt;strong&gt;
&lt;/strong&gt;&lt;/strong&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:An incisive but limited view&quot;&gt;Linux Tools:An incisive but limited view&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:uprobes and kprobes&quot;&gt;Linux Tools:Event Sources:uprobes and kprobes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Event Sources:Tracepoints&quot;&gt;Linux Tools:Event Sources:Tracepoints&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools&quot;&gt;Linux Tools:Extraction Tools&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&quot;&gt;Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Engineering&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Engineering&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&quot;&gt;Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&quot;&gt;Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&quot;&gt;Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:Stack Trace Walking&quot;&gt;Linux Tools:Extraction Tools:BPF:Stack Trace Walking&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:A defining example&quot;&gt;Linux Tools:Extraction Tools:BPF:A defining example&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;MeditatingProcessor-5#Summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;&lt;a name=&quot;Introduction&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The current article is part of a bigger &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;series&lt;/a&gt;&lt;/strong&gt; titled &lt;strong&gt;&lt;a href=&quot;/tags/#Meditating-with-microprocessors&quot;&gt;Meditating-with-microprocessors&lt;/a&gt;&lt;/strong&gt;, in which I demonstrate the use of &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; to tune &lt;strong&gt;microprocessors&lt;/strong&gt; for &lt;strong&gt;Ultra Low Latency&lt;/strong&gt; and &lt;strong&gt;Realtime&lt;/strong&gt; loads. There are 5 parts to the series, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-1&quot;&gt;Meditating with Microprocessors: An essentially simple idea(part-1)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-2&quot;&gt;A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-3&quot;&gt;Core (part-3)&lt;/a&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-4&quot;&gt;Uncore (part-4)&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5&quot;&gt;Appendix:Tools (part-5)&lt;/a&gt;&lt;/strong&gt;. In the current article, we drill down into the inner workings of &lt;strong&gt;Ftrace&lt;/strong&gt; and &lt;strong&gt;EBPF&lt;/strong&gt;. With &lt;strong&gt;EBPF based techniques and frameworks&lt;/strong&gt; Linux tooling has gained &lt;strong&gt;tracing superpowers&lt;/strong&gt;. It has capabilities that now exceed that of Dtrace on Solaris. This is not an exhaustive section and it does not do a breadth-first scan of Linux tooling. I am highlighting a couple of tools, why they work for us, and what makes them special. Moreover, tools play a vital role in verifying or rebutting the theories we make about the systems we design. Building/designing a system is like building a beehive, little blocks you put together making it a whole. But these little blocks fit more snuggly if the theory is rock-solid. Tools provide us the evidence for that.&lt;/p&gt;

&lt;h1 id=&quot;-1&quot;&gt;&lt;a name=&quot;Linux Tools:An incisive but limited view&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsan-incisive-but-limited-view&quot;&gt;Linux Tools:An incisive but limited view&lt;/h4&gt;

&lt;p&gt;There is a &lt;a href=&quot;https://jvns.ca/blog/2017/07/05/linux-tracing-systems/&quot;&gt;great article&lt;/a&gt; that takes an expansive look at linux tooling and declutters the theory behind it. Here we look at the details of how a couple of them work and what makes them special.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The original picture can be found &lt;a href=&quot;https://drawings.jvns.ca/drawings/linux-tracing-1.png&quot;&gt;here&lt;/a&gt;. I have modified it to bit highlight our focus in this article.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/linuxtooling.png&quot; alt=&quot;Image: figure-1: &amp;lt;strong&amp;gt;Linux Tooling: Our focus&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: &lt;strong&gt;Linux Tooling: Our focus&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-2&quot;&gt;&lt;a name=&quot;Linux Tools:Event Sources:uprobes and kprobes&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsevent-sourcesuprobes-and-kprobes&quot;&gt;Linux Tools:Event Sources:uprobes and kprobes&lt;/h4&gt;
&lt;p&gt;The technique of dynamic instrumentation is quite old(1990s if my memory serves me right) and reasonably mature. However, its application to Linux is relatively recent(mid 2000). Similar to techniques used in debuggers. They are called uprobes , uretprobes, kprobes and kretprobes in Linux. uprobes and uretprobes are for user-space processes. uprobes are function entries and uretprobes are function exits. kprobes and kretprobes are for kernel space. The technique of application is nearly identical.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;When a CPU hits the breakpoint instruction, a trap occurs, the CPU’s registers are saved, and control passes to kprobes/uprobe via the notifier_call_chain mechanism.&lt;/li&gt;
    &lt;li&gt;The original instructions are then executed, and instruction flow resumes.&lt;/li&gt;
    &lt;li&gt;When the kprobe/uprobe is no longer needed, the original bytes are copied back to the target address, restoring the instructions to their original state.&lt;/li&gt;
    &lt;li&gt;Since &lt;strong&gt;kprobes&lt;/strong&gt; can probe into a running kernel code, it can change the register set, including instruction pointer. This operation requires maximum care, such as keeping the stack frame, recovering the execution path etc. Since it operates on a running kernel and needs &lt;strong&gt;deep knowledge of computer architecture and concurrent computing&lt;/strong&gt;, &lt;strong&gt;you can easily shoot your foot&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Instrumenting bash’s readline with bpftrace.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;gdb &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &amp;lt;bashpid&amp;gt;
file /usr/lib/debug/bin/bash
disas /r readline

&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;bpftrace &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'uprobe:/bin/bash:readline { @ = count() }'&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;gdb &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &amp;lt;bashpid&amp;gt;
file /usr/lib/debug/bin/bash
disas /r readline&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The figure below shows before and after instrumentation.&lt;/li&gt;
    &lt;li&gt;Notice the insertion of int3 breakpoint instruction.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/linuxtooling2.png&quot; alt=&quot;Image: figure-2: &amp;lt;strong&amp;gt;Uprobe in action&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: &lt;strong&gt;Uprobe in action&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;When instruction flow hits this breakpoint, the breakpoint handler checks if it was installed by kprobes, and, if so, executes a kprobe handler.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/linuxtooling3.png&quot; alt=&quot;Image: figure-3: &amp;lt;strong&amp;gt;Uprobe in action&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: &lt;strong&gt;Uprobe in action&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-3&quot;&gt;&lt;a name=&quot;Linux Tools:Event Sources:Tracepoints&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsevent-sourcestracepoints&quot;&gt;Linux Tools:Event Sources:Tracepoints&lt;/h4&gt;

&lt;p&gt;Tracepoints are used for kernel static instrumentation. They involve tracing calls that developers have inserted into the kernel code at logical places, which are then compiled into the kernel binary. &lt;strong&gt;Tracepoints are a burden for kernel developers&lt;/strong&gt; to maintain, and tracepoints are far more limited in scope than kprobes. The advantage is that tracepoints provide a &lt;strong&gt;stable API: Tools written to use tracepoints should continue working across newer kernel versions&lt;strong&gt;, whereas those written using kprobes may break if the traced function is renamed or changed.&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Try using tracepoints first, if available and sufficient, and turn to kprobes only as a backup.&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;A tracepoint is inserted using the TRACE_EVENT macro.&lt;/li&gt;
    &lt;li&gt;It inserts a callback in the kernel source that gets called with the tracepoint parameters as arguments.&lt;/li&gt;
    &lt;li&gt;Tracepoints added with the TRACE_EVENT macro allow ftrace or any other tracer to use them.&lt;/li&gt;
    &lt;li&gt;The callback inserts the trace at the calling tracer’s ring buffer.–To insert a new tracepoint into the Linux kernel, define a new header file with a special format.&lt;/li&gt;
    &lt;li&gt;By default, tracepoint kernel files are located in include/trace/events(scheduling related tracepoints are declared in (“include/trace/events/sched.h”)). If they are not located in this directory, then other configurations are necessary.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/linuxtools4.png&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;Tracepoints:TRACE_EVENT macro for sched_switch&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;Tracepoints:TRACE_EVENT macro for sched_switch&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Scheduling trace captured using tracepoint.&lt;/li&gt;
    &lt;li&gt;This particular tracepoint is &lt;strong&gt;sched_switch&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;trace_sched_switch&lt;/strong&gt; is statically placed in the code by the TRACE_EVENT macro.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/linuxtools5.png&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;Tracepoints:sched_switch trace by ftrace&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;Tracepoints:sched_switch trace by ftrace&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-4&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-tools&quot;&gt;Linux Tools:Extraction Tools&lt;/h4&gt;

&lt;p&gt;We will look at 2 of these tools. One of them quite old and other one relatively new.&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/linuxtools6.png&quot; alt=&quot;Image: figure-6: &amp;lt;strong&amp;gt;Extraction Tools&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: &lt;strong&gt;Extraction Tools&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-5&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsftracethe-coolest-tracing-dude-on-the-planet&quot;&gt;Linux Tools:Extraction Tools:Ftrace:The coolest tracing dude on the planet&lt;/h4&gt;

&lt;p&gt;Ftrace as the coolest tracing dude on the planet. What makes ftrace such a fire-cracker.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;It is a &lt;strong&gt;part of Linux kernel&lt;/strong&gt; since 2.6.28 (which in ancient times) and is the &lt;strong&gt;official Linux tracer&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;It means there is &lt;strong&gt;no installation or compatibility&lt;/strong&gt; overhead at all.&lt;/li&gt;
    &lt;li&gt;People in the &lt;strong&gt;embedded domain&lt;/strong&gt; &lt;strong&gt;(&lt;a href=&quot;https://www.raspberrypi.org/&quot;&gt;Raspberry Pi&lt;/a&gt;)&lt;/strong&gt; or even distributions like &lt;strong&gt;&lt;a href=&quot;https://www.busybox.net/&quot;&gt;Busybox&lt;/a&gt;&lt;/strong&gt; adore ftrace as it’s &lt;strong&gt;foot print&lt;/strong&gt; is &lt;strong&gt;negligible&lt;/strong&gt; and &lt;strong&gt;absolutely NIL installation requirements&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-6&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:Ftrace:Engineering&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsftraceengineering&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Engineering&lt;/h4&gt;

&lt;p&gt;How is it possible to instrument all(most) kernel functions with almost nil overhead. The way this is done, in my mind, is one of the best pieces of engineering that I have seen. Here is the &lt;a href=&quot;https://www.youtube.com/watch?v=93uE_kWWQjs&amp;amp;t=333s&quot;&gt;original video&lt;/a&gt; by &lt;a href=&quot;https://blogs.vmware.com/opensource/author/steven-rostedt/&quot;&gt;Steven Rostedt&lt;/a&gt; that describes ftrace engineering in all it’s gory details. Below is much simplified version of the same.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;When the Linux kernel is compiled with &lt;strong&gt;“-pg -mfentry”&lt;/strong&gt; option, the GCC compiler adds special &lt;strong&gt;“__fentry__”&lt;/strong&gt; call to all non-inlined functions.&lt;/li&gt;
    &lt;li&gt;All non inlined functions call &lt;strong&gt;“__fentry__”&lt;/strong&gt; function at the &lt;strong&gt;beginning of the function&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;The figure below shows &lt;strong&gt;“schedule”&lt;/strong&gt; when compiled with the above option.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/ftrace.png&quot; alt=&quot;Image: figure-7: &amp;lt;strong&amp;gt;Ftrace Engineering: __fentry__&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: &lt;strong&gt;Ftrace Engineering: __fentry__&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Recall, that the broad goal here is the trace kernel functions with minimal overhead when turned off. “__fentry__” is placed for exactly the same reason.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;But calling “__fentry__” function from every non inlined function&lt;/strong&gt; is going to be &lt;strong&gt;huge overhead&lt;/strong&gt; even if the function does not do anything.&lt;/li&gt;
    &lt;li&gt;The cost of calling and returning from nearly 40 thousand kernel function is nearly &lt;strong&gt;13-15%&lt;/strong&gt; and clearly not acceptable.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;“recordmcount.c”&lt;/strong&gt; reads the object file one at a time and finds the &lt;strong&gt;“__fentry__” call location(call-site address)&lt;/strong&gt; and creates an array called &lt;strong&gt;“mcount_loc&lt;/strong&gt;.”&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;“__start_mcount_loc” and “__stop_mcount_loc” are variables&lt;/strong&gt; that define the start and stop of a section in vmlinux object file. These are inserted into the vmlinux file by the linker and have all the call-site addresses.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;At boot time&lt;/strong&gt; we find the addresses of the functions in the array between “__start_mcount_loc” and “__stop_mcount_loc” and &lt;strong&gt;convert them to NOPs(figure-8)&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;GCC-5 adds -mnop-mcount for the same purpose.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/ftrace2.png&quot; alt=&quot;Image: figure-8: &amp;lt;strong&amp;gt;Ftrace Engineering: NOPs at boot time using array section at __start_mcount_loc &amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-8: &lt;strong&gt;Ftrace Engineering: NOPs at boot time using array section at __start_mcount_loc &lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;However, The array between “__start_mcount_loc” and “__stop_mcount_loc” is not enough for ftrace framework to work.&lt;/li&gt;
    &lt;li&gt;We need &lt;strong&gt;state information&lt;/strong&gt; along with the &lt;strong&gt;function addresses&lt;/strong&gt; to be stored per each kernel function to track the functions to be dynamically traced.&lt;/li&gt;
    &lt;li&gt;This state information is stored in pages using struct &lt;strong&gt;“ftrace_page”&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;Each &lt;strong&gt;“ftrace_page”&lt;/strong&gt; has multiple &lt;strong&gt;“dyn_ftrace”&lt;/strong&gt; as an array of structs. &lt;strong&gt;“ip” stores the class-site addresses&lt;/strong&gt; and the &lt;strong&gt;flags stores the state of that particular call-site&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;  struct ftrace_page &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  	struct ftrace_page &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;next&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  	struct dyn_ftrace	&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;records&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  	int			index&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  	int			size&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

  struct dyn_ftrace &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  	unsigned long		ip&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; /&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; address of mcount call-site &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/
  	unsigned long		flags&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  	struct dyn_arch_ftrace	arch&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Figure-9&lt;/strong&gt; below depicts &lt;strong&gt;“ftrace_page”&lt;/strong&gt; and &lt;strong&gt;array of struct “dyn_ftrace”&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;The memory consumed by &lt;strong&gt;“ftrace_pages”&lt;/strong&gt; is &lt;strong&gt;roughly 630 kilobytes&lt;/strong&gt; in approximately &lt;strong&gt;154 pages&lt;/strong&gt; for close to &lt;strong&gt;40 thousand kernel functions&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/ftrace3.png&quot; alt=&quot;Image: figure-9: &amp;lt;strong&amp;gt;Ftrace Engineering&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-9: &lt;strong&gt;Ftrace Engineering&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;struct “dyn_ftrace” &lt;strong&gt;field flags&lt;/strong&gt; holds the various flags for ftrace to work. For e.g. Bit 29 is to save the CPU registers while tracing, Bit 30 Needs to call ftrace_regs_caller, and Bit 31 means function is being traced.&lt;/li&gt;
    &lt;li&gt;Very long story short, while enabling tracing sometimes CPU registers have to be saved and sometimes not. This results in slightly different ftrace callbacks as shown figure-10.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/ftrace4.png&quot; alt=&quot;Image: figure-10: &amp;lt;strong&amp;gt;Ftrace Engineering&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10: &lt;strong&gt;Ftrace Engineering&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-7&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsftracesummary&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/h4&gt;

&lt;p&gt;Nearly 40 thousand traceable functions in the kernel with negligible overhead is some engineering feet. As you opt-in for events or functions to be traced your overhead goes up in a pay-as-you-go manner. Add to that is the fact that it comes baked in with the kernel. This is not a tutorial of ftrace. Will think of planning one later.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Can be used with “cat” and “echo”.&lt;/li&gt;
    &lt;li&gt;Interface is the tracefs/debugfs usually mounted at “/sys/kernel/debug/tracing”. It may be slightly different depending on your distribution.&lt;/li&gt;
    &lt;li&gt;Last but not least, ftrace is not just a function tracer. It is a &lt;strong&gt;collection of tracers&lt;/strong&gt; including &lt;strong&gt;Hardware Latency Tracer(hwlat)&lt;/strong&gt;, &lt;strong&gt;Wakeup(wakeup)&lt;/strong&gt; and many more.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/ftrace5.png&quot; alt=&quot;Image: figure-11: &amp;lt;strong&amp;gt;Ftrace Example&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-11: &lt;strong&gt;Ftrace Example&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Finally ftrace comes with a few front end tools like the &lt;a href=&quot;https://github.com/rostedt/trace-cmd&quot;&gt;command-line trace-cmd&lt;/a&gt; and a &lt;a href=&quot;https://kernelshark.org/&quot;&gt;visualizer called kernelshark&lt;/a&gt;. However, understanding the data captured by a tracer is often much more important. The the subsystem being traced is understood, then the captured data can be &lt;a href=&quot;MeditatingProcessor-2#CPUIDLE subsystem:Gathering and undertanding latency data&quot;&gt;post processed&lt;/a&gt; to suit any visualizer.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-8&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsbpfthe-most-powerful-and-flexible-tracer&quot;&gt;Linux Tools:Extraction Tools:BPF:The most powerful and flexible tracer&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;EBPF(Extended Berkeley Packet Filters)&lt;/strong&gt; or just &lt;strong&gt;BPF(Berkeley Packet Filters)&lt;/strong&gt; is like a &lt;strong&gt;swiss army knife of performance tools&lt;/strong&gt;. I am avoiding it’s history completely in the interest of brevity. There is an excellent resource on &lt;a href=&quot;http://www.brendangregg.com/ebpf.html&quot;&gt;BPF&lt;/a&gt; by one of it’s &lt;a href=&quot;http://www.brendangregg.com/&quot;&gt;authors&lt;/a&gt;. To be more precise, the author of tools like &lt;a href=&quot;https://github.com/iovisor/bcc&quot;&gt;bcc&lt;/a&gt; and &lt;a href=&quot;https://github.com/iovisor/bpftrace&quot;&gt;bpftrace&lt;/a&gt; built on top of BPF. As an aside, the only thing that does not resonate with me is its name(BPF…Really?? ). We could have a more attractive name but I guess that horse has already bolted. It is a highly capable tool and is one reason why it is a bit difficult to explain. This post is about &lt;strong&gt;it’s engineering&lt;/strong&gt;, what &lt;strong&gt;makes it capable&lt;/strong&gt; and a few examples to &lt;strong&gt;illustrate it superpowers over other tools&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;BPF has many capabilities. But here we focus on a couple of it’s defining features.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;It’s ability to post-process data in kernel safely and securely in a &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&quot;&gt;BPF Virtual Machine&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;It’s &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&quot;&gt;In Kernel rich Data Structures&lt;/a&gt;&lt;/strong&gt; for ease of processing.&lt;/li&gt;
  &lt;li&gt;It’s detailed &lt;strong&gt;&lt;a href=&quot;MeditatingProcessor-5#Linux Tools:Extraction Tools:BPF:Stack Trace Walking&quot;&gt;Stack Trace Walking&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;-9&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsbpfbpf-virtual-machine&quot;&gt;Linux Tools:Extraction Tools:BPF:BPF Virtual Machine&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The post-processing carried out by &lt;strong&gt;BPF Virtual Machine&lt;/strong&gt; has been attempted by many techniques before. One such technique was to use kernel modules. However, they were all not safe in production because they were all executed in a running kernel with practically no checks.&lt;/li&gt;
    &lt;li&gt;BPF provides a &lt;strong&gt;BPF Virtual Machine&lt;/strong&gt; with a verifier that checks for harmful code before executing them.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/bpf11.png&quot; alt=&quot;Image: figure-12: &amp;lt;strong&amp;gt;BPF Virtual Machine&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-12: &lt;strong&gt;BPF Virtual Machine&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The &lt;strong&gt;BPF Virtual Machine&lt;/strong&gt; has a verifier that reject unsafe operations.&lt;/li&gt;
    &lt;li&gt;A Virtual Machine does not necessarily mean slow execution.&lt;/li&gt;
    &lt;li&gt;A JIT compiler generates native instruction for direct and faster execution.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/bpf12.png&quot; alt=&quot;Image: figure-13: &amp;lt;strong&amp;gt;BPF Virtual Machine&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-13: &lt;strong&gt;BPF Virtual Machine&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Figure-14 shows a classic BPF before and after example.&lt;/li&gt;
    &lt;li&gt;In kernel summary and in general in kernel pre/post execution/processing opens up enough and more avenues for making excellent tools for all kinds of purposes.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/bpf13.png&quot; alt=&quot;Image: figure-14: &amp;lt;strong&amp;gt;BPF Virtual Machine&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-14: &lt;strong&gt;BPF Virtual Machine&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-10&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsbpfin-kernel-rich-data-structures&quot;&gt;Linux Tools:Extraction Tools:BPF:In Kernel rich Data Structures&lt;/h4&gt;

&lt;p&gt;BPF provides &lt;strong&gt;rich data structures&lt;/strong&gt; via &lt;strong&gt;maps(Figure-12,13), histograms(Figure-12,13)&lt;/strong&gt;, e.t.c to process in kernel summaries. Many tools like &lt;a href=&quot;https://perf.wiki.kernel.org/index.php/Tutorial&quot;&gt;Perf&lt;/a&gt; can gather data from &lt;a href=&quot;https://en.wikipedia.org/wiki/Hardware_performance_counter&quot;&gt;PMC&lt;/a&gt;s but they have to push it the user space to be post-processed. What this means is a lot more data get transferred to the user-land. With rich data structures and in-kernel processing, this can be minimized. Also, the ability to in-kernel processing by itself is awesome and opens up huge possibilities.&lt;/p&gt;

&lt;h1 id=&quot;-11&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:BPF:Stack Trace Walking&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsbpfstack-trace-walking&quot;&gt;Linux Tools:Extraction Tools:BPF:Stack Trace Walking&lt;/h4&gt;

&lt;p&gt;Stack traces are an invaluable tool for understanding the code path that led to an event, as well as profiling kernel and user code to observe where execution time is spent. While there are other ways to walk the stack, framepointer based stack walking is quite popular and BPF uses it.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This is how a call is setup in X86.&lt;/li&gt;
    &lt;li&gt;The figure-15 depicts the address to jump back to when the callee has finished execution.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/bpfsw1.png&quot; alt=&quot;Image: figure-15: &amp;lt;strong&amp;gt;Coming Soon&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-15: &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This figure-16 depicts the stack trace walking using frame pointers.&lt;/li&gt;
    &lt;li&gt;This will show the trace leading up to the event.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/bpfsw2.png&quot; alt=&quot;Image: figure-16: &amp;lt;strong&amp;gt;Coming Soon&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-16: &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Some code in C++ to traverse the frame, however, this would usually be acomplished by a tool. BPF in our case.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/bpfsw3.png&quot; alt=&quot;Image: figure-17: &amp;lt;strong&amp;gt;Coming Soon&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17: &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-12&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:BPF:A defining example&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsbpfa-defining-example&quot;&gt;Linux Tools:Extraction Tools:BPF:A defining example&lt;/h4&gt;

&lt;p&gt;This example explores the tracing of kernel function and even traversing and processing kernel structures. This would not have been possible earlier without compromising the safety of the kernel. BPF makes this possible.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Consider &lt;strong&gt;“sched_idle_set_state”&lt;/strong&gt; kernel function. It is a part of CPUIDLE generic driver interface. It takes the &lt;strong&gt;struct “cpuidle_state” as parameter&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;Tracing the above function with ftrace I am only able to access the “cpuidle_state” &lt;strong&gt;parameter as an address&lt;/strong&gt; as it is a structure that is of no practical use. This is illustrated in figure-18 below.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/bpf1.png&quot; alt=&quot;Image: figure-18: &amp;lt;strong&amp;gt;Tracing sched_idle_set_state parameters&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-18: &lt;strong&gt;Tracing sched_idle_set_state parameters&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Identical setup as above, except this time I use a BPF script to trace and traverse the struct “cpuidle_state”.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/mwprocs/bpf2.png&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;Tracing sched_idle_set_state parameters with BPF&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;Tracing sched_idle_set_state parameters with BPF&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-13&quot;&gt;&lt;a name=&quot;Linux Tools:Extraction Tools:Ftrace:Summary&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;linux-toolsextraction-toolsftracesummary-1&quot;&gt;Linux Tools:Extraction Tools:Ftrace:Summary&lt;/h4&gt;

&lt;p&gt;BPF performance tools make use of many technologies, including extended BPF, kernel and user dynamic instrumentation (kprobes and uprobes), kernel and user static tracing (tracepoints and user markers), and perf_events.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Summary&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;These are not the only tools. There are a couple of &lt;strong&gt;notable omissions&lt;/strong&gt;. The biggest one being &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Hardware_performance_counter&quot;&gt;Performance Monitoring Counters&lt;/a&gt;&lt;/strong&gt; as an event source and &lt;strong&gt;&lt;a href=&quot;https://perf.wiki.kernel.org/index.php/Tutorial&quot;&gt;PERF&lt;/a&gt;&lt;/strong&gt; and an extraction tool. It is a &lt;strong&gt;frequency-based profiling tool&lt;/strong&gt; and something we use heavily as one can &lt;strong&gt;control the frequency of recording to vary the overhead of the recording being done&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In summary, these are tools that we take to the trenches of performance tuning and monitoring. The important thing being that if used right they have the capability to transform your understanding and take it to the next level. &lt;strong&gt;By the next level I mean, the ability to use this information to impress upon the design stage of your application&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;“&lt;a href=&quot;MeditatingProcessor-5#Ftrace:The coolest tracing dude on the planet&quot;&gt;ftrace&lt;/a&gt;” for example is an always installed &lt;strong&gt;(on most Linux distributions)&lt;/strong&gt; and can be used with just &lt;strong&gt;“echo”&lt;/strong&gt; and &lt;strong&gt;“cat”&lt;/strong&gt;. Yes, It does not have a fancy UI but the captured data can be post-processed to be rendered by any visualizer. &lt;strong&gt;Here are  few sample csv files(&lt;a href=&quot;/img/mwprocs/cpu0.csv&quot;&gt;file1&lt;/a&gt;,&lt;a href=&quot;/img/mwprocs/cpu5.csv&quot;&gt;file2&lt;/a&gt;)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;[BPF] is a different beast altogether. It can look inside the kernel and most userland applications including Java/JVM python with kprobes/uprobes/USDTs respectively. &lt;strong&gt;It’s safe in-kernel/in-application processing is a powerful feature that lends modern Linux Kernel it’s amazing superpowers&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/250967/&quot;&gt;&lt;strong&gt;What every programmer should know about memory:&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(This is a definitive 9 part(the links for the rest of the parts are embedded in the first one.) article on how the hardware works and, how software and data structure design can exploit it. It had a huge impact on the way I thought and still do think about design. The article first came out in 2007 but it is still relevant which is proof that basics don’t change very often.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html&quot;&gt;&lt;strong&gt;Intels documenation:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Intel’s documentation is the authentic source here. But, it is incredibly difficult to read. It is as if Intel’s employees were given a raise to make it “impossible to comprehend” kind of document.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.agner.org/optimize/&quot;&gt;&lt;strong&gt;Agner Fog:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (He benchmarks microprocessors using forward and reverse engineering techniques. My bible.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/torvalds/linux&quot;&gt;&lt;strong&gt;Linux Source:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (If you are going to trace your programs/applications then having the Linux source is a must. Tracers will tell you half the story, the other half will come from here. )&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;&lt;strong&gt;Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (Modern Natural Language processing, in general, must be indebted to Transformer architecture. We however, use an asymmetric transformer setup.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.14794.pdf&quot;&gt;&lt;strong&gt;Performer-A Sparse Transformer:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; (This is as cutting edge as it can get. The transformer at the heart of it is a stacked multi-headed-attention unit. As the sequences(of words or System events or stock prices or vehicle positions e.t.c.) get longer, the quadratic computation and quadratic memory for matrix cannot keep up. Performer, a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=93uE_kWWQjs&amp;amp;t=1178s&quot;&gt;&lt;strong&gt;Ftrace: The Inner workings&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( I dont think there is a better explaination of Ftrace’s inner workings.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lwn.net/Articles/608497/&quot;&gt;&lt;strong&gt;Ftrace: The hidden light switch:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( This article demonstrates the tools based on Ftrace.)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.brendangregg.com/ebpf.html&quot;&gt;&lt;strong&gt;BPF:&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; ( eBPF or just BPF is changing the way programming is done on Linux. Linux now has observability superpowers beyond most OSes. A detailed post on BPF is the need of the hour and I am planning as much. In the meantime, the attached link can be treated as a virtual BPF homepage.)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!--Series--&gt;

&lt;!--Doc1--&gt;

&lt;!--Doc2--&gt;

&lt;!--Doc3--&gt;

&lt;!--Doc4--&gt;

&lt;!--Doc5--&gt;
&lt;!--External--&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="NLP" /><category term="Microarchitecture" /><category term="UltraLowLatency" /><category term="RealTime" /><category term="Performance" /><category term="Meditating-with-microprocessors" /><summary type="html">The current article is part of a bigger series titled Meditating-with-microprocessors,in which I demonstrate the use of Artificial Intelligence to tune microprocessors for Ultra Low Latency and Realtime loads. There are 5 parts to the series Artificial Intelligence based Hardware(Microprocessor) tuning: Implementing a very simple idea(part-1) , A crashcourse in Microarchitecture and Linux CPUIDLE interface(part-2), Trading off power for UltraLowLatency(part-3), Artificial Intelligence guided Predictive MicroProcessor tuning(part-4), Appendix:Tools of the trade(part-5) . In the current article, we drill down into the inner workings of Ftrace and EBPF. With EBPF based techniques and frameworks Linux tooling has gained tracing superpowers. It has capabilities that now exceed that of Dtrace on Solaris. This is not an exhaustive section and it does not do a breadth-first scan of Linux tooling. I am highlighting a couple of tools, why they work for us, and what makes them special. Moreover, tools play a vital role in verifying or rebutting the theories we make about the systems we design. Building/designing a system is like building a beehive, little blocks you put together making it a whole. But these little blocks fit more snuggly if the theory is rock-solid. Tools provide us the evidence for that.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22feature%22=%3E%22mwprocs/bpfsw2.png%22%7D" /></entry><entry><title type="html">Natural Language Processing Series: Neural Machine Translation(NMT):Part-1: Highly Simplified, completely Pictorial understanding of Neural Machine Translation</title><link href="http://localhost:4000/articles/2019-09/NMTPart1" rel="alternate" type="text/html" title="Natural Language Processing Series: Neural Machine Translation(NMT):Part-1: Highly Simplified, completely Pictorial understanding of Neural Machine Translation" /><published>2019-09-19T15:07:19+00:00</published><updated>2019-09-19T15:07:19+00:00</updated><id>http://localhost:4000/articles/2019-09/NMTPart1</id><content type="html" xml:base="http://localhost:4000/articles/2019-09/NMTPart1">&lt;h1&gt;&lt;a name=&quot;Multi layer RNNs backward propagation&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In this &lt;strong&gt;&lt;a href=&quot;/tags/#NMT&quot;&gt;multi-part series&lt;/a&gt;&lt;/strong&gt; we look at Neural Machine Translation, the technology behind how machines understand humans. The &lt;strong&gt;&lt;a href=&quot;NMTPart1&quot;&gt;first of which(current article)&lt;/a&gt;&lt;/strong&gt; is a completely pictorial and a non technical take on the technology and its potential.
The rest of the parts will be a rigorous &lt;strong&gt;under-the-skin (math,code and logic)&lt;/strong&gt; look at &lt;a href=&quot;/comingsoon.html&quot;&gt;&lt;strong&gt;Neural Machine Translators&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;/comingsoon.html&quot;&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/a&gt; and, the cherry on the cake &lt;a href=&quot;/comingsoon.html&quot;&gt;&lt;strong&gt;Googles Neural Machine Translators&lt;/strong&gt;&lt;/a&gt;. The rigorous &lt;strong&gt;under-the-skin (math,code and logic)&lt;/strong&gt; look will enable you to recreate the system for your own needs.&lt;/p&gt;

&lt;p&gt;In the &lt;strong&gt;&lt;a href=&quot;NMTPart1&quot;&gt;current article&lt;/a&gt;&lt;/strong&gt;, we take a &lt;strong&gt;highly simplified&lt;/strong&gt;, &lt;strong&gt;completely pictorial&lt;/strong&gt; look at &lt;strong&gt;Artificial Intelligence(AI)&lt;/strong&gt; based translation systems from one &lt;strong&gt;Natural Language&lt;/strong&gt; to another. Natural language, as in the way humans speak and how far have we come in &lt;strong&gt;designing machines that understand and act on it&lt;/strong&gt;. Once you have a solid understanding of systems like these, we take a look at where are we currently applying it to(&lt;a href=&quot;NMTPart1#Case Studies&quot;&gt;case studies&lt;/a&gt;), to broaden the horizons.&lt;/p&gt;

&lt;p&gt;Artificial Intelligence(AI) is a vast subject. To explain in short and without getting technical is even more difficult. However, if we confine ourselves to examples in a particular domain, it might be a lot easier. Once we understand AI in the context of our chosen example, we could then broaden the horizon.  In our case, we consider software that translates text from one language to another. Let’s look at a quick history first.&lt;/p&gt;

&lt;p&gt;There are multiple parts to the article:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Machine Translation: A quick history&quot;&gt;Machine Translation: A quick history&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Machine Translation(MT)&quot;&gt;Machine Translation(MT)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Rule-based Machine Translation(RBMT)&quot;&gt;Rule-based Machine Translation(RBMT)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Statistical Machine Translation(SMT)&quot;&gt;Statistical Machine Translation(SMT)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Neural Machine Translators(NMT)&quot;&gt;Neural Machine Translators(NMT)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Neural Machine Translators(NMT) with Attention&quot;&gt;Neural Machine Translators(NMT) with Attention&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Process of training&quot;&gt;Process of training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Jump to accuracy first&quot;&gt;Jump to accuracy first&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Generality&quot;&gt;Generality&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Practice makes the NMT perfect&quot;&gt;Practice makes the NMT perfect&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Hindi to Engish Examples&quot;&gt;Hindi to Engish Examples&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;Byte pair Encoding&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Could not resist having some fun&quot;&gt;Could not resist having some fun&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#French to english&quot;&gt;French to english&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Other Languages&quot;&gt;Other Languages&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Case Studies&quot;&gt;Case Studies&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Natural Language Processing case studies&quot;&gt;Natural Language Processing case studies&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;NMTPart1#Automating L1/L2/L3 support systems&quot;&gt;Automating L1/L2/L3 support systems&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;NMTPart1#Sequence based Case studies&quot;&gt;Sequence based Case studies&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;NMTPart1#Neural Microprocessor Branch Predictions&quot;&gt;Microprocessor Branch Predictions&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;NMTPart1#Time Series Data&quot;&gt;Time Series Data&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;-1&quot;&gt;&lt;a name=&quot;Machine Translation: A quick history&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;machine-translation-a-quick-history&quot;&gt;Machine Translation: A quick history&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;Machine Translation(MT)&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;machine-translationmt&quot;&gt;Machine Translation(MT)&lt;/h4&gt;
&lt;p&gt;On a basic level, MT performs simple substitution of words in one language for words in another. But that alone cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Have you ever watched a movie with subtitles being so bad it actually distracts? A few years back I have had the crass luck of accidentally watching a movie with subtitles turn on for the Hindi language. I am wondering if I can call it hilarious or scary, maybe both. A serious plot-based movie’s literal translation of “holy shit” in Hindi forever ruined my movie-watching experience.&lt;/p&gt;
&lt;p&gt;The context of what is being said is extremely vital to do a decent translation. Let's look at an example. Out of many possibilities, one correct literal translations of “Higher ape” to Hindi would be “ooncha bandar”, translated back to English it becomes “Tall monkey”. The last example completely loses the context of what is being said. The genetic ancestral context is completely lost.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Rule-based Machine Translation(RBMT)&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;rule-based-machine-translationrbmt&quot;&gt;Rule-based Machine Translation(RBMT)&lt;/h4&gt;
&lt;p&gt;An RBMT system generates output sentences (in some target language) based on morphological, syntactic, and semantic analysis of both the source and the target languages involved in a concrete translation task. This is difficult, to say the least. And the rules for a language is not reusable for another.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Statistical Machine Translation(SMT)&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;statistical-machine-translationsmt&quot;&gt;Statistical Machine Translation(SMT)&lt;/h4&gt;
&lt;p&gt;Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM’s Thomas J. Watson Research Center. Roughly translated and simplified, SMT measures the conditional probability that a sequence of words Y in the target language is a true translation of a sequence of words X in the source language. The problem with SMT is that the use of probabilities to understand the complex structure of language was a bit too simplistic. They have had moderate success and were used nearly a decade back.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Neural Machine Translators(NMT)&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;neural-machine-translatorsnmt&quot;&gt;Neural Machine Translators(NMT)&lt;/h4&gt;
&lt;p&gt;The object of our discussion in the current article and the accuracy has to be seen to be believed. To be precise, what I am presenting today is &lt;strong&gt;Neural Machine Translators(NMT) with Attention&lt;strong&gt;. Like most(not all) AI-based models, &lt;strong&gt;NMTs&lt;strong&gt; also learn by looking at examples, in our case example translations. In this respect, they are similar to SMTs. However, their internal representation where they store their understanding is a &lt;strong&gt;neural network&lt;strong&gt; instead of a probabilistic one.&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;-2&quot;&gt;&lt;a name=&quot;Neural Machine Translators(NMT) with Attention&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;neural-machine-translatorsnmt-with-attention&quot;&gt;Neural Machine Translators(NMT) with Attention&lt;/h3&gt;
&lt;p&gt;There are multiple parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Process of training&quot;&gt;Process of training&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Jump to accuracy first&quot;&gt;Jump to accuracy first&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Generality&quot;&gt;Generality&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Practice makes the NMT perfect&quot;&gt;Practice makes the NMT perfect&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Hindi to Engish Examples&quot;&gt;Hindi to Engish Examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;Byte pair Encoding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Could not resist having some fun&quot;&gt;Could not resist having some fun&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#French to english&quot;&gt;French to english&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;NMTPart1#Other Languages&quot;&gt;Other Languages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;Process of training&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;process-of-training&quot;&gt;Process of training&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Ignore the blue rectangle for now.&lt;/li&gt;
    &lt;li&gt;Words are converted to their numerical representation called embeddings before being fed into the NMT.&lt;/li&gt;
    &lt;li&gt;Once you have trained with enough data, the NMTs start to understand the 2 languages in question. In figure-1 its Hindi to English.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hinditoengnmt.jpg&quot; alt=&quot;Image: figure-1: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;. Ignore the blue rectangle for now&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: &lt;strong&gt;Hindi to english&lt;/strong&gt;. Ignore the blue rectangle for now&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Ignore the blue rectangle for now.&lt;/li&gt;
    &lt;li&gt;Once you have trained with enough data, the NMTs start to understand the 2 languages in question. In figure-2 its French to English.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/frenchtoengnmt.jpg&quot; alt=&quot;Image: figure-2: &amp;lt;strong&amp;gt;French to english&amp;lt;/strong&amp;gt;. Ignore the blue rectangle for now&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: &lt;strong&gt;French to english&lt;/strong&gt;. Ignore the blue rectangle for now&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Ignore the blue rectangle for now.&lt;/li&gt;
    &lt;li&gt;Once you have trained with enough data, the NMTs start to understand the 2 languages in question. In figure-2 its Vietnamese to English.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/vietnamesetoengnmt.jpg&quot; alt=&quot;Image: figure-3: &amp;lt;strong&amp;gt;Vietnamese to english&amp;lt;/strong&amp;gt;. Ignore the blue rectangle for now&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: &lt;strong&gt;Vietnamese to english&lt;/strong&gt;. Ignore the blue rectangle for now&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The training has to be meticulous, in that, the data has to be as accurate as possible. Various measures of accuracy checks have to employed to see if the desired accuracy has been achieved. Some data has to be kept aside for testing that is not shown during training. That and more in the deep technical analysis the articles that follow.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a name=&quot;Jump to accuracy first&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;jump-to-accuracy-first&quot;&gt;Jump to accuracy first&lt;/h4&gt;

&lt;p&gt;To hell with the theory, show me the accuracy first. &lt;strong&gt;If you have not seen a system like this before, the results are nothing but magical&lt;strong&gt;.&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Take a pause, and imagine yourself translating between any 2 languages you know. It Happens subconsciously, but pause and analyze the process. You read the source sentence first, and then as you translate you keep your focus on the phrases/words and the surrounding words from the source sentence . Attention-based NMTs are built with the same idea.&lt;/li&gt;
    &lt;li&gt;But what is that matrix?&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenex11.jpg&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;Hindi to english&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;During the training, The NMT jots down its understanding of the 2 languages in it’s internal &lt;a href=&quot;/tags/#LSTM&quot;&gt;LSTM&lt;/a&gt; based neural network.&lt;/li&gt;
    &lt;li&gt;I’m merely visualizing it specific to this example translation.&lt;/li&gt;
    &lt;li&gt;It illustrates the &lt;strong&gt;attention(pun intended)&lt;/strong&gt; paid on the source words/phrases to translate to target words/phrases.&lt;/li&gt;
    &lt;li&gt;The process is very similar to human Translators.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenex13.jpg&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;Hindi to english&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The highlighted portions of a matrix illustrate the specific word/phrases within the sentences.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenex15.jpg&quot; alt=&quot;Image: figure-6: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: &lt;strong&gt;Hindi to english&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The highlighted portions show prepositions in both languages.&lt;/li&gt;
    &lt;li&gt;Well, it should come as no surprise that the NMTs can work out “Parts of speech” of a language and not just the prepositions.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenex14.jpg&quot; alt=&quot;Image: figure-7: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: &lt;strong&gt;Hindi to english&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Generality&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;generality&quot;&gt;Generality:&lt;/h4&gt;
&lt;p&gt;If the accuracy is convincing then, let’s talk about generality.
As you can witness yourself, the model architecture remains the same but when trained with different datasets “it acquires knowledge” about that data set and starts making predictions, in our case translations. &lt;strong&gt;This probably the most important reason why AI is going to redefine how programming is done in the future.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Practice makes the NMT perfect&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;practice-makes-the-nmt-perfect&quot;&gt;Practice makes the NMT perfect:&lt;/h4&gt;
&lt;p&gt;The next couple of pictures illustrate the gradual attainment of knowledge by the NMT as we increase the training rounds.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;500 rounds/epochs of training.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenexprog500.jpg&quot; alt=&quot;Image: figure-8: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:500 rounds/epochs of training&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-8: &lt;strong&gt;Hindi to english&lt;/strong&gt;:500 rounds/epochs of training&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;1000 rounds/epochs of training.&lt;/li&gt;
    &lt;li&gt;The most important aspect being the &lt;strong&gt;difference between what is predicted by the NMT and what is the ground truth&lt;strong&gt;.&lt;/strong&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;This difference which we historically call error or loss is “communicated” back to the NMT.&lt;/li&gt;
    &lt;li&gt;The NMT promptly adjusts its internal state and explore other possibilities.&lt;/li&gt;
    &lt;li&gt;This is what “we”(scientific community) refer to as learning, and we think(well, the same scientific community) this learning “somewhat” mimics how human brain work.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenexprog1000.jpg&quot; alt=&quot;Image: figure-9: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:1000 rounds/epochs of training&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-9: &lt;strong&gt;Hindi to english&lt;/strong&gt;:1000 rounds/epochs of training&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;2000 rounds/epochs of training.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenexprog2000.jpg&quot; alt=&quot;Image: figure-10: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:2000 rounds/epochs of training&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10: &lt;strong&gt;Hindi to english&lt;/strong&gt;:2000 rounds/epochs of training&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;5000 rounds/epochs of training.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenexprog5000.jpg&quot; alt=&quot;Image: figure-11: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:5000 rounds/epochs of training&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-11: &lt;strong&gt;Hindi to english&lt;/strong&gt;:5000 rounds/epochs of training&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;7000 rounds/epochs of training.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenexprog7000.jpg&quot; alt=&quot;Image: figure-12: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:7000 rounds/epochs of training&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-12: &lt;strong&gt;Hindi to english&lt;/strong&gt;:7000 rounds/epochs of training&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;10000 rounds/epochs of training.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenexprog10000.jpg&quot; alt=&quot;Image: figure-13: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:10000 rounds/epochs of training&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-13: &lt;strong&gt;Hindi to english&lt;/strong&gt;:10000 rounds/epochs of training&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Incremental comparison&lt;/li&gt;
    &lt;li&gt;The picture on the right has a far better distribution of attention weights then the one on the left.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenexprog109900300000.jpg&quot; alt=&quot;Image: figure-14: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;: Attention comparison&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-14: &lt;strong&gt;Hindi to english&lt;/strong&gt;: Attention comparison&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Hindi to Engish Examples&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;hindi-to-engish-examples&quot;&gt;Hindi to Engish Examples&lt;/h4&gt;

&lt;p&gt;Why Hindi? I could have chosen any language where data was easy to come by. I did not because, with Hindi I can double up as a human evaluator. This is a language we speak at home, to be more precise, it is a mix of English, Hindi, and Kannada. My Hindi skills are, at best average but good enough for evaluating the translations.&lt;/p&gt;

&lt;p&gt;The data for Indian languages are surprisingly difficult to come by. &lt;strong&gt;The data that is used for this blog, does not belong to &lt;a href=&quot;https://www.stillwaters.ai&quot;&gt;Stillwaters&lt;/a&gt; or &lt;a href=&quot;https://www.stillwaters.ai&quot;&gt;Stillwaters&lt;/a&gt;’  client by any means&lt;/strong&gt;. This has been personally sourced for this blog from &lt;a href=&quot;http://ufal.mff.cuni.cz/&quot;&gt;Charles University, Czech Republic&lt;/a&gt; and &lt;a href=&quot;http://www.statmt.org/&quot;&gt;Statistical Machine Translation&lt;/a&gt;. &lt;strong&gt;When &lt;a href=&quot;https://www.stillwaters.ai&quot;&gt;Stillwaters&lt;/a&gt; or me personally work with a social media giant, fintech giant or one of the formost chipmaker, the data is sourced by their team and, one can make guarantees about it’s accuracy. Infact, they have dedicated team(s) which exists solely for this purpose.&lt;/strong&gt; However, that data cannot be shared for various reasons.
The same cannot be said about personally sourced dataset like the ones I’m using for this blog. The Example below shows an example of a really bad translation. It isn’t even a good literal translation. In fact it is an incorrect statement in English.&lt;/p&gt;

&lt;p&gt;“मलेरिया के मुकाबले।”&lt;br /&gt;
“than are put into malaria.”&lt;/p&gt;

&lt;p&gt;One bad translation might be an oversight but there are many bad sample translations. One too many to ignore.
&lt;strong&gt;Long story short, the accuracy of the translations can be much much much better with better data.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Byte pair Encoding&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;byte-pair-encoding&quot;&gt;Byte pair Encoding&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Byte_pair_encoding&quot;&gt;BPE&lt;/a&gt; Stands for &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;Byte Pair Encoding&lt;/a&gt;. It was originally developed for compressing data. While it is still used for compression in Natural Language processing, it has a few more important advantages.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Intuitively speaking, this is how words are made up in many languages. Longer words are made up of shorter syllabic sounds.&lt;/li&gt;
    &lt;li&gt;Let’s say that we train a model on words like “walk”, “walking”, “walked” and it begins to understand the relationship between the words. However, a similar relationship between “talk”, “talked”, “talking” isn’t clear &lt;strong&gt;by analogy&lt;/strong&gt; if it has not trained on all the words before. This can be remedied by breaking words into smaller syllabic tokens. Words like “walking” can be broken down into “walk” and “ing”. Similarly “walked” can be broken down into “walk” and “ed”. &lt;strong&gt;This also clarifies the meaning of “ed” and “ing” to the model and by analogy, the model is able to infer the meaning of talking even though it has never encountered the word before.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Another positive side effect is, rare words not seen at training time can still be recognized because they are made up of smaller syllabic tokens.&lt;/li&gt;
    &lt;li&gt;Lastly, it improves accuracy.&lt;/li&gt;
    &lt;li&gt;Now, even if you have not understood the above points, just remember the words are broken down into smaller syllabic tokens before being fed into the NMT.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Rest of the examples are in BPE encoding&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-1&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenex16.jpg&quot; alt=&quot;Image: figure-15: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:ex-1&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-15: &lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-1&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-2&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenex17.jpg&quot; alt=&quot;Image: figure-16: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:ex-2: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-16: &lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-2: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-3&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/hitoenex18.jpg&quot; alt=&quot;Image: figure-17: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:ex-3: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17: &lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-3: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-4&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;The remaining examples are long and complex sentences. We could solved them only post the arrival of Attention.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Src:”ल@@ ले@@ याल सत@@ वारी के ना@@ यब सर@@ पंच चैन सिंह ने बताया कि यहां आने वाले श ् रद ् धा@@ लुओं के लिए ल@@ ंगर की भी व ् यवस ् था है ।”&lt;br /&gt;
Ref:”Nay@@ ab Sar@@ pan@@ ch of Lal@@ ey@@ al Sat@@ wari , Cha@@ in Singh said that a L@@ ang@@ ar has also been organised for the pilgrims visiting this place .”&lt;br /&gt;
NMT:”Nay@@ ab Sar@@ pan@@ ch of Lal@@ ey@@ al Sat@@ wari , Cha@@ in Singh said that a L@@ ang@@ ar has also been organised for the pilgrims visiting this place .”&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/output_bibsattention_infer_ind112.jpg&quot; alt=&quot;Image: figure-18: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:ex-4: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-18: &lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-4: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-5&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Src:”शुक ् रवार को जय@@ राम ने कहा कि अदालतों का अपनी सीमाएं पार करते हुए अफसरों और सरकार के काम को अपने हाथ में ले लेना ठीक नहीं है ।”&lt;br /&gt;
Ref:”On Friday J@@ airam said that it is not right for the Courts to exceed their powers and take the job of the officers and the Government into their own hands .”&lt;br /&gt;
NMT:”On Friday J@@ airam said that it is not right for the Courts to exceed their powers and take the job of the officers and the Government into their own hands .”&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/output_bibsattention_infer_ind374.jpg&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:ex-5: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-5: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Could not resist having some fun&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;could-not-resist-having-some-fun&quot;&gt;Could not resist having some fun&lt;/h4&gt;

&lt;p&gt;The premise of article is about bad and out of context subtitles. Just couldn’t stop my self from indulging in a little fun.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-6&lt;/li&gt;
    &lt;li&gt;Just in case, I am not quoting a former president of United States.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Src:”बड ़ े-@@ बड ़ े देशों में छोटी-@@ छोटी बातें होती रहती हैं ' ।”&lt;br /&gt;
Ref:”In big countries , little things like these keep happening .”&lt;br /&gt;
NMT:”In big countries , little things like these keep happening .”&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/output_bibsattention_infer_ind520.jpg&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:ex-6: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-6: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-7&lt;/li&gt;
    &lt;li&gt;Neither am I quoting Paulo Coelho.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Src:”इतनी शि@@ द@@ अत से मैं तुम ् हे पाने की कोशिश की है , की हर ज़@@ ारे ने मुझे तुमसे मिलाने की सा@@ ज़@@ ि@@ श की है ।”&lt;br /&gt;
Ref:”I have tried to get you with all my heart , that the whole universe has con@@ sp@@ ired to introduce me to you .”&lt;br /&gt;
NMT:”I have tried to get you with all my heart , the whole universe has con@@ sp@@ ired to introduce me to you .”&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/output_bibsattention_infer_ind521.jpg&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:ex-7: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-7: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-8&lt;/li&gt;
    &lt;li&gt;And he’s the “desi” He-Man.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Src:”ब@@ सं@@ ती इन कुत ् तों के सामने मत ना@@ चना”&lt;br /&gt;
Ref:”Bas@@ anti don 't dance in front of these dogs .”&lt;br /&gt;
NMT:”Bas@@ anti don 't dance in front of these dogs .”&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/output_bibsattention_infer_ind522.jpg&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;Hindi to english&amp;lt;/strong&amp;gt;:ex-8: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;Hindi to english&lt;/strong&gt;:ex-8: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;French to english&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;french-to-english&quot;&gt;French to english&lt;/h3&gt;

&lt;p&gt;My knowledge of the French language is rudimentary. I learnt French for understanding NMT results. That is the limit of French knowledge. However, one of the things that stood out for me in French were adjectives. In English, adjectives are pretty easy to use. You put them before the noun they describe and you’re done. In French however, the placement of adjectives varies. And if that wasn’t enough to confuse you and me, adjectives also change depending on whether the noun they describe is masculine, feminine, singular or plurial.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;French to english&lt;/strong&gt;:ex-1&lt;/li&gt;
    &lt;li&gt;Adjectives to the test.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/frtoenex1.jpg&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;French to english&amp;lt;/strong&amp;gt;:ex-1: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;French to english&lt;/strong&gt;:ex-1: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;French to english&lt;/strong&gt;:ex-2&lt;/li&gt;
    &lt;li&gt;I would like to argue that the one on the right is more finely tuned, because its translations are much more succinct.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/nmt/frtoenex2.jpg&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;French to english&amp;lt;/strong&amp;gt;:ex-2: Refer to &amp;lt;a href='NMTPart1#Byte pair Encoding'&amp;gt;&amp;lt;strong&amp;gt;Byte pair encoding&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt; to understand the '@@' and '्' tokens.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;French to english&lt;/strong&gt;:ex-2: Refer to &lt;a href=&quot;NMTPart1#Byte pair Encoding&quot;&gt;&lt;strong&gt;Byte pair encoding&lt;/strong&gt;&lt;/a&gt; to understand the '@@' and '्' tokens.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a name=&quot;Other Languages&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;other-languages&quot;&gt;Other Languages&lt;/h4&gt;
&lt;p&gt;What about supporting other Indian languages? Technically it is a no brainer. It will take half a day for my team to set it up. The only problem is data. We do not have high-quality data. Irony is I did not find high-quality data for Hindi-to-English training in India. For that, I have to thank &lt;a href=&quot;http://ufal.mff.cuni.cz/&quot;&gt;Charles University, Czech Republic&lt;/a&gt; and &lt;a href=&quot;http://www.statmt.org/&quot;&gt;Statistical Machine Translation&lt;/a&gt; for loaning it to me. If you find parallel corpus for the data of the language(s) you want to support ping us at support@stillwaters.ai. For &lt;strong&gt;Kannada&lt;/strong&gt; I have facing extreme pressure from home as well as office for obvious reasons. ;-) Tamil, Telegu, Punjabi, Malyalam, Gujrati, Bengali, Japanese e.t.c. Imagine speaking to your machine in your native language. Even local dialects can be supported like Swiss-German, Bhojpuri etc.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Case Studies&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;case-studies&quot;&gt;Case Studies&lt;/h3&gt;
&lt;p&gt;Imagine this, if an AI model like NMT can excel at translation merely by looking at examples, what else can it be good at by the same analogy. But lets stay with translation for a while and imagine this. If the translations can be as good as you what you have seen, and can be bettered with better data, the NMT does this by storing in its &lt;strong&gt;“internal neural representation”&lt;/strong&gt; the knowledge it has gained on the languages it has been trained on. Now imagine what one could achieve with the interface to a machine being a Natural Language. A general Listing would be too exhaustive, &lt;strong&gt;so down below are some of the applications that we have directly built using this technology and its derivatives in the last 3-4 years&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Natural Language Processing case studies&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;natural-language-processing-case-studies&quot;&gt;Natural Language Processing case studies&lt;/h4&gt;

&lt;p&gt;&lt;a name=&quot;Automating L1/L2/L3 support systems&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Automating L1/L2/L3 support systems&lt;/strong&gt;: Most organizations have historical data stored in sources(mails, JIRA, in house Knowledge bases, application logs and, source code). Most support systems &lt;strong&gt;manually&lt;/strong&gt; feed of this data collectively referred to as sources. &lt;strong&gt;Automated Support System&lt;/strong&gt; has the smarts to &lt;strong&gt;read or hear&lt;/strong&gt; a support query and because it contexually understands the query(using the below mentioned technologies), transform it to form that can be fired against one or more of the above sources. The sources respond with the information which is then assimilated. The assimilated results are then transformed into Natural Language and sent back to the client.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      * Speech to text: Converting speech to text.
      * Text to speech: Converting text to speech.
      * Handwriting recognition:
      * Speech Translation: Speech from one language to another.   
      * SQL Query Generation: Given a query in Natural Language, the MODEL translates it to an SQL
                     query that is ready to be fired against a database or a knowledge base.
      * Machine Comprehension: Given a passage of text, the MODEL can answer queries based on that.
            (Recall middle school syllabus, Who said this to whom, only this time its the algorithm on the spot.)
      * Parts of Speech taggings: Given a passage of text, the MODEL can mark the various parts of speech in
            a sentence.
      * Name Entity Recognition: Given a passage of text, the MODEL can mark the entities(Nouns) and their types.
      * Sarcasm Detection:
      * Word sense disambiguation:
      * Hate speech Detection:
      * Tone softening: Soften the tone of a sentence from being harsh and critical to a more polite tone without
            making the intent of the message ambigous.
      * Text Summarization: Given a passage, the model summarizes it.
      * Topic Modelling: Given a passage, the MODEL suggests the topic for it.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a name=&quot;Sequence based Case studies&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;sequence-based-case-studies&quot;&gt;Sequence based Case studies&lt;/h4&gt;
&lt;p&gt;Technologies like LSTM, Attention, Memory are fantastic at recognizing patterns in a sequence. Which is why they are great at speech, text etc. But language is not the only sequential data we deal with. Majority of dataset around us are some kind of sequence. Some examples are vehicle postions over time, stock prices, and all kinds of timeseies data. &lt;strong&gt;The details of these projects are extremely proprietary but the idea is simple. Presenting a few of them with a very short description&lt;/strong&gt;. &lt;strong&gt;Neural Microprocessor Branch Predictions&lt;/strong&gt; is something that we are extremely proud of. Once I finish the Natural Language Processing series, Ill look into the below mentioned case studies in a more detailed future post.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Neural Microprocessor Branch Predictions&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Neural Microprocessor Branch Predictions&lt;/strong&gt;: Depending on the exact CPU and code, Control-changing instructions, like branches in code add uncertainty in the execution of dependent instructions and lead to large performance loss in severely pipelined processors. Accurate prediction of branches is vital for modern CPUs. The latency incurred due to mispredictions is roughly proportionate to the size of the pipeline. For the last 4-5 generation CPUs (5-6 years) this should range between 12-30 cycles. It also wastes energy executing the wrong instruction path. Branch prediction improvements can boost performance and energy consumption significantly. For example, experiments on real processors showed that reducing the branch mispredictions by 3/4 &lt;strong&gt;improved the processor performance by 29%&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a name=&quot;Time Series Data&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Time Series Data&lt;/strong&gt;: Return prediction on gold pricing is an example of time series data in finance. Traditionally forecasting time series data uses techniques like univariate Autoregressive (AR), univariateMoving Average (MA), Simple Exponential Smoothing (SES),and more notably Autoregressive Integrated Moving Average(ARIMA) with its many variations. However, with &lt;a href=&quot;/tags/#LSTM&quot;&gt;LSTMs&lt;/a&gt; when combined with Attention in some form, the &lt;strong&gt;improvement is upwards of 88%.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;Is this the State-of-the-art(SOTA) technology for Artificial Intelligence in the field of Natural Language Processing? This field is a moving target and innovations are happening at a blinding speed. &lt;strong&gt;The scary part is this was SOTA less than a year ago. But today we have systems that take accuracy to a different level completely&lt;/strong&gt;. Why then talk about NMTs? Well, NMTs are a major cog in the wheel for whatever is SOTA today. SOTA systems are called “Transformers” and they are an evolution over NMTs. So a great understanding of NMTs and its pieces is a precondition for understanding Transformers. In the immediate next post I’ll get under the skin of NMT and its back-propagation.&lt;/p&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="RNN" /><category term="Sequence-Modelling" /><category term="Natural-Language-Processing" /><category term="NLP" /><category term="Neural-Machine-Translation" /><category term="NMT" /><category term="Attention" /><summary type="html">In the current article, we look at Artificial Intelligence(AI) based translation systems from one Natural Language to another. Natural language, as in the way humans speak and how far have we come in designing machines that understand and act on it. The first of the multi-part series is highly simplified, completely pictorial, dressing down of Neural Machine Translation systems. Once you have a solid understanding of systems like these, we take a look at where are we currently applying it to(case studies), to broaden the horizons. The rest of the parts will be a rigorous under-the-skin(math,code and logic) look of Neural Machine Translators, Attention and, the cherry on the cake Googles Neural Machine Translators</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22feature%22=%3E%22nmt/hitoenex11.jpg%22%7D" /></entry><entry><title type="html">RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs</title><link href="http://localhost:4000/articles/2019-07/LSTMPart-4" rel="alternate" type="text/html" title="RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs" /><published>2019-07-22T15:07:19+00:00</published><updated>2019-07-22T15:07:19+00:00</updated><id>http://localhost:4000/articles/2019-07/LSTMPart-4</id><content type="html" xml:base="http://localhost:4000/articles/2019-07/LSTMPart-4">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In this &lt;a href=&quot;/tags/#LSTM&quot;&gt;multi-part series&lt;/a&gt; (&lt;a href=&quot;/articles/2019-07/LSTMPart-1&quot;&gt;part-1&lt;/a&gt;,&lt;a href=&quot;/articles/2019-07/LSTMPart-2&quot;&gt;part-2&lt;/a&gt;,&lt;a href=&quot;/articles/2019-07/LSTMPart-3&quot;&gt;part-3&lt;/a&gt;), we look into composing LSTM into multiple higher layers and its directionality. Though &lt;strong&gt;Multiple layers&lt;/strong&gt; are compute-intensive, they have better accuracy and so does &lt;strong&gt;bidirectional connections.&lt;/strong&gt; More importantly, a solid understanding of the above mentioned paves the way for concepts like &lt;strong&gt;Highway connections&lt;/strong&gt;, &lt;strong&gt;Residual Connections&lt;/strong&gt;, &lt;strong&gt;Pointer networks&lt;/strong&gt;, &lt;strong&gt;Encoder-Decoder&lt;/strong&gt; Architectures and so forth in future article. &lt;strong&gt;I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;what-this-article-is-not-about&quot;&gt;What this article is not about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This article will not talk about the conceptual model of LSTM, on which there is some great existing material here and here in the order of difficulty.&lt;/li&gt;
    &lt;li&gt;This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome, if a somewhat &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;difficult post&lt;/a&gt;, by Andrej.&lt;/li&gt;
    &lt;li&gt;This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts &lt;a href=&quot;https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577&quot;&gt;here&lt;/a&gt; in the order of difficulty&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;what-this-article-is-about&quot;&gt;What this article is about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Using the &lt;a href=&quot;https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0&quot;&gt;first principles&lt;/a&gt; we picturize/visualize &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt; and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt; and their backward propagation.&lt;/li&gt;
    &lt;li&gt;Then we use a heady mixture of intuition, Logic, and programming to prove mathematical results.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;
&lt;p&gt;This is the same example and the context is the same as described in &lt;a href=&quot;/articles/2019-07/LSTMPart-1&quot;&gt;part-1&lt;/a&gt;. &lt;strong&gt;The focus. however, this time is on &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt; and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There are 2 parts to this article.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-4#Multi layer RNNs&quot;&gt;Multi layer RNNs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-4#Bidirectional RNNs section&quot;&gt;Bidirectional RNNs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-4#Combining Bidirectional and MultiLayerRNNs&quot;&gt;Combining Bidirectional and MultiLayerRNNs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;&lt;a name=&quot;Multi layer RNNs&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;multi-layer-rnns&quot;&gt;Multi layer RNNs&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-4#Recurent depth&quot;&gt;Recurent depth&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-4#Feed Forward depth&quot;&gt;Feed Forward depth&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-4#Multi layer RNNs Forward pass&quot;&gt;Multi layer RNNs Forward pass&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-4#Multi layer RNNs backward propagation&quot;&gt;Multi layer RNNs backward propagation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;-1&quot;&gt;&lt;a name=&quot;Recurent depth&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;recurent-depth&quot;&gt;Recurent depth&lt;/h4&gt;
&lt;p&gt;A quick recap, LSTM encapsulates the internal cell logic. There are many variations to Cell logic, VanillaRNN, LSTMs, GRUs, etc. What we have seen so far is they can be fed with sequential time series data. We feed in the sequence, compare with the labels, calculate errors back-propagate and adjust the weights. The length of the fed in the sequence is informally called recurrent depth.&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell.jpg&quot; alt=&quot;Image: figure-1: &amp;lt;strong&amp;gt;Recurrent depth&amp;lt;/strong&amp;gt;=3&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: &lt;strong&gt;Recurrent depth&lt;/strong&gt;=3&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Figure-1 shows the recurrent depth. Formally stated, &lt;strong&gt;Recurrent depth is the Longest path between the same hidden state in successive time-steps&lt;/strong&gt;. The recurrent depth is amply clear.&lt;/p&gt;

&lt;h1 id=&quot;-2&quot;&gt;&lt;a name=&quot;Feed Forward depth&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;feed-forward-depth&quot;&gt;Feed Forward depth&lt;/h4&gt;
&lt;p&gt;The topic of this article is Feed-forward depth, more akin to the depth we have in vanilla neural networks. It is the depth of a network that is generally attributed to the success of deep learning as a technique. There are downsides too, too much compute budget for one if done indiscriminately. That being said, well look at the inner workings of Multiple layers and how we set it up in code.&lt;/p&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell2.jpg&quot; alt=&quot;Image: figure-2: &amp;lt;strong&amp;gt;Feed forward depth&amp;lt;/strong&amp;gt;=3&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: &lt;strong&gt;Feed forward depth&lt;/strong&gt;=3&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Figure-2 shows the Feed-Forward depth. Formally stated the longest path between an input and output at the same timestep.&lt;/p&gt;

&lt;h1 id=&quot;-3&quot;&gt;&lt;a name=&quot;Multi layer RNNs Forward pass&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;multi-layer-rnns-forward-pass&quot;&gt;Multi layer RNNs Forward pass&lt;/h4&gt;

&lt;p&gt;For the most part this straight forward as you have already seen in the &lt;a href=&quot;/articles/2019-07/LSTMPart-3&quot;&gt;previous article&lt;/a&gt; in this series. The only difference is that there are multiple cells(2 in this example) now and how the state flows forward and gradient flows backwards.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt;&lt;/strong&gt; forward pass summary.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell7.jpg&quot; alt=&quot;Image: figure-3: &amp;lt;strong&amp;gt;Multi layer RNNs Forward pass summary&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: &lt;strong&gt;Multi layer RNNs Forward pass summary&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Multi-layer RNNs Forward pass&lt;/li&gt;
    &lt;li&gt;Notice the state being passed(yellow) from the first layer to the second.&lt;/li&gt;
    &lt;li&gt;The softmax and cross_entropy_loss are done on the second layer output expectedly.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell3.jpg&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;Multi layer RNNs Forward pass&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;Multi layer RNNs Forward pass&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Complete code of &lt;strong&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt;&lt;/strong&gt; from &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;DEEP-Breathe&lt;/a&gt; helps in looking at the internals&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell4.jpg&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;DHt&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;DHt&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt; &lt;strong&gt;outputs&lt;/strong&gt; returned&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell5.jpg&quot; alt=&quot;Image: figure-6: &amp;lt;strong&amp;gt;outputs&amp;lt;/strong&amp;gt; returned.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: &lt;strong&gt;outputs&lt;/strong&gt; returned.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt; &lt;strong&gt;states&lt;/strong&gt; returned&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell6.jpg&quot; alt=&quot;Image: figure-7: &amp;lt;strong&amp;gt;states&amp;lt;/strong&amp;gt; returned.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: &lt;strong&gt;states&lt;/strong&gt; returned.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Also when using &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt; you could enable forward or backward logging for MultiRNNCell like so&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;  &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MultiRNNCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backpassdebug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h1 id=&quot;-4&quot;&gt;&lt;a name=&quot;Multi layer RNNs backward propagation&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;multi-layer-rnns-backward-propagation&quot;&gt;Multi layer RNNs backward propagation&lt;/h4&gt;
&lt;p&gt;Again, for the most part, this is almost identical to standard LSTM Backward Propagation which we went through in detail in &lt;a href=&quot;/articles/2019-07/LSTMPart-3&quot;&gt;part-3&lt;/a&gt;. So if you have got a good hang of it, only a few things change. But before we go on, refresh &lt;a href=&quot;LSTMPart-3#DHX&quot;&gt;DHX&lt;/a&gt; and the complete &lt;a href=&quot;LSTMPart-3#Backward propagation through LSTMCell(Step-2)&quot;&gt;section&lt;/a&gt; before you resume here. Here is what we have to calculate.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt; Backward propagation summary.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell8.jpg&quot; alt=&quot;Image: figure-8: MultiRNNCell Backward propagation summary.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-8: MultiRNNCell Backward propagation summary.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;I’ll reproduce DHX here just to set the context and note down the changes.&lt;/strong&gt;&lt;/p&gt;
&lt;h6 id=&quot;standard-dhx-for-regular-lstm-cell&quot;&gt;Standard DHX for Regular LSTM Cell&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Dhx&lt;/strong&gt;, &lt;strong&gt;Dh_next&lt;/strong&gt;, &lt;strong&gt;dxt&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344&quot;&gt;code-1&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback26.jpg&quot; alt=&quot;Image: figure-9: &amp;lt;strong&amp;gt;Dhx&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;Dh_next&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;dxt&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-9: &lt;strong&gt;Dhx&lt;/strong&gt;, &lt;strong&gt;Dh_next&lt;/strong&gt;, &lt;strong&gt;dxt&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;dhx-for-multirnncell-composing-lstm-cell&quot;&gt;DHX for MultiRNNCell composing LSTM Cell&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Dhx, Dh_next, dxt as usual and then &lt;strong&gt;dh_next_recurr&lt;/strong&gt; which is passed back to MultiRNNCell to be applied to the next layer.&lt;/li&gt;
    &lt;li&gt;Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344&quot;&gt;code-1&lt;/a&gt;. Pay careful attention to the weights shapes.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell9.jpg&quot; alt=&quot;Image: figure-10: &amp;lt;strong&amp;gt;dh_next_recurr&amp;lt;/strong&amp;gt; which is passed back to MultiRNNCell to be applied to the next layer.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10: &lt;strong&gt;dh_next_recurr&lt;/strong&gt; which is passed back to MultiRNNCell to be applied to the next layer.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;standard-dhx-schematically-regular-lstm-cell&quot;&gt;Standard DHX schematically Regular LSTM Cell&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Many models like Neural Machine Translators(NMT), Bidirectional Encoder Representation from Transformers(BERT) use word embeddings as their inputs(Xs) which oftentimes need learning and that is where we need &lt;strong&gt;dxt&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback27.jpg&quot; alt=&quot;Image: figure-30: &amp;lt;strong&amp;gt;Dhx&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;Dh_next&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;dxt&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-30: &lt;strong&gt;Dhx&lt;/strong&gt;, &lt;strong&gt;Dh_next&lt;/strong&gt;, &lt;strong&gt;dxt&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;dhx-schematically-for-multirnncell-composing-lstm-cell&quot;&gt;DHX schematically for MultiRNNCell composing LSTM Cell&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Dhx, Dh_next, dxt as usual and then &lt;strong&gt;dh_next_recurr&lt;/strong&gt; which is passed back to MultiRNNCell to be applied to the next layer.&lt;/li&gt;
    &lt;li&gt;The Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344&quot;&gt;code-1&lt;/a&gt;, in particular, the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L146-L153&quot;&gt;MultiRNNCell “compute_gradient()”&lt;/a&gt;. Pay careful attention to the weights’ shapes.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/multilayercell10.jpg&quot; alt=&quot;Image: figure-10: &amp;lt;strong&amp;gt;dh_next_recurr&amp;lt;/strong&amp;gt; which is passed back to MultiRNNCell to be applied to next layer.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10: &lt;strong&gt;dh_next_recurr&lt;/strong&gt; which is passed back to MultiRNNCell to be applied to next layer.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-5&quot;&gt;&lt;a name=&quot;Bidirectional RNNs section&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;bidirectional-rnns&quot;&gt;Bidirectional RNNs&lt;/h3&gt;
&lt;p&gt;The bidirectional RNNs are great for accuracy but the compute budget goes up. Most advanced architectures use bidirectional for better accuracy. It has 2 cells or 2 multi-cells where the sequence is fed in forwards as well as backwards. For many sequences, language models for instance, when the sequence is fed in-reverse then it provides a bit more context of what is being said. Consider the below statements.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;One of the greatest American Richard Feynman said. “If I could explain it to the average person, it wouldn’t have been worth the Nobel Prize.”&lt;/li&gt;
    &lt;li&gt;One of the greatest American Richard Pryor said. “I became a performer because it was what I enjoyed doing.”&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first six words of the 2 sentences are identical. But if this sequence was fed in backwards as well, then the context would have been much clearer. Setting it up much easier as illustrated in the next few figures.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;Simple 2 separate cells fed in with the same sequence in opposite directions.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/bilstm0.jpg&quot; alt=&quot;Image: figure-11: Bidirectional RNNs.&amp;lt;strong&amp;gt;forward cell&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-11: Bidirectional RNNs.&lt;strong&gt;forward cell&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The State looks identical but reversed, and that is because they have been initialized identically for comparison purposes. &lt;strong&gt;In practice we don’t do that&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/bilstm1.jpg&quot; alt=&quot;Image: figure-12: Bidirectional RNNs.&amp;lt;strong&amp;gt;backward cell&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-12: Bidirectional RNNs.&lt;strong&gt;backward cell&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt; Output&lt;/li&gt;
    &lt;li&gt;Tupled h-states for both the cells.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/bilstm2.jpg&quot; alt=&quot;Image: figure-13: Bidirectional RNNs:Output.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-13: Bidirectional RNNs:Output.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt; States&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Tupled and last (c-state and h-state) for both the cells&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/bilstm3.jpg&quot; alt=&quot;Image: figure-14: MultiRNNCell Backward propagation summary.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-14: MultiRNNCell Backward propagation summary.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt; Pred Calculation&lt;/li&gt;
    &lt;li&gt;Pred calculation is similar except the last h-state is concatenated changing the shape of Wy.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/bilstm4.jpg&quot; alt=&quot;Image: figure-15: MultiRNNCell Backward propagation summary.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-15: MultiRNNCell Backward propagation summary.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-6&quot;&gt;&lt;a name=&quot;Combining Bidirectional and MultiLayerRNNs&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&quot;combining-bidirectional-and-multilayerrnns&quot;&gt;Combining Bidirectional and MultiLayerRNNs&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Combining &lt;strong&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt; and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;Output&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/bilstm5.jpg&quot; alt=&quot;Image: figure-16: Combining Bidirectional RNNs and MultiRNNCell.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-16: Combining Bidirectional RNNs and MultiRNNCell.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Combining &lt;strong&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt; and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;State&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/bilstm6.jpg&quot; alt=&quot;Image: figure-17: Combining Bidirectional RNNs and MultiRNNCell.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17: Combining Bidirectional RNNs and MultiRNNCell.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Combining &lt;strong&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416&quot;&gt;Bidirectional RNNs&lt;/a&gt; and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193&quot;&gt;MultiRNNCell&lt;/a&gt;&lt;/strong&gt; Pred Calculation.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/bilstm7.jpg&quot; alt=&quot;Image: figure-18: Combining Bidirectional RNNs and MultiRNNCell.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-18: Combining Bidirectional RNNs and MultiRNNCell.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;Finally, that would be the complete documentation of LSTMs inner workings. Maybe a comparison with RNNs for vanishing gradient improvement will complete it. But that apart, once we have a drop on LSTMs like we did, using them in more complex architectures will become a lot easier. In the immediate next article, we’ll look at the improvements in propagation because of vanishing and exploding gradients, if any, brought about by LSTMs over RNNs.&lt;/p&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="RNN" /><category term="Sequence-Modelling" /><summary type="html">In this (part-1,part-2,part-3) we look into composing LSTM into multiple higher layers and its directionality. Though Multiple layers are compute-intensive, they have better accuracy and so does bidirectional connections. More importantly, a solid understanding of the above mentioned paves the way for concepts like Highway connections, Residual Connections, Pointer networks, Encoder-Decoder Architectures and so forth in a future article. I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22feature%22=%3E%22rnn/bilstm7.jpg%22%7D" /></entry><entry><title type="html">RNN Series:LSTM internals:Part-3: The Backward Propagation</title><link href="http://localhost:4000/articles/2019-07/LSTMPart-3" rel="alternate" type="text/html" title="RNN Series:LSTM internals:Part-3: The Backward Propagation" /><published>2019-07-15T15:07:19+00:00</published><updated>2019-07-15T15:07:19+00:00</updated><id>http://localhost:4000/articles/2019-07/LSTMPart-3</id><content type="html" xml:base="http://localhost:4000/articles/2019-07/LSTMPart-3">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;In this &lt;a href=&quot;/tags/#LSTM&quot;&gt;multi-part series&lt;/a&gt;, we look inside LSTM forward pass.&lt;/strong&gt; If you haven’t already read it I suggest run through the previous parts (&lt;a href=&quot;/articles/2019-07/LSTMPart-1&quot;&gt;part-1&lt;/a&gt;,&lt;a href=&quot;/articles/2019-07/LSTMPart-2&quot;&gt;part-2&lt;/a&gt;) before you come back here. Once you are back, in this article, we explore LSTM’s Backward Propagation. This would usually involve &lt;strong&gt;lots of math&lt;/strong&gt;. I am &lt;strong&gt;not a trained mathematician&lt;/strong&gt;, however, I have decent &lt;strong&gt;intuition&lt;/strong&gt;. But just intuition can only get you so far. So I used my &lt;strong&gt;programming skills&lt;/strong&gt; to validate pure theoretical results often cited by a &lt;strong&gt;pure mathematician.&lt;/strong&gt; &lt;strong&gt;I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.&lt;/strong&gt;”&lt;/p&gt;

&lt;h3 id=&quot;what-this-article-is-not-about&quot;&gt;What this article is not about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This article will not talk about the conceptual model of LSTM, on which there is some great existing material &lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html&quot;&gt;here&lt;/a&gt; in the order of difficulty.&lt;/li&gt;
    &lt;li&gt;This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome post by Andrej if a somewhat &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;difficult post&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts &lt;a href=&quot;https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577&quot;&gt;here&lt;/a&gt; in the order of difficulty.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;what-this-article-is-about&quot;&gt;What this article is about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Using the &lt;a href=&quot;https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0&quot;&gt;first principles&lt;/a&gt; we picturize/visualize the Backward Propagation.&lt;/li&gt;
    &lt;li&gt;Then we use a heady mixture of intuition, Logic, and programming to prove mathematical results.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;
&lt;p&gt;This is the same example and the context is the same as described in &lt;a href=&quot;/articles/2019-07/LSTMPart-1&quot;&gt;part-1&lt;/a&gt;. &lt;strong&gt;The focus, however, this time is on LSTMCell.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;lstm-cell-backward-propagation&quot;&gt;LSTM Cell Backward Propagation&lt;/h3&gt;
&lt;p&gt;While there are many variations to the LSTM cell. In the current article, we are looking at &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell&quot;&gt;LayerNormBasicLSTMCell&lt;/a&gt;. The next section is divided into 3 parts.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#LSTM Cell Forward Propagation(Summary)&quot;&gt;LSTM Cell Forward Propagation(Summary)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#LSTM Cell Backward Propagation(Summary)&quot;&gt;LSTM Cell Backward Propagation(Summary)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#LSTM Cell Backward Propagation(Detailed)&quot;&gt;LSTM Cell Backward Propagation(Detailed)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;&lt;a name=&quot;LSTM Cell Forward Propagation(Summary)&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;1-lstm-cell-forward-propagationsummary&quot;&gt;1. LSTM Cell Forward Propagation(Summary)&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Forward propagation:Summary with weights&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmcelltemplate5.jpg&quot; alt=&quot;Image: figure-1: &amp;lt;strong&amp;gt;Forward propagation:Summary with weights.&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: &lt;strong&gt;Forward propagation:Summary with weights.&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Forward propagation:Summary as flow diagram&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmforward.jpg&quot; alt=&quot;Image: figure-2: &amp;lt;strong&amp;gt;Forward propagation:Summary as simple flow diagram.&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: &lt;strong&gt;Forward propagation:Summary as simple flow diagram.&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Forward propagation: The complete picture&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback1.jpg&quot; alt=&quot;Image: figure-3: Forward propagation: The complete picture.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: Forward propagation: The complete picture.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-1&quot;&gt;&lt;a name=&quot;LSTM Cell Backward Propagation(Summary)&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;2-lstm-cell-backward-propagationsummary&quot;&gt;2. LSTM Cell Backward Propagation(Summary)&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Backward Propagation through time or BPTT is shown here in 2 steps.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Step-1&lt;/strong&gt; is depicted in &lt;strong&gt;Figure-4&lt;/strong&gt; where it backward propagates through the &lt;strong&gt;FeedForward network&lt;/strong&gt; calculating Wy and By&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback2.jpg&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;Step-1:&amp;lt;/strong&amp;gt;&amp;lt;strong&amp;gt;Wy&amp;lt;/strong&amp;gt; and &amp;lt;strong&amp;gt;By&amp;lt;/strong&amp;gt; first.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;Step-1:&lt;/strong&gt;&lt;strong&gt;Wy&lt;/strong&gt; and &lt;strong&gt;By&lt;/strong&gt; first.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Step-2&lt;/strong&gt; is depicted in &lt;strong&gt;Figure-5&lt;/strong&gt;, &lt;strong&gt;Figure-6&lt;/strong&gt; and &lt;strong&gt;Figure-7&lt;/strong&gt; where it backward propagates through the LSTMCell. This is time step-3 or the last one. BPTT is carried out in reverse order.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback3.jpg&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;Step-2:&amp;lt;/strong&amp;gt;&amp;lt;strong&amp;gt;3rd&amp;lt;/strong&amp;gt; time step &amp;lt;strong&amp;gt;W&amp;lt;sub&amp;gt;f&amp;lt;/sub&amp;gt;,W&amp;lt;sub&amp;gt;i&amp;lt;/sub&amp;gt;,W&amp;lt;sub&amp;gt;c&amp;lt;/sub&amp;gt;,W&amp;lt;sub&amp;gt;o&amp;lt;/sub&amp;gt;&amp;lt;/strong&amp;gt; and &amp;lt;strong&amp;gt;B&amp;lt;sub&amp;gt;f&amp;lt;/sub&amp;gt;,B&amp;lt;sub&amp;gt;i&amp;lt;/sub&amp;gt;,B&amp;lt;sub&amp;gt;c&amp;lt;/sub&amp;gt;,B&amp;lt;sub&amp;gt;o&amp;lt;/sub&amp;gt;&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;Step-2:&lt;/strong&gt;&lt;strong&gt;3rd&lt;/strong&gt; time step &lt;strong&gt;W&lt;sub&gt;f&lt;/sub&gt;,W&lt;sub&gt;i&lt;/sub&gt;,W&lt;sub&gt;c&lt;/sub&gt;,W&lt;sub&gt;o&lt;/sub&gt;&lt;/strong&gt; and &lt;strong&gt;B&lt;sub&gt;f&lt;/sub&gt;,B&lt;sub&gt;i&lt;/sub&gt;,B&lt;sub&gt;c&lt;/sub&gt;,B&lt;sub&gt;o&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Step-2&lt;/strong&gt; LSTMCell BPTT in a simple reverse flow diagram.
      &lt;h1 id=&quot;-2&quot;&gt;&lt;a name=&quot;figure-6&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmbackcell.jpg&quot; alt=&quot;Image: figure-6: &amp;lt;strong&amp;gt;Step-2:&amp;lt;/strong&amp;gt;LSTMCell BPTT in a simple reverse flow diagram.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: &lt;strong&gt;Step-2:&lt;/strong&gt;LSTMCell BPTT in a simple reverse flow diagram.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Step-2&lt;/strong&gt; Repeat for remaining time steps in the sequence.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback4.jpg&quot; alt=&quot;Image: figure-7: &amp;lt;strong&amp;gt;Step-2:&amp;lt;/strong&amp;gt; Repeat for remaining time steps in the sequence&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: &lt;strong&gt;Step-2:&lt;/strong&gt; Repeat for remaining time steps in the sequence&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-3&quot;&gt;&lt;a name=&quot;LSTM Cell Backward Propagation(Detailed)&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&quot;3-lstm-cell-backward-propagationdetailed&quot;&gt;3. LSTM Cell Backward Propagation(Detailed)&lt;/h4&gt;
&lt;p&gt;The gradient calculation through the complete network is divided into 2 parts&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#Feed-Forward Network layer(Step-1)&quot;&gt;Feed-Forward Network layer(Step-1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#Backward propagation through LSTMCell(Step-2)&quot;&gt;Backward propagation through LSTMCell(Step-2)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;-4&quot;&gt;&lt;a name=&quot;Feed-Forward Network layer(Step-1)&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h5 id=&quot;feed-forward-network-layerstep-1&quot;&gt;Feed-Forward Network layer(Step-1)&lt;/h5&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;We have already done this derivation for “&lt;a href=&quot;/articles/2019-05/softmax-and-its-gradient&quot;&gt;softmax&lt;/a&gt;” and “&lt;a href=&quot;/articles/2019-05/softmax-and-cross-entropy&quot;&gt;cross_entropy_loss&lt;/a&gt;”.&lt;/li&gt;
    &lt;li&gt;The final derived form in &lt;a href=&quot;/articles/2019-05/softmax-and-cross-entropy#eq-3&quot;&gt;eq-3&lt;/a&gt;. Which in turn depends on &lt;a href=&quot;/articles/2019-05/softmax-and-its-gradient#eq-1&quot;&gt;eq-1&lt;/a&gt; and &lt;a href=&quot;/articles/2019-05/softmax-and-cross-entropy#eq-2&quot;&gt;eq-2&lt;/a&gt;. &lt;strong&gt;We are calling it “pred” instead of “logits” in figure-5&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback5.jpg&quot; alt=&quot;Image: figure-8: &amp;lt;strong&amp;gt;The proof, intuition and program implementation&amp;lt;/strong&amp;gt; of the derivation can be found at &amp;lt;strong&amp;gt;&amp;lt;a href='/articles/2019-05/softmax-and-its-gradient'&amp;gt;softmax&amp;lt;/a&amp;gt;&amp;lt;/strong&amp;gt; and &amp;lt;strong&amp;gt;&amp;lt;a href='/articles/2019-05/softmax-and-cross-entropy'&amp;gt;cross_entropy_loss&amp;lt;/a&amp;gt;&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-8: &lt;strong&gt;The proof, intuition and program implementation&lt;/strong&gt; of the derivation can be found at &lt;strong&gt;&lt;a href=&quot;/articles/2019-05/softmax-and-its-gradient&quot;&gt;softmax&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;/articles/2019-05/softmax-and-cross-entropy&quot;&gt;cross_entropy_loss&lt;/a&gt;&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;“DBy”&lt;/strong&gt; is straight forward, once the gradients for pred is clear in the previous step.&lt;/li&gt;
    &lt;li&gt;This is just gradient calculation, the weights aren’t  adjusted just yet. They will be adjusted together in the end as a matter of convenience by an optimizer such as GradientDescentOptimizer.&lt;/li&gt;
    &lt;li&gt;The Complete code of listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/3fc68956eeaa082b59af2d181ff66e3932f49415/org/mk/training/dl/core.py#L157-L170&quot;&gt;code-1&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback6.jpg&quot; alt=&quot;Image: figure-9: &amp;lt;strong&amp;gt;DBy&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-9: &lt;strong&gt;DBy&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;“DWy”&lt;/strong&gt; is straight forward too, once the gradients for pred is clear in the previous step(figure-5).&lt;/li&gt;
    &lt;li&gt;The Complete code of listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/3fc68956eeaa082b59af2d181ff66e3932f49415/org/mk/training/dl/rnn.py#L444&quot;&gt;code-2&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback7.jpg&quot; alt=&quot;Image: figure-10: &amp;lt;strong&amp;gt;DWy&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10: &lt;strong&gt;DWy&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;“DWy”&lt;/strong&gt; and &lt;strong&gt;“Dby”&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback8.jpg&quot; alt=&quot;Image: figure-11: &amp;lt;strong&amp;gt;DWy&amp;lt;/strong&amp;gt; and &amp;lt;strong&amp;gt;Dby&amp;lt;/strong&amp;gt;. Only the gradients are calculated, the weights are adjusted once in the end.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-11: &lt;strong&gt;DWy&lt;/strong&gt; and &lt;strong&gt;Dby&lt;/strong&gt;. Only the gradients are calculated, the weights are adjusted once in the end.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-5&quot;&gt;&lt;a name=&quot;Backward propagation through LSTMCell(Step-2)&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h5 id=&quot;backward-propagation-through-lstmcellstep-2&quot;&gt;Backward propagation through LSTMCell(Step-2)&lt;/h5&gt;
&lt;p&gt;There are several gradients we are calculating, &lt;a href=&quot;LSTMPart-3#figure-6&quot;&gt;figure-6&lt;/a&gt; has a complete list.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#DHt&quot;&gt;DHt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#DOt&quot;&gt;DOt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#DCt&quot;&gt;DCt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#DCprojt&quot;&gt;DCprojt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#DFt&quot;&gt;DFt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#DIt&quot;&gt;DIt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#DHX&quot;&gt;DHX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;LSTMPart-3#DCt&quot;&gt;DCt&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;-6&quot;&gt;&lt;a name=&quot;DHt&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h6 id=&quot;dht&quot;&gt;DHt&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Probably the most confusing step backward propagation because there is an element of time.&lt;/li&gt;
    &lt;li&gt;There were 2 recursive elements h-state, c-state, similarly there 2 recursive elements while deriving the gradient backwards. We call it dh_next and dc_next, as you will see later both and many other gradients are derived from &lt;strong&gt;dHt&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback9.jpg&quot; alt=&quot;Image: figure-12: &amp;lt;strong&amp;gt;DHt&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-12: &lt;strong&gt;DHt&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Complete code of &lt;strong&gt;DHt&lt;/strong&gt; can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L445&quot;&gt;code-3&lt;/a&gt;,&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L492-L499&quot;&gt;code-4&lt;/a&gt;, and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L295-L299&quot;&gt;code-5&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback10.jpg&quot; alt=&quot;Image: figure-13: &amp;lt;strong&amp;gt;DHt&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-13: &lt;strong&gt;DHt&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;dHt&lt;/strong&gt; schematically.&lt;strong&gt;Shape is the same as h, for this example it is (5X1)&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback11.jpg&quot; alt=&quot;Image: figure-14: &amp;lt;strong&amp;gt;DHt&amp;lt;/strong&amp;gt; Shape is the same as h, for this example it is (5X1).&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-14: &lt;strong&gt;DHt&lt;/strong&gt; Shape is the same as h, for this example it is (5X1).&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-7&quot;&gt;&lt;a name=&quot;DOt&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h6 id=&quot;dot&quot;&gt;DOt&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Shape of &lt;strong&gt;DOt&lt;/strong&gt; is the same as h, for this example it is (5X1)&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;The complete listing is at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L328&quot;&gt;code-6&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback12.jpg&quot; alt=&quot;Image: figure-15: &amp;lt;strong&amp;gt;DOt&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-15: &lt;strong&gt;DOt&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DOt&lt;/strong&gt; schematically.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback13.jpg&quot; alt=&quot;Image: figure-16: &amp;lt;strong&amp;gt;DOt&amp;lt;/strong&amp;gt; schematically.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-16: &lt;strong&gt;DOt&lt;/strong&gt; schematically.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DOt&lt;/strong&gt;, &lt;strong&gt;DWo&lt;/strong&gt; and &lt;strong&gt;DBo&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L329-L331&quot;&gt;code-7&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback14.jpg&quot; alt=&quot;Image: figure-17: &amp;lt;strong&amp;gt;DOt&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;DWo&amp;lt;/strong&amp;gt; and &amp;lt;strong&amp;gt;DBo&amp;lt;/strong&amp;gt; and there shapes&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-17: &lt;strong&gt;DOt&lt;/strong&gt;, &lt;strong&gt;DWo&lt;/strong&gt; and &lt;strong&gt;DBo&lt;/strong&gt; and there shapes&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-8&quot;&gt;&lt;a name=&quot;DCt&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h6 id=&quot;dct&quot;&gt;DCt&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Shape of &lt;strong&gt;DCt&lt;/strong&gt; is the same as h, for this example it is (5X1)&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;The complete listing is at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L309&quot;&gt;code-8&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback15.jpg&quot; alt=&quot;Image: figure-18: &amp;lt;strong&amp;gt;DCt&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-18: &lt;strong&gt;DCt&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DCt&lt;/strong&gt; schematically.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback16.jpg&quot; alt=&quot;Image: figure-19: &amp;lt;strong&amp;gt;DCt&amp;lt;/strong&amp;gt; schematically.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-19: &lt;strong&gt;DCt&lt;/strong&gt; schematically.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-9&quot;&gt;&lt;a name=&quot;DCprojt&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h6 id=&quot;dcprojt&quot;&gt;DCprojt&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Shape of &lt;strong&gt;DCprojt&lt;/strong&gt; is the same as h, for this example it is (5X1)&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;The complete listing is at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L323&quot;&gt;code-9&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback17.jpg&quot; alt=&quot;Image: figure-20: &amp;lt;strong&amp;gt;DCprojt&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-20: &lt;strong&gt;DCprojt&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DCprojt&lt;/strong&gt; schematically.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback18.jpg&quot; alt=&quot;Image: figure-21: &amp;lt;strong&amp;gt;DCprojt&amp;lt;/strong&amp;gt; schematically.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-21: &lt;strong&gt;DCprojt&lt;/strong&gt; schematically.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DCprojt&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L324-L326&quot;&gt;code-10&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback19.jpg&quot; alt=&quot;Image: figure-22: &amp;lt;strong&amp;gt;DCprojt&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-22: &lt;strong&gt;DCprojt&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-10&quot;&gt;&lt;a name=&quot;DFt&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h6 id=&quot;dft&quot;&gt;DFt&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Shape of &lt;strong&gt;DFt&lt;/strong&gt; is the same as h, for this example it is (5X1)&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;The complete listing is at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L313&quot;&gt;code-11&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback20.jpg&quot; alt=&quot;Image: figure-23: &amp;lt;strong&amp;gt;DFt&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-23: &lt;strong&gt;DFt&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DFt&lt;/strong&gt; schematically.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback21.jpg&quot; alt=&quot;Image: figure-24: &amp;lt;strong&amp;gt;DFt&amp;lt;/strong&amp;gt; schematically.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-24: &lt;strong&gt;DFt&lt;/strong&gt; schematically.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DFt&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L314-L316&quot;&gt;code-12&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback22.jpg&quot; alt=&quot;Image: figure-25: &amp;lt;strong&amp;gt;DFt&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-25: &lt;strong&gt;DFt&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-11&quot;&gt;&lt;a name=&quot;DIt&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h6 id=&quot;dit&quot;&gt;DIt&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Shape of &lt;strong&gt;DIt&lt;/strong&gt; is the same as h, for this example it is (5X1)&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;The complete listing is at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L318&quot;&gt;code-13&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback23.jpg&quot; alt=&quot;Image: figure-26: &amp;lt;strong&amp;gt;DIt&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-26: &lt;strong&gt;DIt&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DIt&lt;/strong&gt; schematically.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback24.jpg&quot; alt=&quot;Image: figure-27: &amp;lt;strong&amp;gt;DIt&amp;lt;/strong&amp;gt; schematically.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-27: &lt;strong&gt;DIt&lt;/strong&gt; schematically.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DIt&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L320-L321&quot;&gt;code-14&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback25.jpg&quot; alt=&quot;Image: figure-28: &amp;lt;strong&amp;gt;DIt&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-28: &lt;strong&gt;DIt&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-12&quot;&gt;&lt;a name=&quot;DHX&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h6 id=&quot;dhx&quot;&gt;DHX&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Dhx&lt;/strong&gt;, &lt;strong&gt;Dh_next&lt;/strong&gt;, &lt;strong&gt;dxt&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344&quot;&gt;code-15&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback26.jpg&quot; alt=&quot;Image: figure-29: &amp;lt;strong&amp;gt;Dhx&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;Dh_next&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;dxt&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-29: &lt;strong&gt;Dhx&lt;/strong&gt;, &lt;strong&gt;Dh_next&lt;/strong&gt;, &lt;strong&gt;dxt&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Many models like Neural Machine Translators(NMT), Bidirectional Encoder Representation from Transformers(BERT) use word embeddings as their inputs(Xs) which oftentimes need learning and that is where we need &lt;strong&gt;dxt&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback27.jpg&quot; alt=&quot;Image: figure-30: &amp;lt;strong&amp;gt;Dhx&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;Dh_next&amp;lt;/strong&amp;gt;, &amp;lt;strong&amp;gt;dxt&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-30: &lt;strong&gt;Dhx&lt;/strong&gt;, &lt;strong&gt;Dh_next&lt;/strong&gt;, &lt;strong&gt;dxt&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;-13&quot;&gt;&lt;a name=&quot;DCt_recur&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h6 id=&quot;dct_recur&quot;&gt;DCt_recur&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;DCt&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;Complete listing can be found at &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L342&quot;&gt;code-16&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmback28.jpg&quot; alt=&quot;Image: figure-31: &amp;lt;strong&amp;gt;DCt&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-31: &lt;strong&gt;DCt&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;There you go, a complete documentation of LSTMCell’s backward propagation. Also when using &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt; you could enable forward or backward logging like so&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;  &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backpassdebug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;out_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_biases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backpassdebug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We went through “hell” and back in this article. But if you have come so far then the good news is, &lt;strong&gt;it can only get easier&lt;/strong&gt;. When we do use LSTMs in more complex architecture like Neural Machine Translators&lt;strong&gt;(NMT)&lt;/strong&gt;, Bidirectional Encoder Representation from Transformers&lt;strong&gt;(BERT)&lt;/strong&gt;, or a Differentiable Neural Computers&lt;strong&gt;(DNC)&lt;/strong&gt; &lt;strong&gt;it will be a walk in a blooming lovely park&lt;/strong&gt;. Well alomost!! ;-). In a later article well demystify the multilayer LSTMcells and Bi-directional LSTMCells.&lt;/p&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="RNN" /><category term="Sequence-Modelling" /><summary type="html">In this multi-part series(part-1,part-2), we look inside LSTM Backward Propagation. This would usually involve lots and lots of math. I love math but I am not a trained mathematician. However, I have decent intuition and intuition by itself can only get you so far. So I used my programming skills to validate pure theoretical results often cited by pure/trained mathematicians. I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</summary></entry><entry><title type="html">RNN Series:LSTM internals:Part-1:The Big Picture</title><link href="http://localhost:4000/articles/2019-07/LSTMPart-1" rel="alternate" type="text/html" title="RNN Series:LSTM internals:Part-1:The Big Picture" /><published>2019-07-09T15:07:19+00:00</published><updated>2019-07-09T15:07:19+00:00</updated><id>http://localhost:4000/articles/2019-07/LSTMPart-1</id><content type="html" xml:base="http://localhost:4000/articles/2019-07/LSTMPart-1">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;LSTMs today are cool. Everything, well almost everything, in the modern Deep Learning landscape is made of LSTMs. I know that might be an exaggeration but, it can only be understated that they are absolutely indispensable. They make up everything directly or indirectly from Time Series(TS) to Neural Machine Translators(NMT) to Bidirectional Encoder Representation from Transformers(BERT) to Neural Turing Machine(NTM) to Differentiable Neural Computers(DNC) etc, and yet they are not very well understood. Whilst there are the elites that write frameworks (Tensorflow, Pytorch, etc), but they are not surprisingly very few in number. The common “Machine learning/Deep Learning”  “man/programmer” at best knows how to use it. &lt;strong&gt;This is a &lt;a href=&quot;/tags/#LSTM&quot;&gt;multi-part series&lt;/a&gt; that will unravel the mystery behind LSTMs. Especially it’s gradient calculation when participating in a more complex model like NMT, BERT, NTM, DNC, etc. I do this using the &lt;a href=&quot;https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0&quot;&gt;first principles&lt;/a&gt; approach for which I have pure python implementation (&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt;) of most complex Deep Learning models.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-this-article-is-not-about&quot;&gt;What this article is not about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This article will not talk about the conceptual model of LSTM, on which there is some great existing material here and here in the order of difficulty.&lt;/li&gt;
    &lt;li&gt;This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome, if a somewhat &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;difficult post&lt;/a&gt;, by Andrej.&lt;/li&gt;
    &lt;li&gt;This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts &lt;a href=&quot;https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577&quot;&gt;here&lt;/a&gt; in the order of difficulty&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;what-this-article-is-about&quot;&gt;What this article is about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The reason why understanding LSTM’s inner working is vital is that it underpins the most important models of modern Artificial Intelligence.&lt;/li&gt;
    &lt;li&gt;Underpins &lt;strong&gt;memory augmented&lt;/strong&gt; models and architecture.&lt;/li&gt;
    &lt;li&gt;Most often it is not used in its vanilla form. There is directionality involved, multiple layers involved, making it a complex moving piece in itself.&lt;/li&gt;
    &lt;li&gt;In this multi-part series, I get under the cover of an LSTM using &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt; especially its back-propagation when it is a part of a more complex model.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;
&lt;p&gt;Start with the simplest word-generation case-study. We train an LSTM network on a short story. Once trained, we ask it to generate new stories giving it a cue of a few starting words. &lt;strong&gt;The focus primarily is on the training process here.&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Picking a starting point in the story that is randomly chosen.&lt;/li&gt;
    &lt;li&gt;The training process is feeding in a sequence of n words from the story and checking if (n+1)th word predicted by the model is correct or not. Back propagating and adjusting the weights of the model.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn.jpeg&quot; alt=&quot;Image: figure-1: While there are many ways in which the RNNs can be used. For this example we consider the many-to-one organization.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: While there are many ways in which the RNNs can be used. For this example we consider the many-to-one organization.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;training-process&quot;&gt;Training Process&lt;/h3&gt;
&lt;p&gt;In the next few illustrations we are looking at primary the shape of input, how is the input fed in, when do we check the output before back-propagation happens, etc. All this &lt;strong&gt;without looking inside the LSTM Cell just yet&lt;/strong&gt;. Batch=1(for parallelism), sequence=3(3 words in our example), size=10(vocabulary size 10 so our representation is a one-hot of size 10).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Sequence of 3 shown below, but that can depend on the use-case.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn.jpg&quot; alt=&quot;Image: figure-2: Batch enables parallelism, but for simplicity assumed as one.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: Batch enables parallelism, but for simplicity assumed as one.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Functionally dynamic_rnn feeds in the batches of sequences but its code is written using “tf.while_loop” and not one of python’s native loop construct. With any one of python’s native loop construct only the code within an iteration be optimized on a GPU but with “tf.while_loop” the advantage is that the complete loop can be optimized on a GPU.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn1.jpg&quot; alt=&quot;Image: figure-3: Function logic of dynamic_rnn on the bottom right. Complete listing of &amp;lt;a href='https://github.com/slowbreathing/Deep-Breathe'&amp;gt;DEEP-Breathe's&amp;lt;/a&amp;gt; &amp;lt;a href='https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn.py#L214-L285'&amp;gt;dynamic_rnn&amp;lt;/a&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: Function logic of dynamic_rnn on the bottom right. Complete listing of &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;DEEP-Breathe's&lt;/a&gt; &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn.py#L214-L285&quot;&gt;dynamic_rnn&lt;/a&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Returns a tuple of &lt;strong&gt;output(H-state across all time steps)&lt;/strong&gt; and states(C-state and H-state for the last time step).&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn2.jpg&quot; alt=&quot;Image: figure-4: &amp;lt;strong&amp;gt;output(H-state across all time steps)&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: &lt;strong&gt;output(H-state across all time steps)&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Returns a tuple of output(H-state across all time steps) and &lt;strong&gt;states(C-state and H-state for the lst time step)&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn3.jpg&quot; alt=&quot;Image: figure-5: &amp;lt;strong&amp;gt;states(C-state and H-state for the lst time step)&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: &lt;strong&gt;states(C-state and H-state for the lst time step)&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Multiplier Wy and By which among other things shape the last “h” into the shape of X(10) so a comparison can be made.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn4.jpg&quot; alt=&quot;Image: figure-6: So the shape of &amp;lt;strong&amp;gt;pred&amp;lt;/strong&amp;gt;(the first return from 'RNN()' function should be that of &amp;lt;strong&amp;gt;X&amp;lt;/strong&amp;gt;)&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: So the shape of &lt;strong&gt;pred&lt;/strong&gt;(the first return from 'RNN()' function should be that of &lt;strong&gt;X&lt;/strong&gt;)&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Finally &lt;a href=&quot;/articles/2019-05/softmax-and-its-gradient&quot;&gt;softmax&lt;/a&gt; and &lt;a href=&quot;/articles/2019-05/softmax-and-cross-entropy&quot;&gt;cross_entropy_loss&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/rnn5.jpg&quot; alt=&quot;Image: figure-7: cross_entropy_loss and then the usual GradientDescentOptimizer or some other optimizer.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: cross_entropy_loss and then the usual GradientDescentOptimizer or some other optimizer.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;quick-comparision-between-tensorflow-and-deep-breathe&quot;&gt;Quick Comparision between &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;DEEP-Breathe&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;This particular branch of &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;DEEP-Breathe&lt;/a&gt; has been designed to compare weights with &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt; code every step of the way. While there are many reasons, one of the primary reasons for that is understanding. However, this is only possible if both &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;DEEP-Breathe&lt;/a&gt; must start with the same initial weights and their Y-multipliers(Wy and By) must also be identical. Consider the code below.&lt;/p&gt;

&lt;h4 id=&quot;tensorflow&quot;&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt;&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;  &lt;span class=&quot;c&quot;&gt;# RNN output node weights and biases&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.09588283&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.2044923&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.74828255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.14180686&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.32083616&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9444244&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.06826905&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9728962&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.18506959&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.0618515&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.156649&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2738173&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.2556943&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9079511&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.82127047&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.1448543&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.60807484&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5885713&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.0378786&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7088431&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.006477&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.28033388&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1804534&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.8093307&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.36991575&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
         &lt;span class=&quot;mf&quot;&gt;0.29115433&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01028167&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7357091&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.92254084&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.10753923&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.19266959&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.6108299&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.2495654&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5288974&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.0172302&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
         &lt;span class=&quot;mf&quot;&gt;1.1311738&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2666629&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.30611828&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01412263&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.44799015&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.19266959&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.6108299&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.2495654&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5288974&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.0172302&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
         &lt;span class=&quot;mf&quot;&gt;1.1311738&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2666629&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.30611828&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01412263&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.44799015&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;biases&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1458478&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3660951&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.1647317&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.9633691&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.24532059&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;mf&quot;&gt;0.14005205&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0961286&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.43737876&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.7028531&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.8481724&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;RNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;biases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;other&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnn_cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LayerNormBasicLSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dynamic_rnn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;biases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;biases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;biases_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;biases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The pre-initialized weights and biases in line(2) and line(16) utilized as Wy and By in line(26)&lt;/li&gt;
    &lt;li&gt;The internal weights of LSTM initialized in line(22-23)&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Tensorflow graph mode is the most non pythonic design done in python&lt;/strong&gt;. It sounds crazy but is true. Consider line(21-26), this function gets called multiple times in the training loop and yet the &lt;strong&gt;cell(line(24)) is the same cell instance across multiple iterations&lt;/strong&gt;. Tensorflow in the graph mode builds the DAG first and &lt;strong&gt;identifies the cell by the name in that graph&lt;/strong&gt;. That is source of confusion to most programmers, especially python programmers.&lt;/li&gt;
    &lt;li&gt;The complete executable &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/tfwordslstm.py&quot;&gt;Listing-1&lt;/a&gt; and its &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L13&quot;&gt;execution script&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;deep-breathe&quot;&gt;&lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe&quot;&gt;Deep-Breathe&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;I could have done without a similar behavior in Deep-Breathe but the programmer in me just could not resist. Especially since I earned my stripes in C/C++ and assembly kind of languages, python proved challenging. Without building a full-fledged graph, I associated the variable name with the corresponding weight, i.e. when the variable name is the same it fetches the same set of weights. This is difficult and not recommended but, if you care to see, then its demonstrated &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn.py#L75-L76&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/common.py#L241-L255&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn_cell.py#L79-L81&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/core.py#L107-L112&quot;&gt;here&lt;/a&gt;. &lt;strong&gt;To summarize in the Listing-2 there is a single set of weights for the LSTMCell despite getting called multiple times in the loop.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;RNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;biases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WeightsInitializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dynamic_rnn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols_in_keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;Dense in this case should be out of WeightsInitializer scope because we are passing constants&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;out_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_biases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;LOSS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_biases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end_offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;offset:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;symbols_in_keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;symbols_in_keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols_in_keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;symbols_in_keys:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols_in_keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;yhat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LOSS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols_in_keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;Listing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The complete executable &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMainGraph.py&quot;&gt;graph-mode&lt;/a&gt; and its &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L15&quot;&gt;execution script&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;The complete executable for &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMain.py&quot;&gt;non-graph-version&lt;/a&gt; and its &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L14&quot;&gt;execution script&lt;/a&gt;. To make Tensorflow more pythonic, they have enabled eager mode and will be the default going forward especially from Tensorflow 2.0 onwards. However, major project as on today is still on the older version.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;In summary, then, that was the walk through of code surrounding LSTM training, without getting into LSTM or its weights just yet. That would be the topic of our next article.&lt;/p&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="RNN" /><category term="Sequence-Modelling" /><summary type="html">LSTMs make up everything directly or indirectly from Time Series(TS) to Neural Machine Translators(NMT) to Bidirectional Encoder Representation from Transformers(BERT) to Neural Turing Machine(NTM) to Differentiable Neural Computers(DNC) etc, and yet they are not very well understood. This is a multi-part series that will unravel the mystery behind LSTMs. Especially it's gradient calculation when participating in a more complex model like NMT, BERT, NTM, DNC, etc. I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</summary></entry><entry><title type="html">RNN Series:LSTM internals:Part-2:The Forward pass</title><link href="http://localhost:4000/articles/2019-07/LSTMPart-2" rel="alternate" type="text/html" title="RNN Series:LSTM internals:Part-2:The Forward pass" /><published>2019-07-09T15:07:19+00:00</published><updated>2019-07-09T15:07:19+00:00</updated><id>http://localhost:4000/articles/2019-07/LSTMPart-2</id><content type="html" xml:base="http://localhost:4000/articles/2019-07/LSTMPart-2">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;In this &lt;a href=&quot;/tags/#LSTM&quot;&gt;multi-part series&lt;/a&gt;, we look inside LSTM forward pass.&lt;/strong&gt; If you haven’t already read it I suggest run through the previous parts (&lt;a href=&quot;/articles/2019-07/LSTMPart-1&quot;&gt;part-1&lt;/a&gt;) before you come back here. Once you are back, in this article, we explore the meaning, math and the implementation of an LSTM cell. &lt;strong&gt;I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-this-article-is-not-about&quot;&gt;What this article is not about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;This article will not talk about the conceptual model of LSTM, on which there is some great existing material &lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html&quot;&gt;here&lt;/a&gt; in the order of difficulty.&lt;/li&gt;
    &lt;li&gt;This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome post by Andrej if a somewhat &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;difficult post&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts &lt;a href=&quot;https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577&quot;&gt;here&lt;/a&gt; in the order of difficulty&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;what-this-article-is-about&quot;&gt;What this article is about?&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Using the &lt;a href=&quot;https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0&quot;&gt;first principles&lt;/a&gt; we picturize/visualize the forward pass of an LSTM cell.&lt;/li&gt;
    &lt;li&gt;Then we associate code with the picture to make our understanding more concrete&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;
&lt;p&gt;This is the same example and the context is the same as described in &lt;a href=&quot;/articles/2019-07/LSTMPart-1&quot;&gt;part-1&lt;/a&gt;. &lt;strong&gt;The focus however, this time is on LSTMCell.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;lstm-cell&quot;&gt;LSTM Cell&lt;/h3&gt;
&lt;p&gt;While there are many variations to the LSTM cell. In the current article, we are looking at &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell&quot;&gt;LayerNormBasicLSTMCell&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Weights and biases of the LSTM cell.&lt;/li&gt;
    &lt;li&gt;Shapes color-coded.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm1.jpg&quot; alt=&quot;Image: figure-1: Batch=1, State=5(internal states(C and H)), input_size=10(assuming vocab size of 10 translates to one-hot of size 10) .&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-1: Batch=1, State=5(internal states(C and H)), input_size=10(assuming vocab size of 10 translates to one-hot of size 10) .&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Weights, biases of the LSTM cell. Also shown is the cell state (c-state) over and above the h-state of a vanilla RNN.&lt;/li&gt;
    &lt;li&gt;h&lt;sub&gt;t-1&lt;/sub&gt; may be initialized by default with zeros.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm2.jpg&quot; alt=&quot;Image: figure-2: Weights and biases for all the gates f(forget), i(input), c(candidatecellstate), o(output). The Shapes of these &amp;lt;strong&amp;gt;weights&amp;lt;/strong&amp;gt; are &amp;lt;strong&amp;gt;5X15&amp;lt;/strong&amp;gt; individually and stacked vertically as shown they measure &amp;lt;strong&amp;gt;20X15&amp;lt;/strong&amp;gt;. The Shapes of the &amp;lt;strong&amp;gt;biases&amp;lt;/strong&amp;gt; are &amp;lt;strong&amp;gt;5X1&amp;lt;/strong&amp;gt; individually and stacked up &amp;lt;strong&amp;gt;20X1&amp;lt;/strong&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-2: Weights and biases for all the gates f(forget), i(input), c(candidatecellstate), o(output). The Shapes of these &lt;strong&gt;weights&lt;/strong&gt; are &lt;strong&gt;5X15&lt;/strong&gt; individually and stacked vertically as shown they measure &lt;strong&gt;20X15&lt;/strong&gt;. The Shapes of the &lt;strong&gt;biases&lt;/strong&gt; are &lt;strong&gt;5X1&lt;/strong&gt; individually and stacked up &lt;strong&gt;20X1&lt;/strong&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Weights and biases in the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L76&quot;&gt;code&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;Often the code accepts constants as weights for comparison with &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm3.jpg&quot; alt=&quot;Image: figure-3: &amp;lt;strong&amp;gt;4*hidden_size(5),input_size(10)+hidden_size(5)+1(biases)&amp;lt;/strong&amp;gt;. Making it &amp;lt;strong&amp;gt;20X16&amp;lt;/strong&amp;gt;, biases allocated with weights is a matter of convenience and performance.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-3: &lt;strong&gt;4*hidden_size(5),input_size(10)+hidden_size(5)+1(biases)&lt;/strong&gt;. Making it &lt;strong&gt;20X16&lt;/strong&gt;, biases allocated with weights is a matter of convenience and performance.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Equations of LSTM and their interplay with weights and biases.&lt;/li&gt;
    &lt;li&gt;h&lt;sub&gt;t-1&lt;/sub&gt; and c&lt;sub&gt;t-1&lt;/sub&gt; may be initialized by default with zeros(Language Model) or some non-zero state for conditional language model.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm4.jpg&quot; alt=&quot;Image: figure-4: Equations of LSTM and their interplay with weights and biases.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-4: Equations of LSTM and their interplay with weights and biases.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;GEMM(General Matrix Multiplication) in &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L210-L214&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;h and x concatenation is only a performance convenience.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm5.jpg&quot; alt=&quot;Image: figure-5: GEMM and running them though various gates.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-5: GEMM and running them though various gates.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;The &lt;strong&gt;mathematical reason&lt;/strong&gt; why the &lt;strong&gt;vanishing gradient&lt;/strong&gt; is &lt;strong&gt;mitigated&lt;/strong&gt;.&lt;/li&gt;
    &lt;li&gt;There are many theories though. Look into this in detail with back-propagation.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm6.jpg&quot; alt=&quot;Image: figure-6: New cell state is a function of old cell state and new candidate state.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-6: New cell state is a function of old cell state and new candidate state.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;New cell state(c-state) in &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L215&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;This is used in 2 ways as illustrated in figure-8.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm7.jpg&quot; alt=&quot;Image: figure-7: The previous steps already decided what to do, we just need to actually do it.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-7: The previous steps already decided what to do, we just need to actually do it.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;c-state&lt;sub&gt;t&lt;/sub&gt; is used to calculate the “externally visible” h-state&lt;sub&gt;t&lt;/sub&gt;&lt;/li&gt;
    &lt;li&gt;Sometimes referred to as c&lt;sub&gt;t&lt;/sub&gt;, h&lt;sub&gt;t&lt;/sub&gt;, used as the states at the next time step.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm8.jpg&quot; alt=&quot;Image: figure-8: Conceptually it is the same cell transitioning into the next state. &amp;lt;strong&amp;gt;In this case c&amp;lt;sub&amp;gt;t&amp;lt;/sub&amp;gt;.&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-8: Conceptually it is the same cell transitioning into the next state. &lt;strong&gt;In this case c&lt;sub&gt;t&lt;/sub&gt;.&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;New h-state in &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L216&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The transition to the next time step is complete with h&lt;sub&gt;t&lt;/sub&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm9.jpg&quot; alt=&quot;Image: figure-9: Conceptually it is the same cell transitioning into the next state. &amp;lt;strong&amp;gt;In this case h&amp;lt;sub&amp;gt;t&amp;lt;/sub&amp;gt;.&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-9: Conceptually it is the same cell transitioning into the next state. &lt;strong&gt;In this case h&lt;sub&gt;t&lt;/sub&gt;.&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;h&lt;sub&gt;t&lt;/sub&gt; multiplied by the &lt;strong&gt;“Wy”&lt;/strong&gt; and &lt;strong&gt;“by”&lt;/strong&gt; added to it.&lt;/li&gt;
    &lt;li&gt;Traditionally we call this value “logits” or “preds”.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm10.jpg&quot; alt=&quot;Image: figure-10: Wy shape is 10X5 multiplied by h&amp;lt;sub&amp;gt;t&amp;lt;/sub&amp;gt; which is 5X1 and by added to it which is 10X1 as shown above.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-10: Wy shape is 10X5 multiplied by h&lt;sub&gt;t&lt;/sub&gt; which is 5X1 and by added to it which is 10X1 as shown above.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Usually, this is done in on the client-side code because only the clients know the model and when the loss calculation can be done.&lt;/li&gt;
    &lt;li&gt;Tensorflow version of the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/tfwordslstm.py#L95-L104&quot;&gt;code&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm11.jpg&quot; alt=&quot;Image: figure-11: DEEP-Breathe version of the &amp;lt;a href='https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/LSTMMainGraph.py#L98-L109'&amp;gt;code&amp;lt;/a&amp;gt;.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-11: DEEP-Breathe version of the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/LSTMMainGraph.py#L98-L109&quot;&gt;code&lt;/a&gt;.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Usually done internally by “cross_entropy_loss” function.&lt;/li&gt;
    &lt;li&gt;Shown here just so that you get an idea.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm12.jpg&quot; alt=&quot;Image: figure-12: Traditionally called yhat.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-12: Traditionally called yhat.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Does a combination of softmax(figure-12) and loss calculation(figure-13).&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;/articles/2019-05/softmax-and-its-gradient&quot;&gt;Softmax&lt;/a&gt; and a &lt;a href=&quot;/articles/2019-05/softmax-and-cross-entropy&quot;&gt;cross_entropy_loss&lt;/a&gt; to jog your memory.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm13.jpg&quot; alt=&quot;Image: figure-13: Similar to logistic regression and its cost function.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-13: Similar to logistic regression and its cost function.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Summary with weights&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmcelltemplate5.jpg&quot; alt=&quot;Image: figure-15: &amp;lt;strong&amp;gt;Summary with weights.&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-15: &lt;strong&gt;Summary with weights.&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Summary as flow diagram&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstmforward.jpg&quot; alt=&quot;Image: figure-14: &amp;lt;strong&amp;gt;Summary as simple flow diagram.&amp;lt;/strong&amp;gt;&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-14: &lt;strong&gt;Summary as simple flow diagram.&lt;/strong&gt;&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;A Sequence of 3 shown below, but that can depend on the use-case.&lt;/li&gt;
    &lt;li&gt;Tensorflow version of the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/tfwordslstm.py#L95-L104&quot;&gt;code&lt;/a&gt; and DEEP-Breathe version of the &lt;a href=&quot;https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/LSTMMainGraph.py#L98-L109&quot;&gt;code&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    
    &lt;img src=&quot;/img/rnn/lstm14.jpg&quot; alt=&quot;Image: figure-16: The code below is merely shown to give you an idea of the sequence. In practice the 'dynamic_rnn' function does iteration part, the preds are multiplied 'Wy' added to 'By' by client code and then handed over to 'cross_entropy_loss' function.&quot; hight=&quot;110%&quot; width=&quot;110%&quot; /&gt;
    
    &lt;figcaption&gt;&lt;center&gt;&lt;span class=&quot;faded_text&quot;&gt;figure-16: The code below is merely shown to give you an idea of the sequence. In practice the 'dynamic_rnn' function does iteration part, the preds are multiplied 'Wy' added to 'By' by client code and then handed over to 'cross_entropy_loss' function.&lt;/span&gt;&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;In summary, then, that was the walk through of LSTM’s forward pass. As a study in contrast, if building a Language model that predicts the next word in the sequence, the training would be similar but we’ll calculate loss at every step. The label would be the ‘X’ just one time-step advanced. However, Let’s not get ahead of ourselves, before we get to a language model, let’s look at the backward pass(Back Propagation) for LSTM in the next post.&lt;/p&gt;</content><author><name></name></author><category term="Artificial-Intelligence" /><category term="Deep-Learning" /><category term="LSTM" /><category term="RNN" /><category term="Sequence-Modelling" /><summary type="html">In this multi-part series, we look inside LSTM forward pass. Read the part-1's before you come back here. Once you are back, in this article, we explore the meaning, math and the implementation of an LSTM cell. I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</summary></entry></feed>