---
layout: post
title:  "RNN Series:LSTM internals:Part-2:The Forward pass"
excerpt: "<strong>In this multi-part series, we look inside LSTM forward pass.</strong> Read the <a href='/articles/2019-07/LSTMPart-1'>part-1's</a> before you come back here. Once you are back, in this article, we explore the meaning, math and the implementation of an LSTM cell. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>"
mathjax: true
date:   2019-07-09 15:07:19
categories: [article]
tags: Artificial-Intelligence Deep-Learning LSTM RNN Sequence-Modelling
comments: true
---

### Introduction
<strong>In this [multi-part series][multi-part series], we look inside LSTM forward pass.</strong> If you haven't already read it I suggest run through the previous parts ([part-1]) before you come back here. Once you are back, in this article, we explore the meaning, math and the implementation of an LSTM cell. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>

### What this article is not about?
> * This article will not talk about the conceptual model of LSTM, on which there is some great existing material [here][lstm-1] and [here][lstm-2] in the order of difficulty.
> * This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome post by Andrej if a somewhat [difficult post][lstm-3].
> * This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts [here][lstm-4] and [here][lstm-5] in the order of difficulty     

### What this article is about?
> * Using the [first principles][first principles] we picturize/visualize the forward pass of an LSTM cell.
> * Then we associate code with the picture to make our understanding more concrete  

### Context
This is the same example and the context is the same as described in [part-1]. <strong>The focus however, this time is on LSTMCell.</strong>

### LSTM Cell
While there are many variations to the LSTM cell. In the current article, we are looking at [LayerNormBasicLSTMCell].
> * Weights and biases of the LSTM cell.
> * Shapes color-coded.
{%
    include image.html
    src="/img/rnn/lstm1.jpg"
    caption="figure-1: Batch=1, State=5(internal states(C and H)), input_size=10(assuming vocab size of 10 translates to one-hot of size 10) ."
    hight="110%"
    width="110%"
%}

> * Weights, biases of the LSTM cell. Also shown is the cell state (c-state) over and above the h-state of a vanilla RNN.
> * h<sub>t-1</sub> may be initialized by default with zeros.
{%
    include image.html
    src="/img/rnn/lstm2.jpg"
    caption="figure-2: Weights and biases for all the gates f(forget), i(input), c(candidatecellstate), o(output). The Shapes of these <strong>weights</strong> are <strong>5X15</strong> individually and stacked vertically as shown they measure <strong>20X15</strong>. The Shapes of the <strong>biases</strong> are <strong>5X1</strong> individually and stacked up <strong>20X1</strong>."
    hight="110%"
    width="110%"
%}

> * Weights and biases in the [code][code-1].
> * Often the code accepts constants as weights for comparison with [Tensorflow]  
{%
    include image.html
    src="/img/rnn/lstm3.jpg"
    caption="figure-3: <strong>4*hidden_size(5),input_size(10)+hidden_size(5)+1(biases)</strong>. Making it <strong>20X16</strong>, biases allocated with weights is a matter of convenience and performance."
    hight="110%"
    width="110%"
%}

> * Equations of LSTM and their interplay with weights and biases.
> * h<sub>t-1</sub> and c<sub>t-1</sub> may be initialized by default with zeros(Language Model) or some non-zero state for conditional language model.
{%
    include image.html
    src="/img/rnn/lstm4.jpg"
    caption="figure-4: Equations of LSTM and their interplay with weights and biases."
    hight="110%"
    width="110%"
%}

> * GEMM(General Matrix Multiplication) in [code][code-2]
> * h and x concatenation is only a performance convenience.
{%
    include image.html
    src="/img/rnn/lstm5.jpg"
    caption="figure-5: GEMM and running them though various gates."
    hight="110%"
    width="110%"
%}

> * The <strong>mathematical reason</strong> why the <strong>vanishing gradient</strong> is <strong>mitigated</strong>.
> * There are many theories though. Look into this in detail with back-propagation.
{%
    include image.html
    src="/img/rnn/lstm6.jpg"
    caption="figure-6: New cell state is a function of old cell state and new candidate state."
    hight="110%"
    width="110%"
%}

> * New cell state(c-state) in [code][code-3]
> * This is used in 2 ways as illustrated in figure-8.
{%
    include image.html
    src="/img/rnn/lstm7.jpg"
    caption="figure-7: The previous steps already decided what to do, we just need to actually do it."
    hight="110%"
    width="110%"
%}

> * c-state<sub>t</sub> is used to calculate the "externally visible" h-state<sub>t</sub>
> * Sometimes referred to as c<sub>t</sub>, h<sub>t</sub>, used as the states at the next time step.
{%
    include image.html
    src="/img/rnn/lstm8.jpg"
    caption="figure-8: Conceptually it is the same cell transitioning into the next state. <strong>In this case c<sub>t</sub>.</strong>"
    hight="110%"
    width="110%"
%}

> * New h-state in [code][code-4]
> * The transition to the next time step is complete with h<sub>t</sub>.
{%
    include image.html
    src="/img/rnn/lstm9.jpg"
    caption="figure-9: Conceptually it is the same cell transitioning into the next state. <strong>In this case h<sub>t</sub>.</strong>"  
    hight="110%"
    width="110%"
%}

> * h<sub>t</sub> multiplied by the <strong>"Wy"</strong> and <strong>"by"</strong> added to it.
> * Traditionally we call this value "logits" or "preds".
{%
    include image.html
    src="/img/rnn/lstm10.jpg"
    caption="figure-10: Wy shape is 10X5 multiplied by h<sub>t</sub> which is 5X1 and by added to it which is 10X1 as shown above."
    hight="110%"
    width="110%"
%}

> * Usually, this is done in on the client-side code because only the clients know the model and when the loss calculation can be done.
> * Tensorflow version of the [code][code-5].
{%
    include image.html
    src="/img/rnn/lstm11.jpg"
    caption="figure-11: DEEP-Breathe version of the <a href='https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/LSTMMainGraph.py#L98-L109'>code</a>."
    hight="110%"
    width="110%"
%}

> * Usually done internally by "cross_entropy_loss" function.
> * Shown here just so that you get an idea.
{%
    include image.html
    src="/img/rnn/lstm12.jpg"
    caption="figure-12: Traditionally called yhat."
    hight="110%"
    width="110%"
%}

> * Does a combination of softmax(figure-12) and loss calculation(figure-13).
> * [Softmax][softmax] and a [cross_entropy_loss][cross_entropy_loss] to jog your memory.  
{%
    include image.html
    src="/img/rnn/lstm13.jpg"
    caption="figure-13: Similar to logistic regression and its cost function."
    hight="110%"
    width="110%"
%}

> * <strong>Summary with weights</strong>.
{%
    include image.html
    src="/img/rnn/lstmcelltemplate5.jpg"
    caption="figure-15: <strong>Summary with weights.</strong>"
    hight="110%"
    width="110%"
%}

> * <strong>Summary as flow diagram</strong>.  
{%
    include image.html
    src="/img/rnn/lstmforward.jpg"
    caption="figure-14: <strong>Summary as simple flow diagram.</strong>"
    hight="110%"
    width="110%"
%}


> * A Sequence of 3 shown below, but that can depend on the use-case.
> * Tensorflow version of the [code][code-5] and DEEP-Breathe version of the [code][code-6].
{%
    include image.html
    src="/img/rnn/lstm14.jpg"
    caption="figure-16: The code below is merely shown to give you an idea of the sequence. In practice the 'dynamic_rnn' function does iteration part, the preds are multiplied 'Wy' added to 'By' by client code and then handed over to 'cross_entropy_loss' function."
    hight="110%"
    width="110%"
%}


### Summary
In summary, then, that was the walk through of LSTM's forward pass. As a study in contrast, if building a Language model that predicts the next word in the sequence, the training would be similar but we'll calculate loss at every step. The label would be the 'X' just one time-step advanced. However, Let's not get ahead of ourselves, before we get to a language model, let's look at the backward pass(Back Propagation) for LSTM in the next post.


[part-1]: /articles/2019-07/LSTMPart-1
[part-2]: /articles/2019-07/LSTMPart-2
[multi-part series]: /tags/#LSTM
[first principles]: https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0
[Deep-Breathe]: https://github.com/slowbreathing/Deep-Breathe
[LayerNormBasicLSTMCell]: https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell
[lstm-1]: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
[lstm-2]: https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
[lstm-3]: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[lstm-4]: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html
[lstm-5]: https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577

[code-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L76
[code-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L210-L214
[code-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L215
[code-4]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/rnn_cell.py#L216
[code-5]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/tfwordslstm.py#L95-L104
[code-6]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/org/mk/training/dl/LSTMMainGraph.py#L98-L109

[softmax]: /articles/2019-05/softmax-and-its-gradient
[cross_entropy_loss]: /articles/2019-05/softmax-and-cross-entropy
[Tensorflow]: https://www.tensorflow.org/
[Listing-1]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/tfwordslstm.py
[Listing-2]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMainGraph.py
[Listing-3]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMain.py
[scr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L13
[scr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L15
[scr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L14
[pygr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn.py#L75-L76
[pygr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/common.py#L241-L255
[pygr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn_cell.py#L79-L81
[pygr-4]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/core.py#L107-L112
