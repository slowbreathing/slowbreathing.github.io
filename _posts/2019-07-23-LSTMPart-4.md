---
layout: post
title:  "RNN Series:LSTM internals:Part-4: MultiRNNCell and Bidirectional RNNs"
excerpt: "In this multi-part series(<a href='/articles/2019-07/LSTMPart-1'>part-1</a>,<a href='/articles/2019-07/LSTMPart-2'>part-2</a>,<a href='/articles/2019-07/LSTMPart-3'>part-3</a>) we look into composing LSTM into multiple higher layers and its directionality. Though <strong>Multiple layers</strong> are compute-intensive, they have better accuracy and so does <strong>bidirectional connections.</strong> More importantly, a solid understanding of the above mentioned paves the way for concepts like <strong>Highway connections</strong>, <strong>Residual Connections</strong>, <strong>Pointer networks</strong>, <strong>Encoder-Decoder</strong> Architectures and so forth in a future article. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>"
mathjax: true
date:   2019-07-22 15:07:19
categories: [article]
tags: Artificial-Intelligence Deep-Learning LSTM RNN Sequence-Modelling
comments: true
---

### Introduction
In this [multi-part series] (<a href='/articles/2019-07/LSTMPart-1'>part-1</a>,<a href='/articles/2019-07/LSTMPart-2'>part-2</a>,<a href='/articles/2019-07/LSTMPart-3'>part-3</a>) we look into composing LSTM into multiple higher layers and its directionality. Though <strong>Multiple layers</strong> are compute-intensive, they have better accuracy and so does <strong>bidirectional connections.</strong> More importantly, a solid understanding of the above mentioned paves the way for concepts like <strong>Highway connections</strong>, <strong>Residual Connections</strong>, <strong>Pointer networks</strong>, <strong>Encoder-Decoder</strong> Architectures and so forth in future article. <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models</strong>.

### What this article is not about?
> * This article will not talk about the conceptual model of LSTM, on which there is some great existing material here and here in the order of difficulty.
> * This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome, if a somewhat [difficult post][lstm-3], by Andrej.
> * This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts [here][lstm-4] and [here][lstm-5] in the order of difficulty     

### What this article is about?
> * Using the [first principles][first principles] we picturize/visualize [MultiRNNCell] and [Bidirectional RNNs] and their backward propagation.
> * Then we use a heady mixture of intuition, Logic, and programming to prove mathematical results.   

### Context
This is the same example and the context is the same as described in [part-1]. <strong>The focus. however, this time is on [MultiRNNCell] and [Bidirectional RNNs]</strong>.

There are 2 parts to this article.
1. [Multi layer RNNs]
2. [Bidirectional RNNs][Bidirectional RNNs section]
3. [Combining Bidirectional and MultiLayerRNNs]

# <a name="Multi layer RNNs"></a>
### Multi layer RNNs
1. [Recurent depth]
2. [Feed Forward depth]
3. [Multi layer RNNs Forward pass]
4. [Multi layer RNNs backward propagation]

# <a name="Recurent depth"></a>
#### Recurent depth
A quick recap, LSTM encapsulates the internal cell logic. There are many variations to Cell logic, VanillaRNN, LSTMs, GRUs, etc. What we have seen so far is they can be fed with sequential time series data. We feed in the sequence, compare with the labels, calculate errors back-propagate and adjust the weights. The length of the fed in the sequence is informally called recurrent depth.

{%
    include image.html
    src="/img/rnn/multilayercell.jpg"
    caption="figure-1: <strong>Recurrent depth</strong>=3"
    hight="110%"
    width="110%"
%}
Figure-1 shows the recurrent depth. Formally stated, <strong>Recurrent depth is the Longest path between the same hidden state in successive time-steps</strong>. The recurrent depth is amply clear.

# <a name="Feed Forward depth"></a>
#### Feed Forward depth
The topic of this article is Feed-forward depth, more akin to the depth we have in vanilla neural networks. It is the depth of a network that is generally attributed to the success of deep learning as a technique. There are downsides too, too much compute budget for one if done indiscriminately. That being said, well look at the inner workings of Multiple layers and how we set it up in code.  

{%
    include image.html
    src="/img/rnn/multilayercell2.jpg"
    caption="figure-2: <strong>Feed forward depth</strong>=3"
    hight="110%"
    width="110%"
%}
Figure-2 shows the Feed-Forward depth. Formally stated the longest path between an input and output at the same timestep.

# <a name="Multi layer RNNs Forward pass"></a>
#### Multi layer RNNs Forward pass

For the most part this straight forward as you have already seen in the [previous article][part-3] in this series. The only difference is that there are multiple cells(2 in this example) now and how the state flows forward and gradient flows backwards.

> * <strong>[MultiRNNCell]</strong> forward pass summary.
{%
    include image.html
    src="/img/rnn/multilayercell7.jpg"
    caption="figure-3: <strong>Multi layer RNNs Forward pass summary</strong>"  
    hight="110%"
    width="110%"
%}

> * Multi-layer RNNs Forward pass
> * Notice the state being passed(yellow) from the first layer to the second.
> * The softmax and cross_entropy_loss are done on the second layer output expectedly.
{%
    include image.html
    src="/img/rnn/multilayercell3.jpg"
    caption="figure-4: <strong>Multi layer RNNs Forward pass</strong>"  
    hight="110%"
    width="110%"
%}

> * Complete code of <strong>[MultiRNNCell]</strong> from [DEEP-Breathe] helps in looking at the internals
{%
    include image.html
    src="/img/rnn/multilayercell4.jpg"
    caption="figure-5: <strong>DHt</strong>"
    hight="110%"
    width="110%"
%}

> * [MultiRNNCell] <strong>outputs</strong> returned
{%
    include image.html
    src="/img/rnn/multilayercell5.jpg"
    caption="figure-6: <strong>outputs</strong> returned."
    hight="110%"
    width="110%"
%}

> * [MultiRNNCell] <strong>states</strong> returned
{%
    include image.html
    src="/img/rnn/multilayercell6.jpg"
    caption="figure-7: <strong>states</strong> returned."
    hight="110%"
    width="110%"
%}
Also when using [Deep-Breathe] you could enable forward or backward logging for MultiRNNCell like so
{% highlight python %}
  cell= MultiRNNCell([cell1, cell2],debug=True,backpassdebug=True)
  Listing-1
{% endhighlight %}

# <a name="Multi layer RNNs backward propagation"></a>
#### Multi layer RNNs backward propagation
Again, for the most part, this is almost identical to standard LSTM Backward Propagation which we went through in detail in [part-3]. So if you have got a good hang of it, only a few things change. But before we go on, refresh [DHX] and the complete [section][Backward propagation through LSTMCell(Step-2)] before you resume here. Here is what we have to calculate.

> * [MultiRNNCell] Backward propagation summary.
{%
    include image.html
    src="/img/rnn/multilayercell8.jpg"
    caption="figure-8: MultiRNNCell Backward propagation summary."
    hight="110%"
    width="110%"
%}

<strong>I'll reproduce DHX here just to set the context and note down the changes.</strong>
###### Standard DHX for Regular LSTM Cell
> * <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>.
> * Complete listing can be found at [code-1]
{%
    include image.html
    src="/img/rnn/lstmback26.jpg"
    caption="figure-9: <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>."
    hight="110%"
    width="110%"
%}
###### DHX for MultiRNNCell composing LSTM Cell
> * Dhx, Dh_next, dxt as usual and then <strong>dh_next_recurr</strong> which is passed back to MultiRNNCell to be applied to the next layer.
> * Complete listing can be found at [code-1]. Pay careful attention to the weights shapes.
{%
    include image.html
    src="/img/rnn/multilayercell9.jpg"
    caption="figure-10: <strong>dh_next_recurr</strong> which is passed back to MultiRNNCell to be applied to the next layer."
    hight="110%"
    width="110%"
%}

###### Standard DHX schematically Regular LSTM Cell
> * Many models like Neural Machine Translators(NMT), Bidirectional Encoder Representation from Transformers(BERT) use word embeddings as their inputs(Xs) which oftentimes need learning and that is where we need <strong>dxt</strong>
{%
    include image.html
    src="/img/rnn/lstmback27.jpg"
    caption="figure-30: <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>"
    hight="110%"
    width="110%"
%}

###### DHX schematically for MultiRNNCell composing LSTM Cell
> * Dhx, Dh_next, dxt as usual and then <strong>dh_next_recurr</strong> which is passed back to MultiRNNCell to be applied to the next layer.
> * The Complete listing can be found at [code-1], in particular, the [MultiRNNCell "compute_gradient()"][code-2]. Pay careful attention to the weights' shapes.
{%
    include image.html
    src="/img/rnn/multilayercell10.jpg"
    caption="figure-10: <strong>dh_next_recurr</strong> which is passed back to MultiRNNCell to be applied to next layer."
    hight="110%"
    width="110%"
%}

# <a name="Bidirectional RNNs section"></a>
### Bidirectional RNNs
The bidirectional RNNs are great for accuracy but the compute budget goes up. Most advanced architectures use bidirectional for better accuracy. It has 2 cells or 2 multi-cells where the sequence is fed in forwards as well as backwards. For many sequences, language models for instance, when the sequence is fed in-reverse then it provides a bit more context of what is being said. Consider the below statements.

> * One of the greatest American Richard Feynman said. "If I could explain it to the average person, it wouldn't have been worth the Nobel Prize."
> * One of the greatest American Richard Pryor said. "I became a performer because it was what I enjoyed doing."

The first six words of the 2 sentences are identical. But if this sequence was fed in backwards as well, then the context would have been much clearer. Setting it up much easier as illustrated in the next few figures.

> * [Bidirectional RNNs].
> * Simple 2 separate cells fed in with the same sequence in opposite directions.
{%
    include image.html
    src="/img/rnn/bilstm0.jpg"
    caption="figure-11: Bidirectional RNNs.<strong>forward cell</strong>"
    hight="110%"
    width="110%"
%}

> * [Bidirectional RNNs]
> * The State looks identical but reversed, and that is because they have been initialized identically for comparison purposes. <strong>In practice we don't do that</strong>.
{%
    include image.html
    src="/img/rnn/bilstm1.jpg"
    caption="figure-12: Bidirectional RNNs.<strong>backward cell</strong>"
    hight="110%"
    width="110%"
%}

> * [Bidirectional RNNs] Output
> * Tupled h-states for both the cells.
{%
    include image.html
    src="/img/rnn/bilstm2.jpg"
    caption="figure-13: Bidirectional RNNs:Output."
    hight="110%"
    width="110%"
%}

> * [Bidirectional RNNs] States
> * <strong>Tupled and last (c-state and h-state) for both the cells</strong>.
{%
    include image.html
    src="/img/rnn/bilstm3.jpg"
    caption="figure-14: MultiRNNCell Backward propagation summary."
    hight="110%"
    width="110%"
%}

> * [Bidirectional RNNs] Pred Calculation
> * Pred calculation is similar except the last h-state is concatenated changing the shape of Wy.

{%
    include image.html
    src="/img/rnn/bilstm4.jpg"
    caption="figure-15: MultiRNNCell Backward propagation summary."
    hight="110%"
    width="110%"
%}

# <a name="Combining Bidirectional and MultiLayerRNNs"></a>
### Combining Bidirectional and MultiLayerRNNs
> * Combining <strong>[Bidirectional RNNs] and [MultiRNNCell]</strong> <strong>Output</strong>.
{%
    include image.html
    src="/img/rnn/bilstm5.jpg"
    caption="figure-16: Combining Bidirectional RNNs and MultiRNNCell."
    hight="110%"
    width="110%"
%}

> * Combining <strong>[Bidirectional RNNs] and [MultiRNNCell]</strong> <strong>State</strong>.
{%
    include image.html
    src="/img/rnn/bilstm6.jpg"
    caption="figure-17: Combining Bidirectional RNNs and MultiRNNCell."
    hight="110%"
    width="110%"
%}

> * Combining <strong>[Bidirectional RNNs] and [MultiRNNCell]</strong> Pred Calculation.
{%
    include image.html
    src="/img/rnn/bilstm7.jpg"
    caption="figure-18: Combining Bidirectional RNNs and MultiRNNCell."
    hight="110%"
    width="110%"
%}



### Summary
Finally, that would be the complete documentation of LSTMs inner workings. Maybe a comparison with RNNs for vanishing gradient improvement will complete it. But that apart, once we have a drop on LSTMs like we did, using them in more complex architectures will become a lot easier. In the immediate next article, we'll look at the improvements in propagation because of vanishing and exploding gradients, if any, brought about by LSTMs over RNNs.   

[Recurent depth]: LSTMPart-4#Recurent depth
[Feed Forward depth]: LSTMPart-4#Feed Forward depth
[Multi layer RNNs Forward pass]: LSTMPart-4#Multi layer RNNs Forward pass
[Multi layer RNNs backward propagation]: LSTMPart-4#Multi layer RNNs backward propagation
[Backward propagation through LSTMCell(Step-2)]: LSTMPart-3#Backward propagation through LSTMCell(Step-2)

[Multi layer RNNs]: LSTMPart-4#Multi layer RNNs
[Bidirectional RNNs section]: LSTMPart-4#Bidirectional RNNs section
[Combining Bidirectional and MultiLayerRNNs]: LSTMPart-4#Combining Bidirectional and MultiLayerRNNs

[DHt]: LSTMPart-3#DHt
[DOt]: LSTMPart-3#DOt
[DCt]: LSTMPart-3#DCt
[DCprojt]: LSTMPart-3#DCprojt
[DFt]: LSTMPart-3#DFt
[DIt]: LSTMPart-3#DIt
[DHX]: LSTMPart-3#DHX
[DCt_recur]: LSTMPart-3#DCt_recur
[figure-6]: LSTMPart-3#figure-6


[part-1]: /articles/2019-07/LSTMPart-1
[part-2]: /articles/2019-07/LSTMPart-2
[part-3]: /articles/2019-07/LSTMPart-3
[part-4]: /articles/2019-07/LSTMPart-4
[multi-part series]: /tags/#LSTM
[first principles]: https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0
[Deep-Breathe]: https://github.com/slowbreathing/Deep-Breathe
[LayerNormBasicLSTMCell]: https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell
[lstm-1]: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
[lstm-2]: https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
[lstm-3]: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[lstm-4]: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html
[lstm-5]: https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577

[eq-1]: /articles/2019-05/softmax-and-its-gradient#eq-1
[eq-2]: /articles/2019-05/softmax-and-cross-entropy#eq-2
[eq-3]: /articles/2019-05/softmax-and-cross-entropy#eq-3


[code-1]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344
[code-2]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L146-L153

[MultiRNNCell]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L61-L193
[Bidirectional RNNs]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L288-L416
[softmax]: /articles/2019-05/softmax-and-its-gradient
[cross_entropy_loss]: /articles/2019-05/softmax-and-cross-entropy
[Tensorflow]: https://www.tensorflow.org/
[Listing-1]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/tfwordslstm.py
[Listing-2]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMainGraph.py
[Listing-3]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMain.py
[scr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L13
[scr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L15
[scr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L14
[pygr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn.py#L75-L76
[pygr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/common.py#L241-L255
[pygr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn_cell.py#L79-L81
