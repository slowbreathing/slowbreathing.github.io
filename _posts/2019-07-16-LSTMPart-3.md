---
layout: post
title:  "RNN Series:LSTM internals:Part-3: The Backward Propagation"
excerpt: "In this multi-part series(<a href='/articles/2019-07/LSTMPart-1'>part-1</a>,<a href='/articles/2019-07/LSTMPart-2'>part-2</a>), we look inside LSTM Backward Propagation. This would usually involve <strong>lots and lots of math</strong>. I love math but I am <strong>not a trained mathematician</strong>. However, I have decent <strong>intuition</strong> and intuition by itself can only get you so far. So I used my <strong>programming skills</strong> to validate pure theoretical results often cited by <strong>pure/trained mathematicians.</strong> <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>"
mathjax: true
date:   2019-07-15 15:07:19
categories: [article]
tags: Artificial-Intelligence Deep-Learning LSTM RNN Sequence-Modelling
comments: true
---

### Introduction
<strong>In this [multi-part series][multi-part series], we look inside LSTM forward pass.</strong> If you haven't already read it I suggest run through the previous parts ([part-1],[part-2]) before you come back here. Once you are back, in this article, we explore LSTM's Backward Propagation. This would usually involve <strong>lots of math</strong>. I am <strong>not a trained mathematician</strong>, however, I have decent <strong>intuition</strong>. But just intuition can only get you so far. So I used my <strong>programming skills</strong> to validate pure theoretical results often cited by a <strong>pure mathematician.</strong> <strong>I do this using the first principles approach for which I have pure python implementation Deep-Breathe of most complex Deep Learning models.</strong>"

### What this article is not about?
> * This article will not talk about the conceptual model of LSTM, on which there is some great existing material [here][lstm-1] and [here][lstm-2] in the order of difficulty.
> * This is not about the differences between vanilla RNN and LSTMs, on which there is an awesome post by Andrej if a somewhat [difficult post][lstm-3].
> * This is not about how LSTMs mitigate the vanishing gradient problem, on which there is a little mathy but awesome posts [here][lstm-4] and [here][lstm-5] in the order of difficulty.     

### What this article is about?
> * Using the [first principles][first principles] we picturize/visualize the Backward Propagation.
> * Then we use a heady mixture of intuition, Logic, and programming to prove mathematical results.   

### Context
This is the same example and the context is the same as described in [part-1]. <strong>The focus, however, this time is on LSTMCell.</strong>

### LSTM Cell Backward Propagation
While there are many variations to the LSTM cell. In the current article, we are looking at [LayerNormBasicLSTMCell]. The next section is divided into 3 parts.
1. [LSTM Cell Forward Propagation(Summary)]
2. [LSTM Cell Backward Propagation(Summary)]
3. [LSTM Cell Backward Propagation(Detailed)]

# <a name="LSTM Cell Forward Propagation(Summary)"></a>
#### 1. LSTM Cell Forward Propagation(Summary)

> * <strong>Forward propagation:Summary with weights</strong>.
{%
    include image.html
    src="/img/rnn/lstmcelltemplate5.jpg"
    caption="figure-1: <strong>Forward propagation:Summary with weights.</strong>"
    hight="110%"
    width="110%"
%}

> * <strong>Forward propagation:Summary as flow diagram</strong>.  
{%
    include image.html
    src="/img/rnn/lstmforward.jpg"
    caption="figure-2: <strong>Forward propagation:Summary as simple flow diagram.</strong>"
    hight="110%"
    width="110%"
%}
> * <strong>Forward propagation: The complete picture</strong>.
{%
    include image.html
    src="/img/rnn/lstmback1.jpg"
    caption="figure-3: Forward propagation: The complete picture."
    hight="110%"
    width="110%"
%}

# <a name="LSTM Cell Backward Propagation(Summary)"></a>
#### 2. LSTM Cell Backward Propagation(Summary)
> * Backward Propagation through time or BPTT is shown here in 2 steps.
> * <strong>Step-1</strong> is depicted in <strong>Figure-4</strong> where it backward propagates through the <strong>FeedForward network</strong> calculating Wy and By
{%
    include image.html
    src="/img/rnn/lstmback2.jpg"
    caption="figure-4: <strong>Step-1:</strong><strong>Wy</strong> and <strong>By</strong> first."
    hight="110%"
    width="110%"
%}
> * <strong>Step-2</strong> is depicted in <strong>Figure-5</strong>, <strong>Figure-6</strong> and <strong>Figure-7</strong> where it backward propagates through the LSTMCell. This is time step-3 or the last one. BPTT is carried out in reverse order.
{%
    include image.html
    src="/img/rnn/lstmback3.jpg"
    caption="figure-5: <strong>Step-2:</strong><strong>3rd</strong> time step <strong>W<sub>f</sub>,W<sub>i</sub>,W<sub>c</sub>,W<sub>o</sub></strong> and <strong>B<sub>f</sub>,B<sub>i</sub>,B<sub>c</sub>,B<sub>o</sub></strong>"
    hight="110%"
    width="110%"
%}

> * <strong>Step-2</strong> LSTMCell BPTT in a simple reverse flow diagram.
# <a name="figure-6"></a>
{%
    include image.html
    src="/img/rnn/lstmbackcell.jpg"
    caption="figure-6: <strong>Step-2:</strong>LSTMCell BPTT in a simple reverse flow diagram."
    hight="110%"
    width="110%"
%}

> * <strong>Step-2</strong> Repeat for remaining time steps in the sequence.
{%
    include image.html
    src="/img/rnn/lstmback4.jpg"
    caption="figure-7: <strong>Step-2:</strong> Repeat for remaining time steps in the sequence"
    hight="110%"
    width="110%"
%}

# <a name="LSTM Cell Backward Propagation(Detailed)"></a>
#### 3. LSTM Cell Backward Propagation(Detailed)
The gradient calculation through the complete network is divided into 2 parts
1. [Feed-Forward Network layer(Step-1)]
2. [Backward propagation through LSTMCell(Step-2)]

# <a name="Feed-Forward Network layer(Step-1)"></a>
##### Feed-Forward Network layer(Step-1)
> * We have already done this derivation for "[softmax]" and "[cross_entropy_loss]".
> * The final derived form in [eq-3]. Which in turn depends on [eq-1][eq-1] and [eq-2][eq-2]. <strong>We are calling it "pred" instead of "logits" in figure-5</strong>.
{%
    include image.html
    src="/img/rnn/lstmback5.jpg"
    caption="figure-8: <strong>The proof, intuition and program implementation</strong> of the derivation can be found at <strong><a href='/articles/2019-05/softmax-and-its-gradient'>softmax</a></strong> and <strong><a href='/articles/2019-05/softmax-and-cross-entropy'>cross_entropy_loss</a></strong>."
    hight="110%"
    width="110%"
%}

> * <strong>"DBy"</strong> is straight forward, once the gradients for pred is clear in the previous step.
> * This is just gradient calculation, the weights aren't  adjusted just yet. They will be adjusted together in the end as a matter of convenience by an optimizer such as GradientDescentOptimizer.
> * The Complete code of listing can be found at [code-1].
{%
    include image.html
    src="/img/rnn/lstmback6.jpg"
    caption="figure-9: <strong>DBy</strong>"
    hight="110%"
    width="110%"
%}

> * <strong>"DWy"</strong> is straight forward too, once the gradients for pred is clear in the previous step(figure-5).
> * The Complete code of listing can be found at [code-2].
{%
    include image.html
    src="/img/rnn/lstmback7.jpg"
    caption="figure-10: <strong>DWy</strong>"
    hight="110%"
    width="110%"
%}

> * <strong>"DWy"</strong> and <strong>"Dby"</strong>
{%
    include image.html
    src="/img/rnn/lstmback8.jpg"
    caption="figure-11: <strong>DWy</strong> and <strong>Dby</strong>. Only the gradients are calculated, the weights are adjusted once in the end."
    hight="110%"
    width="110%"
%}

# <a name="Backward propagation through LSTMCell(Step-2)"></a>
##### Backward propagation through LSTMCell(Step-2)
There are several gradients we are calculating, [figure-6] has a complete list.
1. [DHt]
2. [DOt]
3. [DCt]
4. [DCprojt]
5. [DFt]
6. [DIt]
7. [DHX]
8. [DCt]

# <a name="DHt"></a>
###### DHt

> * Probably the most confusing step backward propagation because there is an element of time.
> * There were 2 recursive elements h-state, c-state, similarly there 2 recursive elements while deriving the gradient backwards. We call it dh_next and dc_next, as you will see later both and many other gradients are derived from <strong>dHt</strong>.
{%
    include image.html
    src="/img/rnn/lstmback9.jpg"
    caption="figure-12: <strong>DHt</strong>"  
    hight="110%"
    width="110%"
%}

> * Complete code of <strong>DHt</strong> can be found at [code-3],[code-4], and [code-5].
{%
    include image.html
    src="/img/rnn/lstmback10.jpg"
    caption="figure-13: <strong>DHt</strong>"
    hight="110%"
    width="110%"
%}

> * <strong>dHt</strong> schematically.<strong>Shape is the same as h, for this example it is (5X1)</strong>
{%
    include image.html
    src="/img/rnn/lstmback11.jpg"
    caption="figure-14: <strong>DHt</strong> Shape is the same as h, for this example it is (5X1)."
    hight="110%"
    width="110%"
%}

# <a name="DOt"></a>
###### DOt

> * <strong>Shape of <strong>DOt</strong> is the same as h, for this example it is (5X1)</strong>
> * The complete listing is at [code-6].
{%
    include image.html
    src="/img/rnn/lstmback12.jpg"
    caption="figure-15: <strong>DOt</strong>."
    hight="110%"
    width="110%"
%}

> * <strong>DOt</strong> schematically.
{%
    include image.html
    src="/img/rnn/lstmback13.jpg"
    caption="figure-16: <strong>DOt</strong> schematically."
    hight="110%"
    width="110%"
%}

> * <strong>DOt</strong>, <strong>DWo</strong> and <strong>DBo</strong>
> * Complete listing can be found at [code-7]
{%
    include image.html
    src="/img/rnn/lstmback14.jpg"
    caption="figure-17: <strong>DOt</strong>, <strong>DWo</strong> and <strong>DBo</strong> and there shapes"
    hight="110%"
    width="110%"
%}

# <a name="DCt"></a>
###### DCt

> * <strong>Shape of <strong>DCt</strong> is the same as h, for this example it is (5X1)</strong>
> * The complete listing is at [code-8].
{%
    include image.html
    src="/img/rnn/lstmback15.jpg"
    caption="figure-18: <strong>DCt</strong>."
    hight="110%"
    width="110%"
%}

> * <strong>DCt</strong> schematically.
{%
    include image.html
    src="/img/rnn/lstmback16.jpg"
    caption="figure-19: <strong>DCt</strong> schematically."
    hight="110%"
    width="110%"
%}

# <a name="DCprojt"></a>
###### DCprojt
> * <strong>Shape of <strong>DCprojt</strong> is the same as h, for this example it is (5X1)</strong>
> * The complete listing is at [code-9].
{%
    include image.html
    src="/img/rnn/lstmback17.jpg"
    caption="figure-20: <strong>DCprojt</strong>."
    hight="110%"
    width="110%"
%}

> * <strong>DCprojt</strong> schematically.
{%
    include image.html
    src="/img/rnn/lstmback18.jpg"
    caption="figure-21: <strong>DCprojt</strong> schematically."
    hight="110%"
    width="110%"
%}

> * <strong>DCprojt</strong>
> * Complete listing can be found at [code-10]
{%
    include image.html
    src="/img/rnn/lstmback19.jpg"
    caption="figure-22: <strong>DCprojt</strong>"
    hight="110%"
    width="110%"
%}

# <a name="DFt"></a>
###### DFt
> * <strong>Shape of <strong>DFt</strong> is the same as h, for this example it is (5X1)</strong>
> * The complete listing is at [code-11].
{%
    include image.html
    src="/img/rnn/lstmback20.jpg"
    caption="figure-23: <strong>DFt</strong>."
    hight="110%"
    width="110%"
%}

> * <strong>DFt</strong> schematically.
{%
    include image.html
    src="/img/rnn/lstmback21.jpg"
    caption="figure-24: <strong>DFt</strong> schematically."
    hight="110%"
    width="110%"
%}

> * <strong>DFt</strong>
> * Complete listing can be found at [code-12]
{%
    include image.html
    src="/img/rnn/lstmback22.jpg"
    caption="figure-25: <strong>DFt</strong>"
    hight="110%"
    width="110%"
%}

# <a name="DIt"></a>
###### DIt
> * <strong>Shape of <strong>DIt</strong> is the same as h, for this example it is (5X1)</strong>
> * The complete listing is at [code-13].
{%
    include image.html
    src="/img/rnn/lstmback23.jpg"
    caption="figure-26: <strong>DIt</strong>."
    hight="110%"
    width="110%"
%}

> * <strong>DIt</strong> schematically.
{%
    include image.html
    src="/img/rnn/lstmback24.jpg"
    caption="figure-27: <strong>DIt</strong> schematically."
    hight="110%"
    width="110%"
%}

> * <strong>DIt</strong>
> * Complete listing can be found at [code-14]
{%
    include image.html
    src="/img/rnn/lstmback25.jpg"
    caption="figure-28: <strong>DIt</strong>"
    hight="110%"
    width="110%"
%}

# <a name="DHX"></a>
###### DHX
> * <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>.
> * Complete listing can be found at [code-15]
{%
    include image.html
    src="/img/rnn/lstmback26.jpg"
    caption="figure-29: <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>."
    hight="110%"
    width="110%"
%}

> * Many models like Neural Machine Translators(NMT), Bidirectional Encoder Representation from Transformers(BERT) use word embeddings as their inputs(Xs) which oftentimes need learning and that is where we need <strong>dxt</strong>
{%
    include image.html
    src="/img/rnn/lstmback27.jpg"
    caption="figure-30: <strong>Dhx</strong>, <strong>Dh_next</strong>, <strong>dxt</strong>"
    hight="110%"
    width="110%"
%}

# <a name="DCt_recur"></a>
###### DCt_recur
> * <strong>DCt</strong>.
> * Complete listing can be found at [code-16]
{%
    include image.html
    src="/img/rnn/lstmback28.jpg"
    caption="figure-31: <strong>DCt</strong>."
    hight="110%"
    width="110%"
%}


### Summary
There you go, a complete documentation of LSTMCell's backward propagation. Also when using [Deep-Breathe] you could enable forward or backward logging like so
{% highlight python %}
  cell = LSTMCell(n_hidden,debug=False,backpassdebug=True)
  out_l = Dense(10,kernel_initializer=init_ops.Constant(out_weights),bias_initializer=init_ops.Constant(out_biases),backpassdebug=True,debug=False)

  Listing-1
{% endhighlight %}
We went through "hell" and back in this article. But if you have come so far then the good news is, <strong>it can only get easier</strong>. When we do use LSTMs in more complex architecture like Neural Machine Translators<strong>(NMT)</strong>, Bidirectional Encoder Representation from Transformers<strong>(BERT)</strong>, or a Differentiable Neural Computers<strong>(DNC)</strong> <strong>it will be a walk in a blooming lovely park</strong>. Well alomost!! ;-). In a later article well demystify the multilayer LSTMcells and Bi-directional LSTMCells.


[LSTM Cell Forward Propagation(Summary)]: LSTMPart-3#LSTM Cell Forward Propagation(Summary)
[LSTM Cell Backward Propagation(Summary)]: LSTMPart-3#LSTM Cell Backward Propagation(Summary)
[LSTM Cell Backward Propagation(Detailed)]: LSTMPart-3#LSTM Cell Backward Propagation(Detailed)
[Feed-Forward Network layer(Step-1)]: LSTMPart-3#Feed-Forward Network layer(Step-1)
[Backward propagation through LSTMCell(Step-2)]: LSTMPart-3#Backward propagation through LSTMCell(Step-2)

[DHt]: LSTMPart-3#DHt
[DOt]: LSTMPart-3#DOt
[DCt]: LSTMPart-3#DCt
[DCprojt]: LSTMPart-3#DCprojt
[DFt]: LSTMPart-3#DFt
[DIt]: LSTMPart-3#DIt
[DHX]: LSTMPart-3#DHX
[DCt_recur]: LSTMPart-3#DCt_recur
[figure-6]: LSTMPart-3#figure-6


[part-1]: /articles/2019-07/LSTMPart-1
[part-2]: /articles/2019-07/LSTMPart-2
[part-3]: /articles/2019-07/LSTMPart-3
[multi-part series]: /tags/#LSTM
[first principles]: https://medium.com/the-mission/elon-musks-3-step-first-principles-thinking-how-to-think-and-solve-difficult-problems-like-a-ba1e73a9f6c0
[Deep-Breathe]: https://github.com/slowbreathing/Deep-Breathe
[LayerNormBasicLSTMCell]: https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell
[lstm-1]: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
[lstm-2]: https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
[lstm-3]: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[lstm-4]: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html
[lstm-5]: https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577

[eq-1]: /articles/2019-05/softmax-and-its-gradient#eq-1
[eq-2]: /articles/2019-05/softmax-and-cross-entropy#eq-2
[eq-3]: /articles/2019-05/softmax-and-cross-entropy#eq-3

[code-1]: https://github.com/slowbreathing/Deep-Breathe/blob/3fc68956eeaa082b59af2d181ff66e3932f49415/org/mk/training/dl/core.py#L157-L170
[code-2]: https://github.com/slowbreathing/Deep-Breathe/blob/3fc68956eeaa082b59af2d181ff66e3932f49415/org/mk/training/dl/rnn.py#L444
[code-3]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L445
[code-4]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn.py#L492-L499
[code-5]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L295-L299
[code-6]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L328
[code-7]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L329-L331
[code-8]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L309
[code-9]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L323
[code-10]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L324-L326
[code-11]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L313
[code-12]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L314-L316
[code-13]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L318
[code-14]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L320-L321
[code-15]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L333-L344
[code-16]: https://github.com/slowbreathing/Deep-Breathe/blob/b5f4073ec616ebe198721bfedcd3ab8060c0847a/org/mk/training/dl/rnn_cell.py#L342

[softmax]: /articles/2019-05/softmax-and-its-gradient
[cross_entropy_loss]: /articles/2019-05/softmax-and-cross-entropy
[Tensorflow]: https://www.tensorflow.org/
[Listing-1]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/tfwordslstm.py
[Listing-2]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMainGraph.py
[Listing-3]: https://github.com/slowbreathing/Deep-Breathe/blob/master/org/mk/training/dl/LSTMMain.py
[scr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L13
[scr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L15
[scr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/f9585bde9cbb61e71f67ccd936aa22a155c36709/scripts#L14
[pygr-1]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn.py#L75-L76
[pygr-2]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/common.py#L241-L255
[pygr-3]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/rnn_cell.py#L79-L81
[pygr-4]: https://github.com/slowbreathing/Deep-Breathe/blob/c3b538d9c3afeeb5a15c3d91ea9063976438c810/org/mk/training/dl/core.py#L107-L112
